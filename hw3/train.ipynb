{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio, torchvision\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import librosa\n",
    "import argparse\n",
    "import numpy as np\n",
    "import wandb\n",
    "from pytorch_lightning import LightningModule, Trainer, LightningDataModule, Callback\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torchmetrics.functional import accuracy\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchaudio.datasets import SPEECHCOMMANDS\n",
    "from torchaudio.datasets.speechcommands import load_speechcommands_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SilenceDataset(SPEECHCOMMANDS):\n",
    "    def __init__(self, root):\n",
    "        super(SilenceDataset, self).__init__(root, subset='training')\n",
    "        self.len = len(self._walker) // 35\n",
    "        path = os.path.join(self._path, torchaudio.datasets.speechcommands.EXCEPT_FOLDER)\n",
    "        self.paths = [os.path.join(path, p) for p in os.listdir(path) if p.endswith('.wav')]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = np.random.randint(0, len(self.paths))\n",
    "        filepath = self.paths[index]\n",
    "        waveform, sample_rate = torchaudio.load(filepath)\n",
    "        return waveform, sample_rate, \"silence\", 0, 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "class UnknownDataset(SPEECHCOMMANDS):\n",
    "    def __init__(self, root):\n",
    "        super(UnknownDataset, self).__init__(root, subset='training')\n",
    "        self.len = len(self._walker) // 35\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = np.random.randint(0, len(self._walker))\n",
    "        fileid = self._walker[index]\n",
    "        waveform, sample_rate, _, speaker_id, utterance_number = load_speechcommands_item(fileid, self._path)\n",
    "        return waveform, sample_rate, \"unknown\", speaker_id, utterance_number\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lightning Data module basic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KWSDataModule(LightningDataModule):\n",
    "    def __init__(self, path, batch_size=128, num_workers=0, n_fft=512, \n",
    "                 n_mels=128, win_length=None, hop_length=256, class_dict={}, \n",
    "                 **kwargs):\n",
    "        print('_'*20, 'kws init')\n",
    "        super().__init__(**kwargs)\n",
    "        self.path = path\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.n_fft = n_fft\n",
    "        self.n_mels = n_mels\n",
    "        self.win_length = win_length\n",
    "        self.hop_length = hop_length\n",
    "        self.class_dict = class_dict\n",
    "\n",
    "    def prepare_data(self):\n",
    "        print('_'*20, 'kws prep data')\n",
    "        self.train_dataset = torchaudio.datasets.SPEECHCOMMANDS(self.path,\n",
    "                                                                download=True,\n",
    "                                                                subset='training')\n",
    "\n",
    "        silence_dataset = SilenceDataset(self.path)\n",
    "        unknown_dataset = UnknownDataset(self.path)\n",
    "        self.train_dataset = torch.utils.data.ConcatDataset([self.train_dataset, silence_dataset, unknown_dataset])\n",
    "                                                                \n",
    "        self.val_dataset = torchaudio.datasets.SPEECHCOMMANDS(self.path,\n",
    "                                                              download=True,\n",
    "                                                              subset='validation')\n",
    "        self.test_dataset = torchaudio.datasets.SPEECHCOMMANDS(self.path,\n",
    "                                                               download=True,\n",
    "                                                               subset='testing')                                                    \n",
    "        _, sample_rate, _, _, _ = self.train_dataset[0]\n",
    "        self.sample_rate = sample_rate\n",
    "        self.transform = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate,\n",
    "                                                              n_fft=self.n_fft,\n",
    "                                                              win_length=self.win_length,\n",
    "                                                              hop_length=self.hop_length,\n",
    "                                                              n_mels=self.n_mels,\n",
    "                                                              power=2.0)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        print('_'*20, 'setup ')\n",
    "        self.prepare_data()\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        print('_'*20, 'kws train dataloader')\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            collate_fn=self.collate_fn\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        print('_'*20, 'kws val dataloader')\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            collate_fn=self.collate_fn\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        print('_'*20, 'kws test dataloader')\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            collate_fn=self.collate_fn\n",
    "        )\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        print('_'*20, 'kws collate fn')\n",
    "        mels = []\n",
    "        labels = []\n",
    "        wavs = []\n",
    "        for sample in batch:\n",
    "            waveform, sample_rate, label, speaker_id, utterance_number = sample\n",
    "            # ensure that all waveforms are 1sec in length; if not pad with zeros\n",
    "            if waveform.shape[-1] < sample_rate:\n",
    "                waveform = torch.cat([waveform, torch.zeros((1, sample_rate - waveform.shape[-1]))], dim=-1)\n",
    "            elif waveform.shape[-1] > sample_rate:\n",
    "                waveform = waveform[:,:sample_rate]\n",
    "\n",
    "            # mel from power to db\n",
    "            mels.append(ToTensor()(librosa.power_to_db(self.transform(waveform).squeeze().numpy(), ref=np.max)))\n",
    "            labels.append(torch.tensor(self.class_dict[label]))\n",
    "            wavs.append(waveform)\n",
    "\n",
    "        mels = torch.stack(mels)\n",
    "        labels = torch.stack(labels)\n",
    "        wavs = torch.stack(wavs)\n",
    "   \n",
    "        return mels, labels, wavs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PL LigtningMOdule MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KWSModel(LightningModule):\n",
    "    def __init__(self, num_classes=37, epochs=30, lr=0.001, **kwargs):\n",
    "        print('_'*20, 'init')\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = torchvision.models.resnet18(num_classes=num_classes)\n",
    "        self.model.conv1 = torch.nn.Conv2d(\n",
    "            1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print('_'*20, 'fwd')\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        print('_'*20, 'steps')\n",
    "        mels, labels, _ = batch\n",
    "\n",
    "        print(mels.size())\n",
    "\n",
    "        preds = self.model(mels)\n",
    "        loss = self.hparams.criterion(preds, labels)\n",
    "        return {'loss': loss}\n",
    "\n",
    "    # calls to self.log() are recorded in wandb\n",
    "    def training_epoch_end(self, outputs):\n",
    "        print('_'*20, 'train ep end')\n",
    "        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        self.log(\"train_loss\", avg_loss, on_epoch=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        print('_'*20, 'valid step')\n",
    "        return self.test_step(batch, batch_idx)\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        print('_'*20, 'vald epoc end')\n",
    "        return self.test_epoch_end(outputs)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        print('_'*20, 'test step')\n",
    "        mels, labels, wavs = batch\n",
    "        preds = self.model(mels)\n",
    "        loss = self.hparams.criterion(preds, labels)\n",
    "        acc = accuracy(preds, labels) * 100.\n",
    "        return {\"preds\": preds, 'test_loss': loss, 'test_acc': acc}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        print('_'*20, 'test ep end')\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x['test_acc'] for x in outputs]).mean()\n",
    "        self.log(\"test_loss\", avg_loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_acc\", avg_acc, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        print('_'*20, 'optimizer')\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(), lr=self.hparams.lr)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer=optimizer, T_max=self.hparams.epochs)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        print('_'*20, 'setup')\n",
    "        self.hparams.criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # model training hyperparameters\n",
    "    parser.add_argument('--batch-size', type=int, default=128, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--max-epochs', type=int, default=30, metavar='N',\n",
    "                        help='number of epochs to train (default: 30)')\n",
    "    parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
    "                        help='learning rate (default: 0.001)')\n",
    "\n",
    "    # where dataset will be stored\n",
    "    parser.add_argument(\"--path\", type=str, default=\"data/speech_commands/\")\n",
    "\n",
    "    # 35 keywords + silence + unknown\n",
    "    parser.add_argument(\"--num-classes\", type=int, default=37)\n",
    "   \n",
    "    # mel spectrogram parameters\n",
    "    parser.add_argument(\"--n-fft\", type=int, default=1024)\n",
    "    parser.add_argument(\"--n-mels\", type=int, default=128)\n",
    "    parser.add_argument(\"--win-length\", type=int, default=None)\n",
    "    parser.add_argument(\"--hop-length\", type=int, default=512)\n",
    "\n",
    "    # 16-bit fp model to reduce the size\n",
    "    parser.add_argument(\"--precision\", default=16)\n",
    "    parser.add_argument(\"--accelerator\", default='gpu')\n",
    "    parser.add_argument(\"--devices\", default=1)\n",
    "    parser.add_argument(\"--num-workers\", type=int, default=96)\n",
    "\n",
    "    parser.add_argument(\"--no-wandb\", default=True, action='store_true')\n",
    "\n",
    "    args = parser.parse_args(\"\")\n",
    "    return args\n",
    "args = get_args()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to ewdit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from pytorch_lightning import LightningModule, Trainer, LightningDataModule\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchmetrics.functional import accuracy\n",
    "from einops import rearrange\n",
    "from torch import nn\n",
    "# from torchvision.datasets.cifar import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LitTransformer(LightningModule):\n",
    "    def __init__(self, num_classes=10, lr=0.001, max_epochs=30, depth=12, embed_dim=64,\n",
    "                 head=4, patch_dim=192, seqlen=16, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.encoder = Transformer(dim=embed_dim, num_heads=head, num_blocks=depth, mlp_ratio=4.,\n",
    "                                   qkv_bias=False, act_layer=nn.GELU, norm_layer=nn.LayerNorm)\n",
    "        self.embed = torch.nn.Linear(patch_dim, embed_dim)\n",
    "\n",
    "        self.fc = nn.Linear(seqlen * embed_dim, num_classes)\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init_weights_vit_timm(self)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear projection\n",
    "        x = self.embed(x)\n",
    "            \n",
    "        # Encoder\n",
    "        x = self.encoder(x)\n",
    "        x = x.flatten(start_dim=1)\n",
    "\n",
    "        # Classification head\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        # this decays the learning rate to 0 after max_epochs using cosine annealing\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=self.hparams.max_epochs)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        acc = accuracy(y_hat, y)\n",
    "        return {\"y_hat\": y_hat, \"test_loss\": loss, \"test_acc\": acc}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"test_loss\"] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x[\"test_acc\"] for x in outputs]).mean()\n",
    "        self.log(\"test_loss\", avg_loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_acc\", avg_acc*100., on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.test_step(batch, batch_idx)\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        return self.test_epoch_end(outputs)\n",
    "\n",
    "\n",
    "# a lightning data module for cifar 10 dataset\n",
    "class LitCifar10(LightningDataModule):\n",
    "    def __init__(self, batch_size=32, num_workers=32, patch_num=4, **kwargs):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.patch_num = patch_num\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.train_set = CIFAR10(root='./data', train=True,\n",
    "                                 download=True, transform=torchvision.transforms.ToTensor())\n",
    "        self.test_set = CIFAR10(root='./data', train=False,\n",
    "                                download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        x, y = zip(*batch)\n",
    "        x = torch.stack(x, dim=0)\n",
    "        y = torch.LongTensor(y)\n",
    "        x = rearrange(x, 'b c (p1 h) (p2 w) -> b (p1 p2) (c h w)', p1=self.patch_num, p2=self.patch_num)\n",
    "        return x, y\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.train_set, batch_size=self.batch_size, \n",
    "                                        shuffle=True, collate_fn=self.collate_fn,\n",
    "                                        num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.test_set, batch_size=self.batch_size, \n",
    "                                        shuffle=False, collate_fn=self.collate_fn,\n",
    "                                        num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.test_dataloader()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_args():\n",
    "    parser = ArgumentParser(description='PyTorch Transformer')\n",
    "    parser.add_argument('--depth', type=int, default=12, help='depth')\n",
    "    parser.add_argument('--embed_dim', type=int, default=64, help='embedding dimension')\n",
    "    parser.add_argument('--num_heads', type=int, default=4, help='num_heads')\n",
    "\n",
    "    parser.add_argument('--patch_num', type=int, default=8, help='patch_num')\n",
    "    parser.add_argument('--kernel_size', type=int, default=3, help='kernel size')\n",
    "    parser.add_argument('--batch_size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: )')\n",
    "    parser.add_argument('--max-epochs', type=int, default=30, metavar='N',\n",
    "                        help='number of epochs to train (default: 0)')\n",
    "    parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
    "                        help='learning rate (default: 0.0)')\n",
    "\n",
    "    parser.add_argument('--accelerator', default='gpu', type=str, metavar='N')\n",
    "    parser.add_argument('--devices', default=1, type=int, metavar='N')\n",
    "    parser.add_argument('--dataset', default='cifar10', type=str, metavar='N')\n",
    "    parser.add_argument('--num_workers', default=4, type=int, metavar='N')\n",
    "    args = parser.parse_args(\"\")\n",
    "    return args\n",
    "args = get_args()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CALLBACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WandbCallback(Callback):\n",
    "\n",
    "    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n",
    "        # log 10 sample audio predictions from the first batch\n",
    "        if batch_idx == 0:\n",
    "            n = 10\n",
    "            mels, labels, wavs = batch\n",
    "            preds = outputs[\"preds\"]\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "\n",
    "            labels = labels.cpu().numpy()\n",
    "            preds = preds.cpu().numpy()\n",
    "            \n",
    "            wavs = torch.squeeze(wavs, dim=1)\n",
    "            wavs = [ (wav.cpu().numpy()*32768.0).astype(\"int16\") for wav in wavs]\n",
    "            \n",
    "            sample_rate = pl_module.hparams.sample_rate\n",
    "            idx_to_class = pl_module.hparams.idx_to_class\n",
    "            \n",
    "            # log audio samples and predictions as a W&B Table\n",
    "            columns = ['audio', 'mel', 'ground truth', 'prediction']\n",
    "            data = [[wandb.Audio(wav, sample_rate=sample_rate), wandb.Image(mel), idx_to_class[label], idx_to_class[pred]] for wav, mel, label, pred in list(\n",
    "                zip(wavs[:n], mels[:n], labels[:n], preds[:n]))]\n",
    "            wandb_logger.log_table(\n",
    "                key='ResNet18 on KWS using PyTorch Lightning',\n",
    "                columns=columns,\n",
    "                data=data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments and Other Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CLASSES = ['silence', 'unknown', 'backward', 'bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'follow',\n",
    "            'forward', 'four', 'go', 'happy', 'house', 'learn', 'left', 'marvin', 'nine', 'no',\n",
    "            'off', 'on', 'one', 'right', 'seven', 'sheila', 'six', 'stop', 'three',\n",
    "            'tree', 'two', 'up', 'visual', 'wow', 'yes', 'zero']\n",
    "\n",
    "\n",
    "# make a dictionary from CLASSES to integers\n",
    "CLASS_TO_IDX = {c: i for i, c in enumerate(CLASSES)}\n",
    "\n",
    "if not os.path.exists(args.path):\n",
    "    os.makedirs(args.path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ init\n",
      "KWSModel(\n",
      "  (model): ResNet(\n",
      "    (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=512, out_features=37, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = KWSModel(num_classes=args.num_classes, epochs=args.max_epochs, lr=args.lr)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KWSModel' object has no attribute 'conv1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/dl/Desktop/dl/object_detection_model_hw2/hw3/train.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/train.ipynb#ch0000020?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mconv1\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1185\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1182'>1183</a>\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1183'>1184</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1184'>1185</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1185'>1186</a>\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KWSModel' object has no attribute 'conv1'"
     ]
    }
   ],
   "source": [
    "model.conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ kws init\n",
      "____________________ setup \n",
      "____________________ kws prep data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "datamodule = KWSDataModule(batch_size=args.batch_size, num_workers=args.num_workers,\n",
    "                            path=args.path, n_fft=args.n_fft, n_mels=args.n_mels,\n",
    "                            win_length=args.win_length, hop_length=args.hop_length,\n",
    "                            class_dict=CLASS_TO_IDX)\n",
    "datamodule.setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dl/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 96 worker processes in total. Our suggested max number of worker in current system is 16, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "t = datamodule.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.5259e-04,  3.0518e-05, -9.1553e-05,  ..., -6.1340e-03,\n",
       "          -2.8992e-03, -1.2207e-04]]),\n",
       " 16000,\n",
       " 'backward',\n",
       " '14c7b073',\n",
       " 4)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.dataset[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16000])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.dataset[1][0].size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ init\n",
      "KWSModel(\n",
      "  (model): ResNet(\n",
      "    (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=512, out_features=37, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malessandrosantiago\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dl/Desktop/dl/object_detection_model_hw2/hw3/wandb/run-20220530_045443-3mjhtw2m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/alessandrosantiago/pl-kws/runs/3mjhtw2m\" target=\"_blank\">bumbling-water-41</a></strong> to <a href=\"https://wandb.ai/alessandrosantiago/pl-kws\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ kws prep data\n",
      "____________________ setup \n",
      "____________________ kws prep data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:611: UserWarning: Checkpoint directory /home/dl/Desktop/dl/object_detection_model_hw2/hw3/data/speech_commands/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ setup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | ResNet | 11.2 M\n",
      "---------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "22.378    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ optimizer\n",
      "Sanity Checking: 0it [00:00, ?it/s]____________________ kws val dataloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dl/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 96 worker processes in total. Our suggested max number of worker in current system is 16, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________ kws collate fn____________________ ________________________________________________________________________________________________________________________kws collate fn   ________________________________________ kws collate fn \n",
      "\n",
      "  kws collate fn kws collate fnkws collate fn\n",
      " kws collate fnkws collate fnkws collate fn\n",
      "\n",
      "kws collate fn\n",
      "\n",
      "____________________\n",
      "____________________\n",
      "____________________ ____________________kws collate fn\n",
      "  ____________________\n",
      " kws collate fnkws collate fn____________________\n",
      "____________________\n",
      " kws collate fn ____________________ ____________________kws collate fn\n",
      " ____________________kws collate fn\n",
      " kws collate fn ____________________\n",
      "kws collate fn\n",
      "kws collate fn____________________ ________________________________________\n",
      "kws collate fn\n",
      " kws collate fn____________________ \n",
      "kws collate fnkws collate fn ____________________kws collate fn\n",
      "kws collate fn\n",
      "________________________________________\n",
      "\n",
      "\n",
      "  ____________________kws collate fn \n",
      "kws collate fn____________________ kws collate fn\n",
      " ____________________  \n",
      "kws collate fnkws collate fnkws collate fn\n",
      "kws collate fn____________________ \n",
      "________________________________________\n",
      "\n",
      " kws collate fnkws collate fn  \n",
      "\n",
      "________________________________________kws collate fn\n",
      " ____________________kws collate fn____________________ kws collate fn\n",
      " kws collate fn____________________\n",
      "kws collate fn ____________________kws collate fn____________________\n",
      "____________________  kws collate fnkws collate fn\n",
      "\n",
      "____________________ \n",
      " kws collate fn\n",
      "____________________ ____________________ \n",
      "kws collate fn \n",
      "kws collate fn____________________kws collate fn \n",
      "____________________\n",
      " kws collate fnkws collate fn____________________\n",
      "kws collate fn\n",
      "________________________________________\n",
      " kws collate fn\n",
      "____________________________________________________________  kws collate fn\n",
      "kws collate fn \n",
      "  kws collate fn____________________kws collate fn____________________\n",
      "\n",
      "kws collate fn________________________________________ kws collate fn____________________\n",
      "   kws collate fn\n",
      "\n",
      "kws collate fn____________________________________________________________  kws collate fn____________________kws collate fn  kws collate fn ____________________kws collate fn\n",
      "\n",
      "\n",
      "kws collate fn\n",
      "____________________kws collate fn____________________ \n",
      " \n",
      "kws collate fn____________________\n",
      "kws collate fn\n",
      " kws collate fn\n",
      " kws collate fn\n",
      "____________________\n",
      "____________________________________________________________  kws collate fn____________________  kws collate fnkws collate fn\n",
      "\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]kws collate fn ________________________________________  kws collate fn\n",
      "\n",
      "kws collate fn________________________________________\n",
      " \n",
      " kws collate fnkws collate fn\n",
      "kws collate fn\n",
      "\n",
      "____________________ kws collate fn\n",
      "____________________ valid step\n",
      "____________________ test step\n",
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  3.44it/s]____________________ valid step\n",
      "____________________ test step\n",
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  3.44it/s]____________________ vald epoc end\n",
      "____________________ test ep end\n",
      "____________________ kws train dataloader                                  \n",
      "Epoch 0:   0%|          | 0/779 [00:00<?, ?it/s] ________________________________________  ________________________________________________________________________________________________________________________ kws collate fn ____________________________________________________________     ____________________kws collate fnkws collate fn\n",
      "  kws collate fnkws collate fnkws collate fnkws collate fn____________________kws collate fn________________________________________\n",
      "\n",
      "\n",
      "kws collate fn____________________\n",
      "\n",
      " kws collate fn____________________ kws collate fn\n",
      "____________________ \n",
      "kws collate fn ________________________________________\n",
      "kws collate fn\n",
      "\n",
      "____________________________________________________________ ____________________kws collate fn\n",
      "\n",
      "____________________   kws collate fn ____________________________________________________________kws collate fnkws collate fn\n",
      "____________________\n",
      "kws collate fnkws collate fn\n",
      "  kws collate fn\n",
      " kws collate fn  kws collate fn\n",
      "\n",
      "____________________kws collate fnkws collate fn  kws collate fn\n",
      "\n",
      "\n",
      " ____________________ \n",
      "\n",
      "\n",
      "kws collate fn kws collate fn kws collate fnkws collate fnkws collate fn\n",
      "kws collate fn\n",
      "\n",
      "\n",
      "________________________________________ \n",
      "____________________ ____________________kws collate fn\n",
      "kws collate fn \n",
      "____________________ kws collate fn\n",
      "____________________\n",
      "________________________________________ ____________________ kws collate fn  kws collate fn \n",
      "\n",
      "kws collate fnkws collate fnkws collate fnkws collate fn\n",
      "\n",
      "\n",
      "____________________________________________________________ ____________________kws collate fn ____________________\n",
      "____________________ ____________________ kws collate fn\n",
      "\n",
      " kws collate fn\n",
      "kws collate fn\n",
      "\n",
      "kws collate fn kws collate fn____________________kws collate fn \n",
      "________________________________________ ____________________\n",
      "kws collate fn ____________________  \n",
      " kws collate fn________________________________________kws collate fn ____________________\n",
      "kws collate fn____________________ ____________________ kws collate fn________________________________________kws collate fn____________________ \n",
      " \n",
      "kws collate fnkws collate fn\n",
      " ____________________ \n",
      "\n",
      " ____________________\n",
      "kws collate fnkws collate fn kws collate fn\n",
      "\n",
      "kws collate fnkws collate fn\n",
      "kws collate fn\n",
      "____________________\n",
      "________________________________________ kws collate fn\n",
      "____________________   kws collate fn\n",
      "____________________ \n",
      "kws collate fnkws collate fn________________________________________\n",
      "\n",
      "kws collate fn   kws collate fnkws collate fn____________________\n",
      " kws collate fn\n",
      "\n",
      "\n",
      "____________________kws collate fn\n",
      " ____________________kws collate fn\n",
      "____________________________________________________________ \n",
      "  kws collate fn kws collate fn____________________kws collate fn____________________\n",
      " kws collate fn____________________\n",
      "kws collate fn\n",
      " \n",
      " kws collate fn________________________________________kws collate fn\n",
      " kws collate fn\n",
      "____________________ ____________________ kws collate fnkws collate fn____________________  \n",
      "\n",
      "________________________________________ ________________________________________kws collate fn kws collate fn\n",
      "\n",
      " kws collate fn \n",
      "____________________kws collate fn\n",
      "kws collate fn________________________________________\n",
      " kws collate fn \n",
      "____________________\n",
      "____________________kws collate fn ____________________  kws collate fn____________________kws collate fn____________________________________________________________  ____________________\n",
      "________________________________________kws collate fn____________________ ________________________________________kws collate fn____________________kws collate fn____________________kws collate fnkws collate fn________________________________________\n",
      "\n",
      "\n",
      "kws collate fn ________________________________________\n",
      "____________________   ____________________kws collate fn   \n",
      "  ____________________kws collate fn____________________ \n",
      "________________________________________ \n",
      "____________________ \n",
      "\n",
      "________________________________________ ____________________\n",
      "____________________ ________________________________________kws collate fn ____________________kws collate fn kws collate fn____________________kws collate fn kws collate fnkws collate fn kws collate fn kws collate fnkws collate fn____________________\n",
      " ____________________  ________________________________________kws collate fn________________________________________\n",
      "\n",
      "\n",
      "____________________  kws collate fn____________________kws collate fn ____________________ ____________________\n",
      "________________________________________kws collate fn\n",
      "   ____________________  ____________________ ____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "kws collate fn \n",
      "____________________kws collate fnkws collate fn kws collate fnkws collate fn____________________kws collate fn \n",
      "kws collate fn kws collate fnkws collate fn kws collate fnkws collate fnkws collate fnkws collate fnkws collate fn________________________________________ ________________________________________\n",
      "____________________\n",
      "\n",
      " ________________________________________kws collate fn   \n",
      "____________________  \n",
      "\n",
      "\n",
      "\n",
      " ____________________ ________________________________________________________________________________________________________________________ kws collate fn kws collate fn\n",
      "\n",
      "____________________\n",
      "  kws collate fnkws collate fn  ____________________\n",
      "____________________kws collate fnkws collate fn____________________ \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " kws collate fn kws collate fn \n",
      "\n",
      "\n",
      " kws collate fn________________________________________kws collate fn \n",
      "____________________ ____________________kws collate fnkws collate fn kws collate fnkws collate fn\n",
      "kws collate fn\n",
      "kws collate fn____________________ kws collate fn____________________kws collate fnkws collate fn \n",
      "kws collate fn____________________ \n",
      "\n",
      "\n",
      "kws collate fn\n",
      " \n",
      "kws collate fn\n",
      " ____________________kws collate fn____________________kws collate fn\n",
      "____________________kws collate fn\n",
      "kws collate fnkws collate fn\n",
      "\n",
      " kws collate fn \n",
      " \n",
      "kws collate fn\n",
      "____________________kws collate fnkws collate fn  ____________________________________________________________kws collate fn \n",
      " \n",
      "\n",
      "________________________________________kws collate fn\n",
      "kws collate fn \n",
      "\n",
      "kws collate fn\n",
      "\n",
      "kws collate fnkws collate fn \n",
      "\n",
      "kws collate fn ____________________ ____________________ \n",
      " \n",
      "kws collate fnkws collate fn\n",
      "\n",
      "\n",
      "kws collate fn\n",
      "\n",
      "\n",
      "\n",
      "kws collate fn________________________________________ \n",
      "kws collate fn\n",
      " \n",
      " kws collate fn \n",
      "kws collate fnkws collate fn____________________\n",
      "\n",
      "kws collate fnkws collate fn\n",
      " \n",
      "kws collate fn\n",
      "____________________ \n",
      " kws collate fnkws collate fn\n",
      "\n",
      "\n",
      "kws collate fnkws collate fn \n",
      "kws collate fn\n",
      "____________________\n",
      "\n",
      "____________________\n",
      "________________________________________   kws collate fnkws collate fn\n",
      "kws collate fnkws collate fn \n",
      "\n",
      "\n",
      "Epoch 0:   0%|          | 1/779 [00:04<53:46,  4.15s/it, loss=3.82] fn____________________ kws collate fn\n",
      "____________________\n",
      "________________________________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   0%|          | 2/779 [00:04<27:16,  2.11s/it, loss=3.75]  kws collate fn____________________kws collate fn\n",
      "________________________________________ ____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   0%|          | 3/779 [00:04<18:28,  1.43s/it, loss=3.75]\n",
      "Epoch 0:   0%|          | 3/779 [00:04<18:31,  1.43s/it, loss=3.66]\n",
      "____________________\n",
      " ____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "kws collate fn ________________________________________ kws collate fn\n",
      "kws collate fn\n",
      " \n",
      "____________________kws collate fn\n",
      "Epoch 0:   1%|          | 4/779 [00:04<14:04,  1.09s/it, loss=3.6] ____________________  kws collate fnkws collate fn\n",
      "____________________ kws collate fn\n",
      "\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   1%|          | 5/779 [00:04<11:21,  1.14it/s, loss=3.58]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:   1%|          | 6/779 [00:04<09:34,  1.35it/s, loss=3.54]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:   1%|          | 7/779 [00:04<08:17,  1.55it/s, loss=3.52]____________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   1%|          | 8/779 [00:04<07:19,  1.75it/s, loss=3.5] ____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   1%|          | 9/779 [00:04<06:35,  1.95it/s, loss=3.46]____________________ kws collate fn\n",
      "____________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   1%|▏         | 10/779 [00:04<05:59,  2.14it/s, loss=3.43]____________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   1%|▏         | 11/779 [00:04<05:30,  2.33it/s, loss=3.42]____________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   2%|▏         | 12/779 [00:04<05:05,  2.51it/s, loss=3.39]____________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   2%|▏         | 13/779 [00:04<04:44,  2.69it/s, loss=3.38]____________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   2%|▏         | 14/779 [00:04<04:26,  2.87it/s, loss=3.36]____________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   2%|▏         | 15/779 [00:04<04:10,  3.04it/s, loss=3.34]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:   2%|▏         | 16/779 [00:04<03:57,  3.22it/s, loss=3.3] ____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:   2%|▏         | 17/779 [00:05<03:45,  3.38it/s, loss=3.29]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:   2%|▏         | 18/779 [00:05<03:34,  3.55it/s, loss=3.27]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:   2%|▏         | 19/779 [00:05<03:24,  3.72it/s, loss=3.25]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:   3%|▎         | 20/779 [00:05<03:15,  3.88it/s, loss=3.22]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:   3%|▎         | 21/779 [00:05<03:07,  4.04it/s, loss=3.16]________________________________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      " kws collate fn\n",
      "Epoch 0:   3%|▎         | 22/779 [00:05<03:00,  4.19it/s, loss=3.12]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:   3%|▎         | 23/779 [00:05<02:54,  4.34it/s, loss=3.07]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:   3%|▎         | 24/779 [00:05<02:48,  4.49it/s, loss=3.04]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:   3%|▎         | 25/779 [00:05<02:42,  4.65it/s, loss=2.99]____________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   3%|▎         | 26/779 [00:05<02:36,  4.81it/s, loss=2.95]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   3%|▎         | 27/779 [00:05<02:31,  4.97it/s, loss=2.91]____________________kws collate fn \n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   4%|▎         | 28/779 [00:05<02:26,  5.13it/s, loss=2.91]\n",
      "Epoch 0:   4%|▎         | 28/779 [00:05<02:26,  5.13it/s, loss=2.86]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   4%|▎         | 29/779 [00:05<02:21,  5.29it/s, loss=2.83]____________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   4%|▍         | 30/779 [00:05<02:17,  5.44it/s, loss=2.79]____________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   4%|▍         | 31/779 [00:05<02:13,  5.59it/s, loss=2.75]____________________ ____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "kws collate fn\n",
      "____________________ kws collate fn\n",
      "Epoch 0:   4%|▍         | 32/779 [00:05<02:10,  5.74it/s, loss=2.71]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:   4%|▍         | 33/779 [00:05<02:06,  5.89it/s, loss=2.67]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:   4%|▍         | 34/779 [00:05<02:03,  6.04it/s, loss=2.63]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   4%|▍         | 35/779 [00:05<02:00,  6.19it/s, loss=2.6] ____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "____________________ kws collate fn\n",
      "Epoch 0:   5%|▍         | 36/779 [00:05<01:57,  6.34it/s, loss=2.59]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:   5%|▍         | 37/779 [00:05<01:54,  6.49it/s, loss=2.54]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:   5%|▍         | 38/779 [00:05<01:51,  6.63it/s, loss=2.51]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:   5%|▌         | 39/779 [00:05<01:49,  6.78it/s, loss=2.47]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:   5%|▌         | 40/779 [00:05<01:46,  6.91it/s, loss=2.45]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:   5%|▌         | 41/779 [00:05<01:44,  7.05it/s, loss=2.43]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:   5%|▌         | 42/779 [00:05<01:42,  7.19it/s, loss=2.41]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:   6%|▌         | 43/779 [00:05<01:40,  7.33it/s, loss=2.37]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   6%|▌         | 44/779 [00:05<01:38,  7.47it/s, loss=2.37]____________________ kws collate fn\n",
      "Epoch 0:   6%|▌         | 44/779 [00:05<01:38,  7.47it/s, loss=2.35]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   6%|▌         | 45/779 [00:05<01:36,  7.60it/s, loss=2.32]____________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   6%|▌         | 46/779 [00:05<01:34,  7.74it/s, loss=2.32]____________________ kws collate fn\n",
      "Epoch 0:   6%|▌         | 46/779 [00:05<01:34,  7.74it/s, loss=2.31]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   6%|▌         | 47/779 [00:05<01:32,  7.87it/s, loss=2.28]____________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   6%|▌         | 48/779 [00:05<01:31,  8.01it/s, loss=2.27]____________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   6%|▋         | 49/779 [00:06<01:29,  8.13it/s, loss=2.24]____________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:   6%|▋         | 50/779 [00:06<01:28,  8.26it/s, loss=2.23]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   7%|▋         | 51/779 [00:06<01:26,  8.39it/s, loss=2.19]____________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   7%|▋         | 52/779 [00:06<01:25,  8.51it/s, loss=2.18]____________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:   7%|▋         | 53/779 [00:06<01:24,  8.63it/s, loss=2.16]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   7%|▋         | 54/779 [00:06<01:22,  8.76it/s, loss=2.13]________________________________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      " kws collate fn\n",
      "Epoch 0:   7%|▋         | 55/779 [00:06<01:21,  8.88it/s, loss=2.1] ____________________ ____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "kws collate fn\n",
      "____________________ kws collate fn\n",
      "Epoch 0:   7%|▋         | 56/779 [00:06<01:20,  8.99it/s, loss=2.06]________________________________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      " kws collate fn\n",
      "Epoch 0:   7%|▋         | 57/779 [00:06<01:19,  9.11it/s, loss=2.04]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   7%|▋         | 58/779 [00:06<01:18,  9.23it/s, loss=2.01]____________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   8%|▊         | 59/779 [00:06<01:17,  9.34it/s, loss=2]   ____________________ kws collate fn\n",
      "____________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   8%|▊         | 60/779 [00:06<01:16,  9.45it/s, loss=1.97]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   8%|▊         | 61/779 [00:06<01:15,  9.57it/s, loss=1.95]____________________ kws collate fn\n",
      "____________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   8%|▊         | 62/779 [00:06<01:14,  9.69it/s, loss=1.93]____________________ ____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "kws collate fn\n",
      "Epoch 0:   8%|▊         | 63/779 [00:06<01:13,  9.80it/s, loss=1.92]____________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   8%|▊         | 64/779 [00:06<01:12,  9.91it/s, loss=1.9] ____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   8%|▊         | 65/779 [00:06<01:11, 10.03it/s, loss=1.87]________________________________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      " kws collate fn\n",
      "____________________ kws collate fn\n",
      "Epoch 0:   8%|▊         | 66/779 [00:06<01:10, 10.13it/s, loss=1.85]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:   9%|▊         | 67/779 [00:06<01:09, 10.23it/s, loss=1.83]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   9%|▊         | 68/779 [00:06<01:08, 10.34it/s, loss=1.81]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "____________________ kws collate fn\n",
      "Epoch 0:   9%|▉         | 69/779 [00:06<01:07, 10.45it/s, loss=1.77]____________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   9%|▉         | 70/779 [00:06<01:07, 10.55it/s, loss=1.76]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   9%|▉         | 71/779 [00:06<01:06, 10.66it/s, loss=1.74]____________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   9%|▉         | 72/779 [00:06<01:05, 10.77it/s, loss=1.71] kws collate fn\n",
      "____________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:   9%|▉         | 73/779 [00:06<01:04, 10.87it/s, loss=1.68]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:   9%|▉         | 74/779 [00:06<01:04, 10.97it/s, loss=1.66]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:  10%|▉         | 75/779 [00:06<01:03, 11.07it/s, loss=1.66]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:  10%|▉         | 76/779 [00:06<01:02, 11.18it/s, loss=1.66]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:  10%|▉         | 77/779 [00:06<01:02, 11.28it/s, loss=1.64]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:  10%|█         | 78/779 [00:06<01:01, 11.38it/s, loss=1.63]____________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:  10%|█         | 79/779 [00:06<01:00, 11.48it/s, loss=1.61]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:  10%|█         | 80/779 [00:06<01:00, 11.57it/s, loss=1.59]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:  10%|█         | 81/779 [00:06<00:59, 11.67it/s, loss=1.58]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:  11%|█         | 82/779 [00:06<00:59, 11.77it/s, loss=1.56]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:  11%|█         | 83/779 [00:06<00:58, 11.87it/s, loss=1.54]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:  11%|█         | 84/779 [00:07<00:58, 11.97it/s, loss=1.52]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:  11%|█         | 85/779 [00:07<00:57, 12.06it/s, loss=1.51]____________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:  11%|█         | 86/779 [00:07<00:56, 12.16it/s, loss=1.49]____________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:  11%|█         | 87/779 [00:07<00:56, 12.26it/s, loss=1.47]____________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:  11%|█▏        | 88/779 [00:07<00:57, 12.09it/s, loss=1.46]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:  11%|█▏        | 89/779 [00:07<00:56, 12.18it/s, loss=1.46]____________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:  12%|█▏        | 90/779 [00:07<00:56, 12.28it/s, loss=1.42]____________________ kws collate fn\n",
      "____________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:  12%|█▏        | 91/779 [00:07<00:55, 12.37it/s, loss=1.42]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:  12%|█▏        | 92/779 [00:07<00:55, 12.45it/s, loss=1.42]____________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:  12%|█▏        | 93/779 [00:07<00:54, 12.54it/s, loss=1.4] ____________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:  12%|█▏        | 94/779 [00:07<00:54, 12.63it/s, loss=1.39]___________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:  12%|█▏        | 95/779 [00:07<00:53, 12.73it/s, loss=1.36]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn____________________\n",
      " kws collate fn\n",
      "Epoch 0:  12%|█▏        | 96/779 [00:07<00:53, 12.81it/s, loss=1.34]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:  12%|█▏        | 97/779 [00:07<00:52, 12.90it/s, loss=1.34]____________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:  13%|█▎        | 98/779 [00:07<00:52, 12.98it/s, loss=1.32]____________________ kws collate fn____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "\n",
      "____________________ kws collate fn\n",
      "Epoch 0:  13%|█▎        | 99/779 [00:07<00:52, 13.07it/s, loss=1.31]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:  13%|█▎        | 100/779 [00:07<00:51, 13.15it/s, loss=1.29]____________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:  13%|█▎        | 101/779 [00:07<00:51, 13.23it/s, loss=1.28]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:  13%|█▎        | 102/779 [00:07<00:50, 13.32it/s, loss=1.28]____________________ kws collate fn\n",
      "Epoch 0:  13%|█▎        | 102/779 [00:07<00:50, 13.32it/s, loss=1.26]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:  13%|█▎        | 103/779 [00:07<00:50, 13.40it/s, loss=1.27]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "____________________ kws collate fn\n",
      "Epoch 0:  13%|█▎        | 104/779 [00:07<00:50, 13.48it/s, loss=1.25]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:  13%|█▎        | 105/779 [00:07<00:49, 13.56it/s, loss=1.24]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________\n",
      "Epoch 0:  14%|█▎        | 106/779 [00:07<00:49, 13.63it/s, loss=1.24]kws collate fn____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:  14%|█▎        | 107/779 [00:07<00:49, 13.71it/s, loss=1.24] kws collate fn\n",
      "Epoch 0:  14%|█▎        | 107/779 [00:07<00:49, 13.71it/s, loss=1.22]____________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:  14%|█▍        | 108/779 [00:07<00:48, 13.79it/s, loss=1.22]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:  14%|█▍        | 109/779 [00:07<00:48, 13.87it/s, loss=1.22]____________________ kws collate fn\n",
      "____________________ kws collate fn____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:  14%|█▍        | 110/779 [00:07<00:47, 13.95it/s, loss=1.22]\n",
      "Epoch 0:  14%|█▍        | 110/779 [00:07<00:47, 13.95it/s, loss=1.22]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:  14%|█▍        | 111/779 [00:07<00:47, 14.02it/s, loss=1.21]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:  14%|█▍        | 112/779 [00:07<00:47, 14.09it/s, loss=1.2] ____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "Epoch 0:  15%|█▍        | 113/779 [00:07<00:46, 14.17it/s, loss=1.21]____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n",
      "____________________ kws collate fn\n",
      "Epoch 0:  15%|█▍        | 114/779 [00:07<00:46, 14.25it/s, loss=1.22]____________________ kws collate fn\n",
      "____________________ steps\n",
      "torch.Size([128, 1, 128, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:724: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ kws prep data\n",
      "____________________ setup \n",
      "____________________ kws prep data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:487: PossibleUserWarning: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ setup\n",
      "____________________ kws test dataloader\n",
      "Testing: 0it [00:00, ?it/s]________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________       ________________________________________  kws collate fnkws collate fn  kws collate fnkws collate fnkws collate fnkws collate fn \n",
      "kws collate fnkws collate fn\n",
      "  \n",
      "\n",
      "\n",
      "kws collate fn\n",
      "\n",
      "kws collate fn\n",
      "\n",
      "kws collate fn\n",
      "kws collate fn____________________________________________________________kws collate fn \n",
      "kws collate fn  kws collate fn\n",
      "________________________________________\n",
      "\n",
      "\n",
      "kws collate fn   kws collate fn \n",
      "kws collate fnkws collate fn\n",
      "____________________\n",
      "____________________kws collate fnkws collate fn\n",
      " ____________________ kws collate fn\n",
      " \n",
      "____________________ ________________________________________  kws collate fn\n",
      "\n",
      "kws collate fnkws collate fnkws collate fn\n",
      "\n",
      "\n",
      "____________________________________________________________kws collate fn   kws collate fn\n",
      "\n",
      "kws collate fn____________________\n",
      "kws collate fn \n",
      "____________________kws collate fn\n",
      "____________________ ____________________ kws collate fn ____________________\n",
      "____________________ \n",
      "kws collate fnkws collate fn\n",
      "____________________kws collate fn ________________________________________\n",
      "____________________  ____________________kws collate fn \n",
      "kws collate fn________________________________________\n",
      " ____________________ ____________________ kws collate fn \n",
      "\n",
      "kws collate fnkws collate fn____________________kws collate fn kws collate fn____________________  \n",
      "kws collate fn____________________\n",
      "\n",
      "\n",
      "____________________ kws collate fn kws collate fn\n",
      "____________________kws collate fnkws collate fn____________________ kws collate fn\n",
      "____________________ ________________________________________  kws collate fn  kws collate fn________________________________________kws collate fn\n",
      "____________________\n",
      "\n",
      "kws collate fn\n",
      " \n",
      " kws collate fn\n",
      "kws collate fn\n",
      "kws collate fn \n",
      "\n",
      "kws collate fn________________________________________\n",
      " kws collate fn\n",
      " kws collate fnkws collate fn\n",
      "____________________\n",
      "\n",
      " ____________________kws collate fn\n",
      "____________________________________________________________________________________________________ ____________________  kws collate fnkws collate fn ____________________________________________________________  ____________________kws collate fn\n",
      " kws collate fn\n",
      "kws collate fn\n",
      "\n",
      "  \n",
      " ____________________kws collate fnkws collate fnkws collate fn\n",
      "kws collate fn\n",
      "\n",
      "\n",
      " ________________________________________kws collate fn kws collate fn____________________ kws collate fnkws collate fn________________________________________  ____________________\n",
      "\n",
      " kws collate fn kws collate fnkws collate fn\n",
      "kws collate fn\n",
      "\n",
      " \n",
      "kws collate fn\n",
      "____________________________________________________________\n",
      "Testing DataLoader 0:   0%|          | 0/86 [00:00<?, ?it/s]____________________ kws collate fn\n",
      "\n",
      "  ____________________ test step\n",
      "Testing DataLoader 0:   1%|          | 1/86 [00:00<00:02, 39.63it/s]kws collate fn\n",
      "____________________\n",
      " ____________________ kws collate fn ____________________kws collate fn\n",
      "kws collate fn\n",
      "kws collate fn kws collate fn\n",
      "\n",
      "____________________ test step\n",
      "Testing DataLoader 0:   2%|▏         | 2/86 [00:00<00:03, 22.50it/s]\n",
      "____________________ test step\n",
      "Testing DataLoader 0:   3%|▎         | 3/86 [00:00<00:03, 26.33it/s]____________________ test step\n",
      "Testing DataLoader 0:   5%|▍         | 4/86 [00:00<00:03, 26.33it/s]____________________ test step\n",
      "Testing DataLoader 0:   6%|▌         | 5/86 [00:00<00:03, 26.33it/s]____________________ test step\n",
      "Testing DataLoader 0:   7%|▋         | 6/86 [00:00<00:03, 26.33it/s]____________________ test step\n",
      "Testing DataLoader 0:   8%|▊         | 7/86 [00:00<00:03, 26.33it/s]____________________ test step\n",
      "Testing DataLoader 0:   9%|▉         | 8/86 [00:00<00:02, 26.33it/s]____________________ test step\n",
      "Testing DataLoader 0:  10%|█         | 9/86 [00:00<00:02, 26.33it/s]____________________ test step\n",
      "Testing DataLoader 0:  12%|█▏        | 10/86 [00:00<00:01, 47.91it/s]____________________ test step\n",
      "Testing DataLoader 0:  13%|█▎        | 11/86 [00:00<00:01, 47.91it/s]____________________ test step\n",
      "Testing DataLoader 0:  14%|█▍        | 12/86 [00:00<00:01, 47.91it/s]____________________ test step\n",
      "Testing DataLoader 0:  15%|█▌        | 13/86 [00:00<00:01, 47.91it/s]____________________ test step\n",
      "Testing DataLoader 0:  16%|█▋        | 14/86 [00:00<00:01, 47.91it/s]____________________ test step\n",
      "Testing DataLoader 0:  17%|█▋        | 15/86 [00:00<00:01, 47.91it/s]____________________ test step\n",
      "Testing DataLoader 0:  19%|█▊        | 16/86 [00:00<00:01, 47.91it/s]____________________ test step\n",
      "Testing DataLoader 0:  20%|█▉        | 17/86 [00:00<00:01, 47.91it/s]____________________ test step\n",
      "Testing DataLoader 0:  21%|██        | 18/86 [00:00<00:01, 60.16it/s]____________________ test step\n",
      "Testing DataLoader 0:  22%|██▏       | 19/86 [00:00<00:01, 60.16it/s]____________________ test step\n",
      "Testing DataLoader 0:  23%|██▎       | 20/86 [00:00<00:01, 60.16it/s]____________________ test step\n",
      "Testing DataLoader 0:  24%|██▍       | 21/86 [00:00<00:01, 60.16it/s]____________________ test step\n",
      "Testing DataLoader 0:  26%|██▌       | 22/86 [00:00<00:01, 60.16it/s]____________________ test step\n",
      "Testing DataLoader 0:  27%|██▋       | 23/86 [00:00<00:01, 60.16it/s]____________________ test step\n",
      "Testing DataLoader 0:  28%|██▊       | 24/86 [00:00<00:01, 60.16it/s]____________________ test step\n",
      "Testing DataLoader 0:  29%|██▉       | 25/86 [00:00<00:01, 60.16it/s]____________________ test step\n",
      "Testing DataLoader 0:  30%|███       | 26/86 [00:00<00:00, 60.16it/s]____________________ test step\n",
      "Testing DataLoader 0:  31%|███▏      | 27/86 [00:00<00:00, 60.16it/s]____________________ test step\n",
      "Testing DataLoader 0:  33%|███▎      | 28/86 [00:00<00:00, 60.16it/s]____________________ test step\n",
      "Testing DataLoader 0:  34%|███▎      | 29/86 [00:00<00:00, 76.68it/s]____________________ test step\n",
      "Testing DataLoader 0:  35%|███▍      | 30/86 [00:00<00:00, 76.68it/s]____________________ test step\n",
      "Testing DataLoader 0:  36%|███▌      | 31/86 [00:00<00:00, 76.68it/s]____________________ test step\n",
      "Testing DataLoader 0:  37%|███▋      | 32/86 [00:00<00:00, 76.68it/s]____________________ test step\n",
      "Testing DataLoader 0:  38%|███▊      | 33/86 [00:00<00:00, 76.68it/s]____________________ test step\n",
      "Testing DataLoader 0:  40%|███▉      | 34/86 [00:00<00:00, 76.68it/s]____________________ test step\n",
      "Testing DataLoader 0:  41%|████      | 35/86 [00:00<00:00, 76.68it/s]____________________ test step\n",
      "Testing DataLoader 0:  42%|████▏     | 36/86 [00:00<00:00, 76.68it/s]____________________ test step\n",
      "Testing DataLoader 0:  43%|████▎     | 37/86 [00:00<00:00, 76.68it/s]____________________ test step\n",
      "Testing DataLoader 0:  44%|████▍     | 38/86 [00:00<00:00, 76.68it/s]____________________ test step\n",
      "Testing DataLoader 0:  45%|████▌     | 39/86 [00:00<00:00, 76.68it/s]____________________ test step\n",
      "Testing DataLoader 0:  47%|████▋     | 40/86 [00:00<00:00, 76.68it/s]____________________ test step\n",
      "Testing DataLoader 0:  48%|████▊     | 41/86 [00:00<00:00, 76.68it/s]____________________ test step\n",
      "Testing DataLoader 0:  49%|████▉     | 42/86 [00:00<00:00, 76.68it/s]____________________ test step\n",
      "Testing DataLoader 0:  50%|█████     | 43/86 [00:00<00:00, 95.77it/s]____________________ test step\n",
      "Testing DataLoader 0:  51%|█████     | 44/86 [00:00<00:00, 95.77it/s]____________________ test step\n",
      "Testing DataLoader 0:  52%|█████▏    | 45/86 [00:00<00:00, 95.77it/s]____________________ test step\n",
      "Testing DataLoader 0:  53%|█████▎    | 46/86 [00:00<00:00, 95.77it/s]____________________ test step\n",
      "Testing DataLoader 0:  55%|█████▍    | 47/86 [00:00<00:00, 95.77it/s]____________________ test step\n",
      "Testing DataLoader 0:  56%|█████▌    | 48/86 [00:00<00:00, 95.77it/s]____________________ test step\n",
      "Testing DataLoader 0:  57%|█████▋    | 49/86 [00:00<00:00, 95.77it/s]____________________ test step\n",
      "Testing DataLoader 0:  58%|█████▊    | 50/86 [00:00<00:00, 95.77it/s]____________________ test step\n",
      "Testing DataLoader 0:  59%|█████▉    | 51/86 [00:00<00:00, 95.77it/s]____________________ test step\n",
      "Testing DataLoader 0:  60%|██████    | 52/86 [00:00<00:00, 95.77it/s]____________________ test step\n",
      "Testing DataLoader 0:  62%|██████▏   | 53/86 [00:00<00:00, 95.77it/s]____________________ test step\n",
      "Testing DataLoader 0:  63%|██████▎   | 54/86 [00:00<00:00, 95.77it/s]____________________ test step\n",
      "Testing DataLoader 0:  64%|██████▍   | 55/86 [00:00<00:00, 95.77it/s]____________________ test step\n",
      "Testing DataLoader 0:  65%|██████▌   | 56/86 [00:00<00:00, 105.63it/s]____________________ test step\n",
      "Testing DataLoader 0:  66%|██████▋   | 57/86 [00:00<00:00, 105.63it/s]____________________ test step\n",
      "Testing DataLoader 0:  67%|██████▋   | 58/86 [00:00<00:00, 105.63it/s]____________________ test step\n",
      "Testing DataLoader 0:  69%|██████▊   | 59/86 [00:00<00:00, 105.63it/s]____________________ test step\n",
      "Testing DataLoader 0:  70%|██████▉   | 60/86 [00:00<00:00, 105.63it/s]____________________ test step\n",
      "Testing DataLoader 0:  71%|███████   | 61/86 [00:00<00:00, 105.63it/s]____________________ test step\n",
      "Testing DataLoader 0:  72%|███████▏  | 62/86 [00:00<00:00, 105.63it/s]____________________ test step\n",
      "Testing DataLoader 0:  73%|███████▎  | 63/86 [00:00<00:00, 105.63it/s]____________________ test step\n",
      "Testing DataLoader 0:  74%|███████▍  | 64/86 [00:00<00:00, 105.63it/s]____________________ test step\n",
      "Testing DataLoader 0:  76%|███████▌  | 65/86 [00:00<00:00, 105.63it/s]____________________ test step\n",
      "Testing DataLoader 0:  77%|███████▋  | 66/86 [00:00<00:00, 105.63it/s]____________________ test step\n",
      "Testing DataLoader 0:  78%|███████▊  | 67/86 [00:00<00:00, 105.63it/s]____________________ test step\n",
      "Testing DataLoader 0:  79%|███████▉  | 68/86 [00:00<00:00, 105.63it/s]____________________ test step\n",
      "Testing DataLoader 0:  80%|████████  | 69/86 [00:00<00:00, 105.63it/s]____________________ test step\n",
      "Testing DataLoader 0:  81%|████████▏ | 70/86 [00:00<00:00, 113.53it/s]____________________ test step\n",
      "Testing DataLoader 0:  83%|████████▎ | 71/86 [00:00<00:00, 113.53it/s]____________________ test step\n",
      "Testing DataLoader 0:  84%|████████▎ | 72/86 [00:00<00:00, 113.53it/s]____________________ test step\n",
      "Testing DataLoader 0:  85%|████████▍ | 73/86 [00:00<00:00, 113.53it/s]____________________ test step\n",
      "Testing DataLoader 0:  86%|████████▌ | 74/86 [00:00<00:00, 113.53it/s]____________________ test step\n",
      "Testing DataLoader 0:  87%|████████▋ | 75/86 [00:00<00:00, 113.53it/s]____________________ test step\n",
      "Testing DataLoader 0:  88%|████████▊ | 76/86 [00:00<00:00, 113.53it/s]____________________ test step\n",
      "Testing DataLoader 0:  90%|████████▉ | 77/86 [00:00<00:00, 113.53it/s]____________________ test step\n",
      "Testing DataLoader 0:  91%|█████████ | 78/86 [00:00<00:00, 113.53it/s]____________________ test step\n",
      "Testing DataLoader 0:  92%|█████████▏| 79/86 [00:00<00:00, 113.53it/s]____________________ test step\n",
      "Testing DataLoader 0:  93%|█████████▎| 80/86 [00:00<00:00, 113.53it/s]____________________ test step\n",
      "Testing DataLoader 0:  94%|█████████▍| 81/86 [00:00<00:00, 113.53it/s]____________________ test step\n",
      "Testing DataLoader 0:  95%|█████████▌| 82/86 [00:00<00:00, 114.00it/s]____________________ test step\n",
      "Testing DataLoader 0:  97%|█████████▋| 83/86 [00:00<00:00, 114.00it/s]____________________ test step\n",
      "Testing DataLoader 0:  98%|█████████▊| 84/86 [00:00<00:00, 114.00it/s]____________________ test step\n",
      "Testing DataLoader 0:  99%|█████████▉| 85/86 [00:00<00:00, 114.00it/s]____________________ test step\n",
      "Testing DataLoader 0: 100%|██████████| 86/86 [00:01<00:00, 114.00it/s]____________________ test ep end\n",
      "Testing DataLoader 0: 100%|██████████| 86/86 [00:01<00:00, 84.90it/s] \n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc             65.24382019042969\n",
      "        test_loss           1.1606817245483398\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">bumbling-water-41</strong>: <a href=\"https://wandb.ai/alessandrosantiago/pl-kws/runs/3mjhtw2m\" target=\"_blank\">https://wandb.ai/alessandrosantiago/pl-kws/runs/3mjhtw2m</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220530_045443-3mjhtw2m/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    args = get_args()\n",
    "    CLASSES = ['silence', 'unknown', 'backward', 'bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'follow',\n",
    "               'forward', 'four', 'go', 'happy', 'house', 'learn', 'left', 'marvin', 'nine', 'no',\n",
    "               'off', 'on', 'one', 'right', 'seven', 'sheila', 'six', 'stop', 'three',\n",
    "               'tree', 'two', 'up', 'visual', 'wow', 'yes', 'zero']\n",
    "    \n",
    "    # make a dictionary from CLASSES to integers\n",
    "    CLASS_TO_IDX = {c: i for i, c in enumerate(CLASSES)}\n",
    "\n",
    "    if not os.path.exists(args.path):\n",
    "        os.makedirs(args.path, exist_ok=True)\n",
    "\n",
    "    model = KWSModel(num_classes=args.num_classes, epochs=args.max_epochs, lr=args.lr)\n",
    "    print(model)\n",
    "\n",
    "\n",
    "\n",
    "    # datamodule = KWSDataModule(batch_size=args.batch_size, num_workers=args.num_workers,\n",
    "    #                            path=args.path, n_fft=args.n_fft, n_mels=args.n_mels,\n",
    "    #                            win_length=args.win_length, hop_length=args.hop_length,\n",
    "    #                            class_dict=CLASS_TO_IDX)\n",
    "    # datamodule.setup()\n",
    "\n",
    "\n",
    "\n",
    "    # wandb is a great way to debug and visualize this model\n",
    "    wandb_logger = WandbLogger(project=\"pl-kws\")\n",
    "\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        dirpath=os.path.join(args.path, \"checkpoints\"),\n",
    "        filename=\"resnet18-kws-best-acc\",\n",
    "        save_top_k=1,\n",
    "        verbose=True,\n",
    "        monitor='test_acc',\n",
    "        mode='max',\n",
    "    )\n",
    "    idx_to_class = {v: k for k, v in CLASS_TO_IDX.items()}\n",
    "    trainer = Trainer(accelerator=args.accelerator,\n",
    "                      devices=args.devices,\n",
    "                      precision=args.precision,\n",
    "                      max_epochs=args.max_epochs,\n",
    "                      logger=wandb_logger if not args.no_wandb else None,\n",
    "                      callbacks=[model_checkpoint])\n",
    "    model.hparams.sample_rate = datamodule.sample_rate\n",
    "    model.hparams.idx_to_class = idx_to_class\n",
    "    trainer.fit(model, datamodule=datamodule)\n",
    "    trainer.test(model, datamodule=datamodule)\n",
    "\n",
    "    wandb.finish()\n",
    "    #trainer.save_checkpoint('../mnist/checkpoint.ckpt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/dl/Desktop/dl/object_detection_model_hw2/hw3/train.ipynb Cell 25'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/train.ipynb#ch0000019?line=0'>1</a>\u001b[0m \u001b[39m# https://pytorch-lightning.readthedocs.io/en/stable/common/production_inference.html\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/train.ipynb#ch0000019?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mload_from_checkpoint(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/train.ipynb#ch0000019?line=2'>3</a>\u001b[0m     args\u001b[39m.\u001b[39mpath, \u001b[39m\"\u001b[39m\u001b[39mcheckpoints\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mresnet18-kws-best-acc.ckpt\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/train.ipynb#ch0000019?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/train.ipynb#ch0000019?line=4'>5</a>\u001b[0m script \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto_torchscript()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# https://pytorch-lightning.readthedocs.io/en/stable/common/production_inference.html\n",
    "model = model.load_from_checkpoint(os.path.join(\n",
    "    args.path, \"checkpoints\", \"resnet18-kws-best-acc.ckpt\"))\n",
    "model.eval()\n",
    "script = model.to_torchscript()\n",
    "\n",
    "# save for use in production environment\n",
    "model_path = os.path.join(args.path, \"checkpoints\",\n",
    "                          \"resnet18-kws-best-acc.pt\")\n",
    "torch.jit.save(script, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# list wav files given a folder\n",
    "label = CLASSES[2:]\n",
    "label = np.random.choice(label)\n",
    "path = os.path.join(args.path, \"SpeechCommands/speech_commands_v0.02/\")\n",
    "path = os.path.join(path, label)\n",
    "wav_files = [os.path.join(path, f)\n",
    "             for f in os.listdir(path) if f.endswith('.wav')]\n",
    "# select random wav file\n",
    "wav_file = np.random.choice(wav_files)\n",
    "waveform, sample_rate = torchaudio.load(wav_file)\n",
    "transform = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate,\n",
    "                                                 n_fft=args.n_fft,\n",
    "                                                 win_length=args.win_length,\n",
    "                                                 hop_length=args.hop_length,\n",
    "                                                 n_mels=args.n_mels,\n",
    "                                                 power=2.0)\n",
    "\n",
    "mel = ToTensor()(librosa.power_to_db(\n",
    "    transform(waveform).squeeze().numpy(), ref=np.max))\n",
    "mel = mel.unsqueeze(0)\n",
    "# scripted_module = torch.jit.load(model_path)\n",
    "# pred = torch.argmax(scripted_module(mel), dim=1)\n",
    "# print(f\"Ground Truth: {label}, Prediction: {idx_to_class[pred.item()]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel = ToTensor()(librosa.power_to_db(\n",
    "    transform(waveform).squeeze().numpy(), ref=np.max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 32])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel = mel.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-55.9640, -80.0000, -70.9560,  ..., -50.1307, -43.0756, -37.5130],\n",
       "          [-52.5513, -61.7541, -62.1342,  ..., -54.5414, -39.6732, -39.5243],\n",
       "          [-52.3738, -53.8485, -54.9334,  ..., -42.0820, -43.3070, -43.0445],\n",
       "          ...,\n",
       "          [-66.3301, -67.9963, -72.3319,  ..., -65.8876, -69.3849, -67.3708],\n",
       "          [-70.0317, -72.7043, -75.6694,  ..., -65.5525, -69.2419, -67.4310],\n",
       "          [-73.3275, -76.8653, -78.0272,  ..., -70.0991, -71.6350, -72.7343]]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sample_rate\n",
    "\n",
    "    figure, axes = plt.subplots(num_channels, 1)\n",
    "    if num_channels == 1:\n",
    "        axes = [axes]\n",
    "    for c in range(num_channels):\n",
    "        axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
    "        axes[c].grid(True)\n",
    "        if num_channels > 1:\n",
    "            axes[c].set_ylabel(f'Channel {c+1}')\n",
    "        if xlim:\n",
    "            axes[c].set_xlim(xlim)\n",
    "        if ylim:\n",
    "            axes[c].set_ylim(ylim)\n",
    "    figure.suptitle(title)\n",
    "    plt.show(block=False)\n",
    "\n",
    "plot_waveform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
