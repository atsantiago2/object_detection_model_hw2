{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer for CIFAR10\n",
    "\n",
    "A configurable transformer model will be used for CIFAR10 image classification. \n",
    "\n",
    "The vision transformer model is a modified version of ViT. The changes are:\n",
    "1) No position embedding.\n",
    "2) No dropout is used.\n",
    "3) All encoder features are used for class prediction.\n",
    "\n",
    "The code below is a simplified version of Timm modules.\n",
    "\n",
    "Let us import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dl/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio, torchvision\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from pytorch_lightning import LightningModule, Trainer, LightningDataModule\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchmetrics.functional import accuracy\n",
    "from einops import rearrange\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Module\n",
    "\n",
    "The `Attention` module is the core of the vision transformer model. It implements the attention mechanism:\n",
    "\n",
    "1) Multiply QKV by their weights\n",
    "2) Perform dot product on Q and K. \n",
    "3) Normalize the result in 2) by sqrt of `head_dim`  \n",
    "4) Softmax is applied to the result.\n",
    "5) Perform dot product on the result of 4) and V and the result is the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Module\n",
    "\n",
    "The MLP module is a made of two linear layers. A non-linear activation is applied to the output of the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "      \n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Block Module\n",
    "\n",
    "The `Block` module represents one encoder transformer block. It consists of two sub-modules:\n",
    "1) The Attention module\n",
    "2) The MLP module\n",
    "\n",
    "Layer norm is applied before and after the Attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, dim, num_heads, mlp_ratio=4., qkv_bias=False, \n",
    "            act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias) \n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer) \n",
    "   \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Transformer Module\n",
    "\n",
    "The feature encoder is made of several transformer blocks. The most important attributes are:\n",
    "1) `depth` : representing the number of encoder blocks\n",
    "2) `num_heads` : representing the number of attention heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_blocks, mlp_ratio=4., qkv_bias=False,  \n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([Block(dim, num_heads, mlp_ratio, qkv_bias, \n",
    "                                     act_layer, norm_layer) for _ in range(num_blocks)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The optional parameter initialization as adopted from `timm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights_vit_timm(module: nn.Module):\n",
    "    \"\"\" ViT weight initialization, original timm impl (for reproducibility) \"\"\"\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.trunc_normal_(module.weight, mean=0.0, std=0.02)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif hasattr(module, 'init_weights'):\n",
    "        module.init_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Lightning for CIFAR10 Image Classification\n",
    "\n",
    "We use the `Transformer` module to build the feature encoder. Before the `Transformer` can be used, we convert the input image into patches. The patches are then embedded into a linear space. The output is then passed to the Transformer.\n",
    "\n",
    "Another difference between this model is we use all output features for the final classification. In the ViT, only the first feature is used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitTransformer(LightningModule):\n",
    "    def __init__(self, num_classes=10, lr=0.001, max_epochs=30, depth=12, embed_dim=64,\n",
    "                 head=4, patch_dim=192, seqlen=16, **kwargs):\n",
    "        super().__init__()\n",
    "        print('_'*20, 'init')\n",
    "        self.save_hyperparameters()\n",
    "        self.encoder = Transformer(dim=embed_dim, num_heads=head, num_blocks=depth, mlp_ratio=4.,\n",
    "                                   qkv_bias=False, act_layer=nn.GELU, norm_layer=nn.LayerNorm)\n",
    "        self.embed = torch.nn.Linear(patch_dim, embed_dim)\n",
    "\n",
    "        self.fc = nn.Linear(seqlen * embed_dim, num_classes)\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        print('_'*20, 'reset param')\n",
    "        init_weights_vit_timm(self)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # print('_'*20, 'fwd')\n",
    "        # Linear projection\n",
    "        x = self.embed(x)\n",
    "            \n",
    "        # Encoder\n",
    "        x = self.encoder(x)\n",
    "        x = x.flatten(start_dim=1)\n",
    "\n",
    "        # Classification head\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        print('_'*20, 'config optimizers')\n",
    "        optimizer = Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        # this decays the learning rate to 0 after max_epochs using cosine annealing\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=self.hparams.max_epochs)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # print('_'*20, 'steps')\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # print('_'*20, 'test step')\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        acc = accuracy(y_hat, y)\n",
    "        return {\"y_hat\": y_hat, \"test_loss\": loss, \"test_acc\": acc}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        # print('_'*20, 'test ep end')\n",
    "        avg_loss = torch.stack([x[\"test_loss\"] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x[\"test_acc\"] for x in outputs]).mean()\n",
    "        self.log(\"test_loss\", avg_loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_acc\", avg_acc*100., on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # print('_'*20, 'valid step')\n",
    "        return self.test_step(batch, batch_idx)\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # print('_'*20, 'vald epoc end')\n",
    "        return self.test_epoch_end(outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor\n",
    "from torchaudio.datasets import SPEECHCOMMANDS\n",
    "from torchaudio.datasets.speechcommands import load_speechcommands_item\n",
    "from pytorch_lightning import LightningModule, Trainer, LightningDataModule, Callback\n",
    "\n",
    "\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SilenceDataset(SPEECHCOMMANDS):\n",
    "    def __init__(self, root):\n",
    "        super(SilenceDataset, self).__init__(root, subset='training')\n",
    "        self.len = len(self._walker) // 35\n",
    "        path = os.path.join(self._path, torchaudio.datasets.speechcommands.EXCEPT_FOLDER)\n",
    "        self.paths = [os.path.join(path, p) for p in os.listdir(path) if p.endswith('.wav')]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = np.random.randint(0, len(self.paths))\n",
    "        filepath = self.paths[index]\n",
    "        waveform, sample_rate = torchaudio.load(filepath)\n",
    "        return waveform, sample_rate, \"silence\", 0, 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "class UnknownDataset(SPEECHCOMMANDS):\n",
    "    def __init__(self, root):\n",
    "        super(UnknownDataset, self).__init__(root, subset='training')\n",
    "        self.len = len(self._walker) // 35\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = np.random.randint(0, len(self._walker))\n",
    "        fileid = self._walker[index]\n",
    "        waveform, sample_rate, _, speaker_id, utterance_number = load_speechcommands_item(fileid, self._path)\n",
    "        return waveform, sample_rate, \"unknown\", speaker_id, utterance_number\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# a lightning data module for cifar 10 dataset\n",
    "class LitCifar10(LightningDataModule):\n",
    "    def __init__(self, path, batch_size=32, num_workers=32, patch_num=4\n",
    "                , n_fft=512, n_mels=128, win_length=None, hop_length=256\n",
    "                , class_dict={}\n",
    "                , **kwargs):\n",
    "    # def __init__(self, batch_size=32, num_workers=32, patch_num=4, **kwargs):\n",
    "        \n",
    "        super().__init__()\n",
    "        print('_'*20, 'datamodule init')\n",
    "        self.path = path\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.patch_num = patch_num\n",
    "\n",
    "        # Window\n",
    "        self.n_fft = n_fft\n",
    "        self.n_mels = n_mels\n",
    "        self.win_length = win_length\n",
    "        self.hop_length = hop_length\n",
    "        self.class_dict = class_dict\n",
    "\n",
    "    def prepare_data(self):\n",
    "        print('_'*20, 'datamodule preparing data')\n",
    "        print('\\n{}'.format(self.path))\n",
    "        #________________________________\n",
    "        self.train_dataset = torchaudio.datasets.SPEECHCOMMANDS(self.path, download=True,subset='training')\n",
    "        # self.train_set = CIFAR10(root='./data', train=True,download=True, transform=torchvision.transforms.ToTensor())\n",
    "        silence_dataset = SilenceDataset(self.path)\n",
    "        unknown_dataset = UnknownDataset(self.path)\n",
    "        self.train_set = torch.utils.data.ConcatDataset([self.train_dataset, silence_dataset, unknown_dataset])\n",
    "\n",
    "        #________________________________\n",
    "        self.val_dataset = torchaudio.datasets.SPEECHCOMMANDS(self.path,download=True,subset='validation')\n",
    "        # self.val_set = torchaudio.datasets.SPEECHCOMMANDS(self.path,download=True,subset='validation')\n",
    "        \n",
    "        \n",
    "        #________________________________\n",
    "        self.test_set = torchaudio.datasets.SPEECHCOMMANDS(self.path, download=True, subset='testing')      \n",
    "        # self.test_set = CIFAR10(root='./data', train=False,download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        _, sample_rate, _, _, _ = self.train_dataset[0]\n",
    "        self.sample_rate = sample_rate\n",
    "        self.transform = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate,\n",
    "                                                              n_fft=self.n_fft,\n",
    "                                                              win_length=self.win_length,\n",
    "                                                              hop_length=self.hop_length,\n",
    "                                                              n_mels=self.n_mels,\n",
    "                                                              power=2.0)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        print('_'*20, 'setup ')\n",
    "        self.prepare_data()\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        print('_'*20, 'datamodule train dataloader')\n",
    "        return torch.utils.data.DataLoader( self.train_set, \n",
    "                                            batch_size=self.batch_size,\n",
    "                                            num_workers=self.num_workers,\n",
    "                                            shuffle=True, \n",
    "                                            # pin_memory=True,\n",
    "                                            collate_fn=self.collate_fn)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        print('_'*20, 'datamodule test dataload')\n",
    "        return torch.utils.data.DataLoader( self.test_set, \n",
    "                                            batch_size=self.batch_size,\n",
    "                                            num_workers=self.num_workers, \n",
    "                                            shuffle=False, \n",
    "                                            # pin_memory=True,\n",
    "                                            collate_fn=self.collate_fn)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        print('_'*20, 'kws val dataloader')\n",
    "        return torch.utils.data.DataLoader( self.val_dataset,\n",
    "                                            batch_size=self.batch_size,\n",
    "                                            num_workers=self.num_workers,\n",
    "                                            shuffle=True,\n",
    "                                            pin_memory=True,\n",
    "                                            collate_fn=self.collate_fn\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        mels = []\n",
    "        xmels = []\n",
    "        labels = []\n",
    "        wavs = []\n",
    "        for sample in batch:\n",
    "            waveform, sample_rate, label, speaker_id, utterance_number = sample\n",
    "            # ensure that all waveforms are 1sec in length; if not pad with zeros\n",
    "            if waveform.shape[-1] < sample_rate:\n",
    "                waveform = torch.cat([waveform, torch.zeros((1, sample_rate - waveform.shape[-1]))], dim=-1)\n",
    "            elif waveform.shape[-1] > sample_rate:\n",
    "                waveform = waveform[:,:sample_rate]\n",
    "\n",
    "            # mel from power to db\n",
    "            mel1 = ToTensor()(librosa.power_to_db(self.transform(waveform).squeeze().numpy(), ref=np.max))\n",
    "\n",
    "            # print('mel1 shapre', mel1.shape)\n",
    "            # mel1 = rearrange(mel1, 'c (p1 h) (p2 w) -> 1 (p1 p2) (c h w)', p1=self.patch_num, p2=self.patch_num)\n",
    "            # print('mel1 shapre', mel1.shape)\n",
    "\n",
    "            xmels.append(xmels)\n",
    "            mels.append(mel1)\n",
    "            labels.append(torch.tensor(self.class_dict[label]))\n",
    "            wavs.append(waveform)\n",
    "\n",
    "        mels = torch.stack(mels)\n",
    "        labels = torch.stack(labels)\n",
    "        wavs = torch.stack(wavs)\n",
    "\n",
    "        # print('mels sh:', mels.shape)\n",
    "        x = rearrange(mels, 'b 1 h w -> b h w')\n",
    "   \n",
    "        # print('x sh:', x.shape)\n",
    "        # return x, labels, wavs, sample_rate\n",
    "\n",
    "        return x, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_BATCH_SIZE = 128\n",
    "# DEFAULT_BATCH_SIZE = 4 #for debugging\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "def get_args():\n",
    "    parser = ArgumentParser(description='PyTorch Transformer')\n",
    "    parser.add_argument('--depth', type=int, default=12, help='depth')\n",
    "    parser.add_argument('--embed_dim', type=int, default=64, help='embedding dimension')\n",
    "    parser.add_argument('--num_heads', type=int, default=4, help='num_heads')\n",
    "\n",
    "\n",
    "    parser.add_argument('--patch_num', type=int, default=8, help='patch_num')\n",
    "    parser.add_argument('--kernel_size', type=int, default=3, help='kernel size')\n",
    "    parser.add_argument('--batch-size', type=int, default=DEFAULT_BATCH_SIZE, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--max-epochs', type=int, default=30, metavar='N',\n",
    "                        help='number of epochs to train (default: 30)')\n",
    "    parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
    "                        help='learning rate (default: 0.001)')\n",
    "\n",
    "    parser.add_argument('--accelerator', default='gpu', type=str, metavar='N')\n",
    "    parser.add_argument('--devices', default=1, type=int, metavar='N')\n",
    "    # parser.add_argument('--dataset', default='cifar10', type=str, metavar='N')\n",
    "    parser.add_argument('--num_workers', default=12, type=int, metavar='N')\n",
    "\n",
    "\n",
    "    # where dataset will be stored\n",
    "    parser.add_argument(\"--path\", type=str, default=\"data/speech_commands/\")\n",
    "\n",
    "    # 35 keywords + silence + unknown\n",
    "    parser.add_argument(\"--num-classes\", type=int, default=37)\n",
    "\n",
    "    # mel spectrogram parameters\n",
    "    parser.add_argument(\"--n-fft\", type=int, default=1024)\n",
    "    parser.add_argument(\"--n-mels\", type=int, default=128)\n",
    "    parser.add_argument(\"--win-length\", type=int, default=None)\n",
    "    parser.add_argument(\"--hop-length\", type=int, default=512)\n",
    "    # parser.add_argument(\"--hop-length\", type=int, default=334)\n",
    "\n",
    "\n",
    "    # 16-bit fp model to reduce the size\n",
    "    parser.add_argument(\"--precision\", default=16)\n",
    "    # parser.add_argument(\"--accelerator\", default='gpu')\n",
    "    # parser.add_argument(\"--devices\", default=1)\n",
    "    # parser.add_argument(\"--num-workers\", type=int, default=48)\n",
    "\n",
    "    parser.add_argument(\"--no-wandb\", default=True, action='store_true')\n",
    "\n",
    "    \n",
    "    args = parser.parse_args(\"\")\n",
    "    return args\n",
    "args = get_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "CLASSES = ['silence', 'unknown', 'backward', 'bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'follow',\n",
    "            'forward', 'four', 'go', 'happy', 'house', 'learn', 'left', 'marvin', 'nine', 'no',\n",
    "            'off', 'on', 'one', 'right', 'seven', 'sheila', 'six', 'stop', 'three',\n",
    "            'tree', 'two', 'up', 'visual', 'wow', 'yes', 'zero']\n",
    "\n",
    "# make a dictionary from CLASSES to integers\n",
    "CLASS_TO_IDX = {c: i for i, c in enumerate(CLASSES)}\n",
    "\n",
    "if not os.path.exists(args.path):\n",
    "    os.makedirs(args.path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n",
    "#     waveform = waveform.numpy()\n",
    "\n",
    "#     num_channels, num_frames = waveform.shape\n",
    "#     time_axis = torch.arange(0, num_frames) / sample_rate\n",
    "\n",
    "#     figure, axes = plt.subplots(num_channels, 1)\n",
    "#     if num_channels == 1:\n",
    "#         axes = [axes]\n",
    "#     for c in range(num_channels):\n",
    "#         axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
    "#         axes[c].grid(True)\n",
    "#         if num_channels > 1:\n",
    "#             axes[c].set_ylabel(f'Channel {c+1}')\n",
    "#         if xlim:\n",
    "#             axes[c].set_xlim(xlim)\n",
    "#         if ylim:\n",
    "#             axes[c].set_ylim(ylim)\n",
    "#     figure.suptitle(title)\n",
    "#     plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on different settings\n",
    "\n",
    "The following table shows different performances on different settings. Generally, Transformer is better than MLP in terms of accuracy and parameter count. However, the performance is worse compared to CNN models. \n",
    "\n",
    "However, the most important thing to note is that Transformers are more general purpose models than CNNs. They can process different types of data. They can process multiple types of data at the same time. This is why they are considered to be the backbone of many high-performing models like BERT, GPT3, PalM and Gato.\n",
    "\n",
    "| **Depth** | **Head** | **Embed dim** | **Patch size** | **Seq len** | **Params** | **Accuracy** | \n",
    "| -: | -: | -: | -: | -: | -: | -: |\n",
    "| 12 | 4 | 32 | 4x4 | 64 | 173k | 68.2% |\n",
    "| 12 | 4 | 64 | 4x4 | 64 | 641k | 71.1% |\n",
    "| 12 | 4 | 128 | 4x4 | 64 | 2.5M | 71.5% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ datamodule init\n",
      "____________________ setup \n",
      "____________________ datamodule preparing data\n",
      "\n",
      "data/speech_commands/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 269M/2.26G [00:18<02:23, 15.0MB/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/dl/Desktop/dl/object_detection_model_hw2/recreate/transformer_demo_modified.ipynb Cell 23'\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/recreate/transformer_demo_modified.ipynb#ch0000022?line=2'>3</a>\u001b[0m \u001b[39m# datamodule = LitCifar10(batch_size=args.batch_size,\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/recreate/transformer_demo_modified.ipynb#ch0000022?line=3'>4</a>\u001b[0m \u001b[39m#                         patch_num=args.patch_num, \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/recreate/transformer_demo_modified.ipynb#ch0000022?line=4'>5</a>\u001b[0m \u001b[39m#                         num_workers=args.num_workers * args.devices)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/recreate/transformer_demo_modified.ipynb#ch0000022?line=5'>6</a>\u001b[0m datamodule \u001b[39m=\u001b[39m LitCifar10(batch_size\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mbatch_size,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/recreate/transformer_demo_modified.ipynb#ch0000022?line=6'>7</a>\u001b[0m                         num_workers\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mnum_workers \u001b[39m*\u001b[39m args\u001b[39m.\u001b[39mdevices,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/recreate/transformer_demo_modified.ipynb#ch0000022?line=7'>8</a>\u001b[0m                         path\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mpath,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/recreate/transformer_demo_modified.ipynb#ch0000022?line=14'>15</a>\u001b[0m                         class_dict\u001b[39m=\u001b[39mCLASS_TO_IDX\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/recreate/transformer_demo_modified.ipynb#ch0000022?line=15'>16</a>\u001b[0m                         )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/recreate/transformer_demo_modified.ipynb#ch0000022?line=16'>17</a>\u001b[0m datamodule\u001b[39m.\u001b[39;49msetup()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/recreate/transformer_demo_modified.ipynb#ch0000022?line=18'>19</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(datamodule\u001b[39m.\u001b[39mtrain_dataloader())\u001b[39m.\u001b[39mnext()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/recreate/transformer_demo_modified.ipynb#ch0000022?line=19'>20</a>\u001b[0m patch_dim \u001b[39m=\u001b[39m data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[1;32m/home/dl/Desktop/dl/object_detection_model_hw2/recreate/transformer_demo_modified.ipynb Cell 18'\u001b[0m in \u001b[0;36mLitCifar10.setup\u001b[0;34m(self, stage)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/recreate/transformer_demo_modified.ipynb#ch0000017?line=54'>55</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msetup\u001b[39m(\u001b[39mself\u001b[39m, stage\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/recreate/transformer_demo_modified.ipynb#ch0000017?line=55'>56</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m20\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msetup \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/recreate/transformer_demo_modified.ipynb#ch0000017?line=56'>57</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_data()\n",
      "\u001b[1;32m/home/dl/Desktop/dl/object_detection_model_hw2/recreate/transformer_demo_modified.ipynb Cell 18'\u001b[0m in \u001b[0;36mLitCifar10.prepare_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/recreate/transformer_demo_modified.ipynb#ch0000017?line=25'>26</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpath))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/recreate/transformer_demo_modified.ipynb#ch0000017?line=26'>27</a>\u001b[0m \u001b[39m#________________________________\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/recreate/transformer_demo_modified.ipynb#ch0000017?line=27'>28</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_dataset \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39;49mdatasets\u001b[39m.\u001b[39;49mSPEECHCOMMANDS(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpath, download\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,subset\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtraining\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/recreate/transformer_demo_modified.ipynb#ch0000017?line=28'>29</a>\u001b[0m \u001b[39m# self.train_set = CIFAR10(root='./data', train=True,download=True, transform=torchvision.transforms.ToTensor())\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/recreate/transformer_demo_modified.ipynb#ch0000017?line=29'>30</a>\u001b[0m silence_dataset \u001b[39m=\u001b[39m SilenceDataset(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpath)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchaudio/datasets/speechcommands.py:112\u001b[0m, in \u001b[0;36mSPEECHCOMMANDS.__init__\u001b[0;34m(self, root, url, folder_in_archive, download, subset)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torchaudio/datasets/speechcommands.py?line=109'>110</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(archive):\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torchaudio/datasets/speechcommands.py?line=110'>111</a>\u001b[0m             checksum \u001b[39m=\u001b[39m _CHECKSUMS\u001b[39m.\u001b[39mget(url, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> <a href='file:///home/dl/.local/lib/python3.10/site-packages/torchaudio/datasets/speechcommands.py?line=111'>112</a>\u001b[0m             download_url_to_file(url, archive, hash_prefix\u001b[39m=\u001b[39;49mchecksum)\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torchaudio/datasets/speechcommands.py?line=112'>113</a>\u001b[0m         extract_archive(archive, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path)\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torchaudio/datasets/speechcommands.py?line=114'>115</a>\u001b[0m \u001b[39mif\u001b[39;00m subset \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/hub.py:479\u001b[0m, in \u001b[0;36mdownload_url_to_file\u001b[0;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/hub.py?line=475'>476</a>\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(total\u001b[39m=\u001b[39mfile_size, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m progress,\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/hub.py?line=476'>477</a>\u001b[0m           unit\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mB\u001b[39m\u001b[39m'\u001b[39m, unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, unit_divisor\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m) \u001b[39mas\u001b[39;00m pbar:\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/hub.py?line=477'>478</a>\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/hub.py?line=478'>479</a>\u001b[0m         buffer \u001b[39m=\u001b[39m u\u001b[39m.\u001b[39;49mread(\u001b[39m8192\u001b[39;49m)\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/hub.py?line=479'>480</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(buffer) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/hub.py?line=480'>481</a>\u001b[0m             \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:465\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/lib/python3.10/http/client.py?line=461'>462</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[1;32m    <a href='file:///usr/lib/python3.10/http/client.py?line=462'>463</a>\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/lib/python3.10/http/client.py?line=463'>464</a>\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[0;32m--> <a href='file:///usr/lib/python3.10/http/client.py?line=464'>465</a>\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mread(amt)\n\u001b[1;32m    <a href='file:///usr/lib/python3.10/http/client.py?line=465'>466</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[1;32m    <a href='file:///usr/lib/python3.10/http/client.py?line=466'>467</a>\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/lib/python3.10/http/client.py?line=467'>468</a>\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/lib/python3.10/http/client.py?line=468'>469</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/lib/python3.10/socket.py?line=702'>703</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/lib/python3.10/socket.py?line=703'>704</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///usr/lib/python3.10/socket.py?line=704'>705</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    <a href='file:///usr/lib/python3.10/socket.py?line=705'>706</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    <a href='file:///usr/lib/python3.10/socket.py?line=706'>707</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1273\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/lib/python3.10/ssl.py?line=1268'>1269</a>\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   <a href='file:///usr/lib/python3.10/ssl.py?line=1269'>1270</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   <a href='file:///usr/lib/python3.10/ssl.py?line=1270'>1271</a>\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   <a href='file:///usr/lib/python3.10/ssl.py?line=1271'>1272</a>\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> <a href='file:///usr/lib/python3.10/ssl.py?line=1272'>1273</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   <a href='file:///usr/lib/python3.10/ssl.py?line=1273'>1274</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///usr/lib/python3.10/ssl.py?line=1274'>1275</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1129\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/lib/python3.10/ssl.py?line=1126'>1127</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///usr/lib/python3.10/ssl.py?line=1127'>1128</a>\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///usr/lib/python3.10/ssl.py?line=1128'>1129</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   <a href='file:///usr/lib/python3.10/ssl.py?line=1129'>1130</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///usr/lib/python3.10/ssl.py?line=1130'>1131</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "args = get_args()\n",
    "\n",
    "# datamodule = LitCifar10(batch_size=args.batch_size,\n",
    "#                         patch_num=args.patch_num, \n",
    "#                         num_workers=args.num_workers * args.devices)\n",
    "datamodule = LitCifar10(batch_size=args.batch_size,\n",
    "                        num_workers=args.num_workers * args.devices,\n",
    "                        path=args.path,\n",
    "                        patch_num=args.patch_num, \n",
    "\n",
    "                        n_fft=args.n_fft, \n",
    "                        n_mels=args.n_mels,\n",
    "                        win_length=args.win_length, \n",
    "                        hop_length=args.hop_length,\n",
    "                        class_dict=CLASS_TO_IDX\n",
    "                        )\n",
    "datamodule.setup()\n",
    "\n",
    "data = iter(datamodule.train_dataloader()).next()\n",
    "patch_dim = data[0].shape[-1]\n",
    "seqlen = data[0].shape[-2]\n",
    "print(\"Embed dim:\", args.embed_dim)\n",
    "print(\"Patch size:\", 32 // args.patch_num)\n",
    "print(\"Sequence length:\", seqlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    dirpath=os.path.join(args.path, \"checkpoints\"),\n",
    "    filename=\"trans-kws-best-acc\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor='test_acc',\n",
    "    mode='max',\n",
    ")\n",
    "idx_to_class = {v: k for k, v in CLASS_TO_IDX.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ init\n",
      "____________________ reset param\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = LitTransformer(num_classes=args.num_classes, lr=args.lr, epochs=args.max_epochs, \n",
    "                        depth=args.depth, embed_dim=args.embed_dim, head=args.num_heads,\n",
    "                        patch_dim=patch_dim, seqlen=seqlen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ init\n",
      "____________________ reset param\n"
     ]
    }
   ],
   "source": [
    "model = model.load_from_checkpoint(os.path.join(\n",
    "    args.path, \"checkpoints\", \"trans-kws-best-acc.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = Trainer(  accelerator=args.accelerator, \n",
    "                    devices=args.devices,\n",
    "                    precision=16 if args.accelerator == 'gpu' else 32,\n",
    "                    max_epochs=args.max_epochs, \n",
    "                    callbacks=[model_checkpoint],\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ datamodule prepare data\n",
      "____________________ setup \n",
      "____________________ datamodule prepare data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:611: UserWarning: Checkpoint directory /home/dl/Desktop/dl/object_detection_model_hw2/hw3/data/speech_commands/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params\n",
      "---------------------------------------------\n",
      "0 | encoder | Transformer      | 597 K \n",
      "1 | embed   | Linear           | 2.1 K \n",
      "2 | fc      | Linear           | 303 K \n",
      "3 | loss    | CrossEntropyLoss | 0     \n",
      "---------------------------------------------\n",
      "902 K     Trainable params\n",
      "0         Non-trainable params\n",
      "902 K     Total params\n",
      "1.806     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ config optimizers\n",
      "Sanity Checking: 0it [00:00, ?it/s]____________________ kws val dataloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:487: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ datamodule train dataloader                           \n",
      "Epoch 0: 100%|██████████| 779/779 [00:42<00:00, 18.18it/s, loss=0.603, v_num=33, test_loss=0.500, test_acc=85.60]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 701: 'test_acc' reached 85.58951 (best 85.58951), saving model to '/home/dl/Desktop/dl/object_detection_model_hw2/hw3/data/speech_commands/checkpoints/trans-kws-best-acc-v8.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  13%|█▎        | 104/779 [00:06<00:42, 15.95it/s, loss=0.456, v_num=33, test_loss=0.500, test_acc=85.60]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:724: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ datamodule prepare data\n",
      "____________________ setup \n",
      "____________________ datamodule prepare data\n",
      "Epoch 1:  13%|█▎        | 104/779 [00:16<01:46,  6.33it/s, loss=0.456, v_num=33, test_loss=0.500, test_acc=85.60]"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ datamodule prepare data\n",
      "____________________ setup \n",
      "____________________ datamodule prepare data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ datamodule test dataload\n",
      "Testing DataLoader 0: 100%|██████████| 86/86 [00:01<00:00, 46.92it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc             84.70567321777344\n",
      "        test_loss           0.5519934892654419\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.5519934892654419, 'test_acc': 84.70567321777344}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer.test(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ init\n",
      "____________________ reset param\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LitTransformer(\n",
       "  (encoder): Transformer(\n",
       "    (blocks): ModuleList(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
       "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
       "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
       "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
       "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
       "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
       "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
       "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
       "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
       "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
       "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
       "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
       "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (embed): Linear(in_features=32, out_features=64, bias=True)\n",
       "  (fc): Linear(in_features=8192, out_features=37, bias=True)\n",
       "  (loss): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.load_from_checkpoint(os.path.join(\n",
    "    args.path, \"checkpoints\", \"trans-kws-best-acc.ckpt\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "script = model.to_torchscript()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load most accurate checkpoint\n",
    "model_path = os.path.join(args.path, \"checkpoints\",\n",
    "                          \"trans-kws-best-acc.pt\")\n",
    "\n",
    "# save for use in production environment\n",
    "torch.jit.save(script, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth: follow, Prediction: follow\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# list wav files given a folder\n",
    "label = CLASSES[2:]\n",
    "label = np.random.choice(label)\n",
    "path = os.path.join(args.path, \"SpeechCommands/speech_commands_v0.02/\")\n",
    "path = os.path.join(path, label)\n",
    "wav_files = [os.path.join(path, f)\n",
    "             for f in os.listdir(path) if f.endswith('.wav')]\n",
    "# select random wav file\n",
    "wav_file = np.random.choice(wav_files)\n",
    "waveform, sample_rate = torchaudio.load(wav_file)\n",
    "transform = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate,\n",
    "                                                 n_fft=args.n_fft,\n",
    "                                                 win_length=args.win_length,\n",
    "                                                 hop_length=args.hop_length,\n",
    "                                                 n_mels=args.n_mels,\n",
    "                                                 power=2.0)\n",
    "\n",
    "mel = ToTensor()(librosa.power_to_db(\n",
    "    transform(waveform).squeeze().numpy(), ref=np.max))\n",
    "    \n",
    "scripted_module = torch.jit.load(model_path)\n",
    "pred = torch.argmax(scripted_module(mel), dim=1)\n",
    "print(f\"Ground Truth: {label}, Prediction: {idx_to_class[pred.item()]}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
