{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Spotting (KWS) using PyTorch Lightning \n",
    "\n",
    "We illustrate how to use PyTorch Lightning to train a model for keyword spotting. We also show how to build a model that is data agnostic, i.e. it can be trained on any dataset. A separate module handles the datasets and dataloaders.\n",
    "\n",
    "Using Experience, Task and Performance concept, KWS can be described as follows:\n",
    "\n",
    "1. **Experience:** The model is trained on a dataset of audio files containing 1sec speech samples, each is a single keyword. There are 35 distinct words in the dataset such as `yes`, `no`, `right`, `left` and so forth. However, 2 additional categories are added to the dataset: `silence` and `unknown`. `silence` is when there is silence or non-word audio audio activity such as background noise. `unknown` is when there is a keyword but not one of those 35 distinct words.\n",
    "\n",
    "2. **Task:** The model is trained to classify keywords in audio samples. We use a modified ResNet18 model.\n",
    "\n",
    "3. **Performance:** We measure the performance of the model by calculating the accuracy of the model on the test set. \n",
    "\n",
    "For simplicity, we do not use the validation set for hyperparameter tuning.\n",
    "\n",
    "We use KWS Version 2. The original KWS paper can be found [here](https://arxiv.org/pdf/1804.03209.pdf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in /home/dl/.local/lib/python3.10/site-packages (3.5.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/dl/.local/lib/python3.10/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/dl/.local/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/dl/.local/lib/python3.10/site-packages (from matplotlib) (4.33.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/dl/.local/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/dl/.local/lib/python3.10/site-packages (from matplotlib) (9.1.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/dl/.local/lib/python3.10/site-packages (from matplotlib) (1.22.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/dl/.local/lib/python3.10/site-packages (from matplotlib) (1.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# %pip install pytorch-lightning --upgrade\n",
    "# %pip install torchmetrics --upgrade\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dl/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torchaudio, torchvision\n",
    "# import os\n",
    "# import matplotlib.pyplot as plt \n",
    "# import librosa\n",
    "# import argparse\n",
    "# import numpy as np\n",
    "# import wandb\n",
    "# from pytorch_lightning import LightningModule, Trainer, LightningDataModule, Callback\n",
    "# from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "# from pytorch_lightning.loggers import WandbLogger\n",
    "# from torchmetrics.functional import accuracy\n",
    "# from torchvision.transforms import ToTensor\n",
    "# from torchaudio.datasets import SPEECHCOMMANDS\n",
    "# from torchaudio.datasets.speechcommands import load_speechcommands_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Silence and Unknown Datasets\n",
    "\n",
    "We create a custom `silence` dataset. The dataset randomly samples background audio supplied in the KWS dataset. These files are under the `_background_noise_` folder.\n",
    "\n",
    "We also create `unknown` dataset uses random audio samples from the train set but labelled as `unknown`. \n",
    "\n",
    "The creation of these 2 datasets is described in the [KWS paper](https://arxiv.org/pdf/1804.03209.pdf) and implemented below.\n",
    "\n",
    "We limit the number of samples to about the size of train dataset divided by 35 (the number of distinct words in the KWS dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SilenceDataset(SPEECHCOMMANDS):\n",
    "#     def __init__(self, root):\n",
    "#         super(SilenceDataset, self).__init__(root, subset='training')\n",
    "#         self.len = len(self._walker) // 35\n",
    "#         path = os.path.join(self._path, torchaudio.datasets.speechcommands.EXCEPT_FOLDER)\n",
    "#         self.paths = [os.path.join(path, p) for p in os.listdir(path) if p.endswith('.wav')]\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         index = np.random.randint(0, len(self.paths))\n",
    "#         filepath = self.paths[index]\n",
    "#         waveform, sample_rate = torchaudio.load(filepath)\n",
    "#         return waveform, sample_rate, \"silence\", 0, 0\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.len\n",
    "\n",
    "# class UnknownDataset(SPEECHCOMMANDS):\n",
    "#     def __init__(self, root):\n",
    "#         super(UnknownDataset, self).__init__(root, subset='training')\n",
    "#         self.len = len(self._walker) // 35\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         index = np.random.randint(0, len(self._walker))\n",
    "#         fileid = self._walker[index]\n",
    "#         waveform, sample_rate, _, speaker_id, utterance_number = load_speechcommands_item(fileid, self._path)\n",
    "#         return waveform, sample_rate, \"unknown\", speaker_id, utterance_number\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The PyTorch Lightning Data Module for KWS\n",
    "\n",
    "`KWSDataModule` cleanly separates the data handling from the model. The data module handles the datasets and dataloaders.\n",
    "\n",
    "We use `torchaudio` `SPEECHCOMMANDS` dataset to load the training, testing and validation sets. \n",
    "\n",
    "A custom `collate_fn` is used to handle the different lengths of the audio samples. The function also converts the wav files into mel spectrograms for the ResNet18 model input layer. A mel spectrogram is a log-mel spectrogram. It is an image that shows the power spectrum of the audio signal in dB. Basically, we convert audio to image. Then, we can use an image classifier like ResNet18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class KWSDataModule(LightningDataModule):\n",
    "#     def __init__(self, path, batch_size=128, num_workers=0, n_fft=512, \n",
    "#                  n_mels=128, win_length=None, hop_length=256, class_dict={}, \n",
    "#                  **kwargs):\n",
    "#         super().__init__(**kwargs)\n",
    "#         self.path = path\n",
    "#         self.batch_size = batch_size\n",
    "#         self.num_workers = num_workers\n",
    "#         self.n_fft = n_fft\n",
    "#         self.n_mels = n_mels\n",
    "#         self.win_length = win_length\n",
    "#         self.hop_length = hop_length\n",
    "#         self.class_dict = class_dict\n",
    "\n",
    "#     def prepare_data(self):\n",
    "#         self.train_dataset = torchaudio.datasets.SPEECHCOMMANDS(self.path,\n",
    "#                                                                 download=True,\n",
    "#                                                                 subset='training')\n",
    "\n",
    "#         silence_dataset = SilenceDataset(self.path)\n",
    "#         unknown_dataset = UnknownDataset(self.path)\n",
    "#         self.train_dataset = torch.utils.data.ConcatDataset([self.train_dataset, silence_dataset, unknown_dataset])\n",
    "                                                                \n",
    "#         self.val_dataset = torchaudio.datasets.SPEECHCOMMANDS(self.path,\n",
    "#                                                               download=True,\n",
    "#                                                               subset='validation')\n",
    "#         self.test_dataset = torchaudio.datasets.SPEECHCOMMANDS(self.path,\n",
    "#                                                                download=True,\n",
    "#                                                                subset='testing')                                                    \n",
    "#         _, sample_rate, _, _, _ = self.train_dataset[0]\n",
    "#         self.sample_rate = sample_rate\n",
    "#         self.transform = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate,\n",
    "#                                                               n_fft=self.n_fft,\n",
    "#                                                               win_length=self.win_length,\n",
    "#                                                               hop_length=self.hop_length,\n",
    "#                                                               n_mels=self.n_mels,\n",
    "#                                                               power=2.0)\n",
    "\n",
    "#     def setup(self, stage=None):\n",
    "#         self.prepare_data()\n",
    "\n",
    "#     def train_dataloader(self):\n",
    "#         return torch.utils.data.DataLoader(\n",
    "#             self.train_dataset,\n",
    "#             batch_size=self.batch_size,\n",
    "#             num_workers=self.num_workers,\n",
    "#             shuffle=True,\n",
    "#             pin_memory=True,\n",
    "#             collate_fn=self.collate_fn\n",
    "#         )\n",
    "\n",
    "#     def val_dataloader(self):\n",
    "#         return torch.utils.data.DataLoader(\n",
    "#             self.val_dataset,\n",
    "#             batch_size=self.batch_size,\n",
    "#             num_workers=self.num_workers,\n",
    "#             shuffle=False,\n",
    "#             pin_memory=True,\n",
    "#             collate_fn=self.collate_fn\n",
    "#         )\n",
    "    \n",
    "#     def test_dataloader(self):\n",
    "#         return torch.utils.data.DataLoader(\n",
    "#             self.test_dataset,\n",
    "#             batch_size=self.batch_size,\n",
    "#             num_workers=self.num_workers,\n",
    "#             shuffle=True,\n",
    "#             pin_memory=True,\n",
    "#             collate_fn=self.collate_fn\n",
    "#         )\n",
    "\n",
    "#     def collate_fn(self, batch):\n",
    "#         mels = []\n",
    "#         labels = []\n",
    "#         wavs = []\n",
    "#         for sample in batch:\n",
    "#             waveform, sample_rate, label, speaker_id, utterance_number = sample\n",
    "#             # ensure that all waveforms are 1sec in length; if not pad with zeros\n",
    "#             if waveform.shape[-1] < sample_rate:\n",
    "#                 waveform = torch.cat([waveform, torch.zeros((1, sample_rate - waveform.shape[-1]))], dim=-1)\n",
    "#             elif waveform.shape[-1] > sample_rate:\n",
    "#                 waveform = waveform[:,:sample_rate]\n",
    "\n",
    "#             # mel from power to db\n",
    "#             mels.append(ToTensor()(librosa.power_to_db(self.transform(waveform).squeeze().numpy(), ref=np.max)))\n",
    "#             labels.append(torch.tensor(self.class_dict[label]))\n",
    "#             wavs.append(waveform)\n",
    "\n",
    "#         mels = torch.stack(mels)\n",
    "#         labels = torch.stack(labels)\n",
    "#         wavs = torch.stack(wavs)\n",
    "   \n",
    "#         return mels, labels, wavs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The PL LightningModule for KWS\n",
    "\n",
    "The `KWSModel` is a ResNet18 model with first and last layers modified to suport single channel mel spectrogram inputs and 37-category outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class KWSModel(LightningModule):\n",
    "#     def __init__(self, num_classes=37, epochs=30, lr=0.001, **kwargs):\n",
    "#         super().__init__()\n",
    "#         self.save_hyperparameters()\n",
    "#         self.model = torchvision.models.resnet18(num_classes=num_classes)\n",
    "#         self.model.conv1 = torch.nn.Conv2d(\n",
    "#             1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         mels, labels, _ = batch\n",
    "#         preds = self.model(mels)\n",
    "#         loss = self.hparams.criterion(preds, labels)\n",
    "#         return {'loss': loss}\n",
    "\n",
    "#     # calls to self.log() are recorded in wandb\n",
    "#     def training_epoch_end(self, outputs):\n",
    "#         avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "#         self.log(\"train_loss\", avg_loss, on_epoch=True)\n",
    "\n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         return self.test_step(batch, batch_idx)\n",
    "\n",
    "#     def validation_epoch_end(self, outputs):\n",
    "#         return self.test_epoch_end(outputs)\n",
    "\n",
    "#     def test_step(self, batch, batch_idx):\n",
    "#         mels, labels, wavs = batch\n",
    "#         preds = self.model(mels)\n",
    "#         loss = self.hparams.criterion(preds, labels)\n",
    "#         acc = accuracy(preds, labels) * 100.\n",
    "#         return {\"preds\": preds, 'test_loss': loss, 'test_acc': acc}\n",
    "\n",
    "#     def test_epoch_end(self, outputs):\n",
    "#         avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "#         avg_acc = torch.stack([x['test_acc'] for x in outputs]).mean()\n",
    "#         self.log(\"test_loss\", avg_loss, on_epoch=True, prog_bar=True)\n",
    "#         self.log(\"test_acc\", avg_acc, on_epoch=True, prog_bar=True)\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         optimizer = torch.optim.AdamW(\n",
    "#             self.parameters(), lr=self.hparams.lr)\n",
    "#         lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "#             optimizer=optimizer, T_max=self.hparams.epochs)\n",
    "#         return [optimizer], [lr_scheduler]\n",
    "\n",
    "#     def setup(self, stage=None):\n",
    "#         self.hparams.criterion = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Lightning Callback\n",
    "\n",
    "We can instantiate a callback object to perform certain tasks during training. In this case, we log sample audio files, ground truth labels, and predicted labels.\n",
    "\n",
    "We can also `ModelCheckpoint` callback to save the model after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class WandbCallback(Callback):\n",
    "\n",
    "#     def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n",
    "#         # log 10 sample audio predictions from the first batch\n",
    "#         if batch_idx == 0:\n",
    "#             n = 10\n",
    "#             mels, labels, wavs = batch\n",
    "#             preds = outputs[\"preds\"]\n",
    "#             preds = torch.argmax(preds, dim=1)\n",
    "\n",
    "#             labels = labels.cpu().numpy()\n",
    "#             preds = preds.cpu().numpy()\n",
    "            \n",
    "#             wavs = torch.squeeze(wavs, dim=1)\n",
    "#             wavs = [ (wav.cpu().numpy()*32768.0).astype(\"int16\") for wav in wavs]\n",
    "            \n",
    "#             sample_rate = pl_module.hparams.sample_rate\n",
    "#             idx_to_class = pl_module.hparams.idx_to_class\n",
    "            \n",
    "#             # log audio samples and predictions as a W&B Table\n",
    "#             columns = ['audio', 'mel', 'ground truth', 'prediction']\n",
    "#             data = [[wandb.Audio(wav, sample_rate=sample_rate), wandb.Image(mel), idx_to_class[label], idx_to_class[pred]] for wav, mel, label, pred in list(\n",
    "#                 zip(wavs[:n], mels[:n], labels[:n], preds[:n]))]\n",
    "#             wandb_logger.log_table(\n",
    "#                 key='ResNet18 on KWS using PyTorch Lightning',\n",
    "#                 columns=columns,\n",
    "#                 data=data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program Arguments\n",
    "\n",
    "The default configuration is shown below. \n",
    "\n",
    "We also define `plot_waveform` for plotting the waveform of the audio samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # model training hyperparameters\n",
    "    parser.add_argument('--batch-size', type=int, default=128, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--max-epochs', type=int, default=30, metavar='N',\n",
    "                        help='number of epochs to train (default: 30)')\n",
    "    parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
    "                        help='learning rate (default: 0.001)')\n",
    "\n",
    "    # where dataset will be stored\n",
    "    parser.add_argument(\"--path\", type=str, default=\"data/speech_commands/\")\n",
    "\n",
    "    # 35 keywords + silence + unknown\n",
    "    parser.add_argument(\"--num-classes\", type=int, default=37)\n",
    "   \n",
    "    # mel spectrogram parameters\n",
    "    parser.add_argument(\"--n-fft\", type=int, default=1024)\n",
    "    parser.add_argument(\"--n-mels\", type=int, default=128)\n",
    "    parser.add_argument(\"--win-length\", type=int, default=None)\n",
    "    parser.add_argument(\"--hop-length\", type=int, default=512)\n",
    "\n",
    "    # 16-bit fp model to reduce the size\n",
    "    parser.add_argument(\"--precision\", default=16)\n",
    "    parser.add_argument(\"--accelerator\", default='gpu')\n",
    "    parser.add_argument(\"--devices\", default=1)\n",
    "    parser.add_argument(\"--num-workers\", type=int, default=96)\n",
    "\n",
    "    parser.add_argument(\"--no-wandb\", default=True, action='store_true')\n",
    "\n",
    "    args = parser.parse_args(\"\")\n",
    "    return args\n",
    "\n",
    "def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sample_rate\n",
    "\n",
    "    figure, axes = plt.subplots(num_channels, 1)\n",
    "    if num_channels == 1:\n",
    "        axes = [axes]\n",
    "    for c in range(num_channels):\n",
    "        axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
    "        axes[c].grid(True)\n",
    "        if num_channels > 1:\n",
    "            axes[c].set_ylabel(f'Channel {c+1}')\n",
    "        if xlim:\n",
    "            axes[c].set_xlim(xlim)\n",
    "        if ylim:\n",
    "            axes[c].set_ylim(ylim)\n",
    "    figure.suptitle(title)\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KWS Training and Evaluation using `Trainer`\n",
    "\n",
    "The actual training and validation using `Trainer`.\n",
    "\n",
    "Accuracy is about `94.5%` (`94.0%` 16-bit) on the test set. Benchmark accuracy is `88.2%` on the original paper. The state of the art (as of 1 Apr 2022) is `97.0%`. For updated benchmarks, see [paperswithcode.com](https://paperswithcode.com/sota/keyword-spotting-on-google-speech-commands)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KWSModel(\n",
      "  (model): ResNet(\n",
      "    (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=512, out_features=37, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malessandrosantiago\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dl/Desktop/dl/object_detection_model_hw2/hw3/wandb/run-20220530_012919-2iiliat4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/alessandrosantiago/pl-kws/runs/2iiliat4\" target=\"_blank\">likely-cloud-39</a></strong> to <a href=\"https://wandb.ai/alessandrosantiago/pl-kws\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:611: UserWarning: Checkpoint directory /home/dl/Desktop/dl/object_detection_model_hw2/hw3/data/speech_commands/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | ResNet | 11.2 M\n",
      "---------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "22.378    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dl/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 96 worker processes in total. Our suggested max number of worker in current system is 16, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 779/779 [00:28<00:00, 27.62it/s, loss=0.608, test_loss=0.482, test_acc=86.60]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 701: 'test_acc' reached 86.58446 (best 86.58446), saving model to '/home/dl/Desktop/dl/object_detection_model_hw2/hw3/data/speech_commands/checkpoints/resnet18-kws-best-acc-v2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 779/779 [00:28<00:00, 27.42it/s, loss=0.474, test_loss=0.368, test_acc=89.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 1402: 'test_acc' reached 89.76980 (best 89.76980), saving model to '/home/dl/Desktop/dl/object_detection_model_hw2/hw3/data/speech_commands/checkpoints/resnet18-kws-best-acc-v2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 779/779 [00:28<00:00, 27.14it/s, loss=0.416, test_loss=0.356, test_acc=90.20]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 2103: 'test_acc' reached 90.18069 (best 90.18069), saving model to '/home/dl/Desktop/dl/object_detection_model_hw2/hw3/data/speech_commands/checkpoints/resnet18-kws-best-acc-v2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 779/779 [00:27<00:00, 27.90it/s, loss=0.359, test_loss=0.295, test_acc=91.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 2804: 'test_acc' reached 91.76370 (best 91.76370), saving model to '/home/dl/Desktop/dl/object_detection_model_hw2/hw3/data/speech_commands/checkpoints/resnet18-kws-best-acc-v2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 779/779 [00:28<00:00, 27.37it/s, loss=0.36, test_loss=0.308, test_acc=91.50] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 3505: 'test_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 779/779 [00:27<00:00, 27.94it/s, loss=0.312, test_loss=0.310, test_acc=91.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 4206: 'test_acc' reached 91.84311 (best 91.84311), saving model to '/home/dl/Desktop/dl/object_detection_model_hw2/hw3/data/speech_commands/checkpoints/resnet18-kws-best-acc-v2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 779/779 [00:27<00:00, 27.94it/s, loss=0.254, test_loss=0.266, test_acc=92.70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 4907: 'test_acc' reached 92.74576 (best 92.74576), saving model to '/home/dl/Desktop/dl/object_detection_model_hw2/hw3/data/speech_commands/checkpoints/resnet18-kws-best-acc-v2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 779/779 [00:27<00:00, 27.98it/s, loss=0.254, test_loss=0.298, test_acc=92.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 5608: 'test_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 779/779 [00:27<00:00, 27.94it/s, loss=0.275, test_loss=0.321, test_acc=92.20]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 6309: 'test_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 779/779 [00:27<00:00, 28.10it/s, loss=0.222, test_loss=0.316, test_acc=92.60]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 7010: 'test_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 779/779 [00:27<00:00, 28.10it/s, loss=0.17, test_loss=0.303, test_acc=92.80] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 7711: 'test_acc' reached 92.79439 (best 92.79439), saving model to '/home/dl/Desktop/dl/object_detection_model_hw2/hw3/data/speech_commands/checkpoints/resnet18-kws-best-acc-v2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 779/779 [00:27<00:00, 27.99it/s, loss=0.188, test_loss=0.316, test_acc=93.00]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 8412: 'test_acc' reached 92.96611 (best 92.96611), saving model to '/home/dl/Desktop/dl/object_detection_model_hw2/hw3/data/speech_commands/checkpoints/resnet18-kws-best-acc-v2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 779/779 [00:27<00:00, 27.83it/s, loss=0.176, test_loss=0.321, test_acc=93.00]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 9113: 'test_acc' reached 92.99664 (best 92.99664), saving model to '/home/dl/Desktop/dl/object_detection_model_hw2/hw3/data/speech_commands/checkpoints/resnet18-kws-best-acc-v2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 779/779 [00:27<00:00, 28.09it/s, loss=0.177, test_loss=0.304, test_acc=93.40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 9814: 'test_acc' reached 93.44688 (best 93.44688), saving model to '/home/dl/Desktop/dl/object_detection_model_hw2/hw3/data/speech_commands/checkpoints/resnet18-kws-best-acc-v2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 779/779 [00:27<00:00, 27.90it/s, loss=0.158, test_loss=0.342, test_acc=93.10]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 10515: 'test_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 779/779 [00:28<00:00, 27.76it/s, loss=0.14, test_loss=0.302, test_acc=93.80] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 11216: 'test_acc' reached 93.77596 (best 93.77596), saving model to '/home/dl/Desktop/dl/object_detection_model_hw2/hw3/data/speech_commands/checkpoints/resnet18-kws-best-acc-v2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 779/779 [00:28<00:00, 27.73it/s, loss=0.155, test_loss=0.304, test_acc=93.90]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 11917: 'test_acc' reached 93.85658 (best 93.85658), saving model to '/home/dl/Desktop/dl/object_detection_model_hw2/hw3/data/speech_commands/checkpoints/resnet18-kws-best-acc-v2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 779/779 [00:28<00:00, 27.73it/s, loss=0.125, test_loss=0.311, test_acc=93.60] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 12618: 'test_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 779/779 [00:28<00:00, 27.76it/s, loss=0.146, test_loss=0.311, test_acc=94.00]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 13319: 'test_acc' reached 94.00705 (best 94.00705), saving model to '/home/dl/Desktop/dl/object_detection_model_hw2/hw3/data/speech_commands/checkpoints/resnet18-kws-best-acc-v2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 779/779 [00:28<00:00, 27.63it/s, loss=0.146, test_loss=0.332, test_acc=94.00]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 14020: 'test_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 779/779 [00:28<00:00, 27.82it/s, loss=0.153, test_loss=0.344, test_acc=93.60]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, global step 14721: 'test_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 779/779 [00:28<00:00, 27.62it/s, loss=0.124, test_loss=0.299, test_acc=94.20]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, global step 15422: 'test_acc' reached 94.16827 (best 94.16827), saving model to '/home/dl/Desktop/dl/object_detection_model_hw2/hw3/data/speech_commands/checkpoints/resnet18-kws-best-acc-v2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 779/779 [00:28<00:00, 27.69it/s, loss=0.132, test_loss=0.303, test_acc=94.20] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22, global step 16123: 'test_acc' reached 94.24792 (best 94.24792), saving model to '/home/dl/Desktop/dl/object_detection_model_hw2/hw3/data/speech_commands/checkpoints/resnet18-kws-best-acc-v2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 779/779 [00:27<00:00, 27.84it/s, loss=0.14, test_loss=0.307, test_acc=94.30]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23, global step 16824: 'test_acc' reached 94.27773 (best 94.27773), saving model to '/home/dl/Desktop/dl/object_detection_model_hw2/hw3/data/speech_commands/checkpoints/resnet18-kws-best-acc-v2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 779/779 [00:27<00:00, 27.83it/s, loss=0.116, test_loss=0.305, test_acc=94.50] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24, global step 17525: 'test_acc' reached 94.51835 (best 94.51835), saving model to '/home/dl/Desktop/dl/object_detection_model_hw2/hw3/data/speech_commands/checkpoints/resnet18-kws-best-acc-v2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 779/779 [00:28<00:00, 27.56it/s, loss=0.115, test_loss=0.313, test_acc=94.30] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25, global step 18226: 'test_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 779/779 [00:28<00:00, 27.30it/s, loss=0.141, test_loss=0.314, test_acc=94.30] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26, global step 18927: 'test_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 779/779 [00:28<00:00, 27.55it/s, loss=0.115, test_loss=0.313, test_acc=94.40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27, global step 19628: 'test_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 779/779 [00:28<00:00, 27.57it/s, loss=0.131, test_loss=0.313, test_acc=94.40] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28, global step 20329: 'test_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 779/779 [00:28<00:00, 27.31it/s, loss=0.132, test_loss=0.314, test_acc=94.30] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29, global step 21030: 'test_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 779/779 [00:28<00:00, 27.31it/s, loss=0.132, test_loss=0.314, test_acc=94.30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:487: PossibleUserWarning: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 86/86 [00:00<00:00, 120.09it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc             94.03895568847656\n",
      "        test_loss           0.31350356340408325\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">likely-cloud-39</strong>: <a href=\"https://wandb.ai/alessandrosantiago/pl-kws/runs/2iiliat4\" target=\"_blank\">https://wandb.ai/alessandrosantiago/pl-kws/runs/2iiliat4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220530_012919-2iiliat4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # args = get_args()\n",
    "    # CLASSES = ['silence', 'unknown', 'backward', 'bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'follow',\n",
    "    #            'forward', 'four', 'go', 'happy', 'house', 'learn', 'left', 'marvin', 'nine', 'no',\n",
    "    #            'off', 'on', 'one', 'right', 'seven', 'sheila', 'six', 'stop', 'three',\n",
    "    #            'tree', 'two', 'up', 'visual', 'wow', 'yes', 'zero']\n",
    "    \n",
    "    # make a dictionary from CLASSES to integers\n",
    "    # CLASS_TO_IDX = {c: i for i, c in enumerate(CLASSES)}\n",
    "\n",
    "    # if not os.path.exists(args.path):\n",
    "    #     os.makedirs(args.path, exist_ok=True)\n",
    "\n",
    "    model = KWSModel(num_classes=args.num_classes, epochs=args.max_epochs, lr=args.lr)\n",
    "    print(model)\n",
    "\n",
    "\n",
    "\n",
    "    # datamodule = KWSDataModule(batch_size=args.batch_size, num_workers=args.num_workers,\n",
    "    #                            path=args.path, n_fft=args.n_fft, n_mels=args.n_mels,\n",
    "    #                            win_length=args.win_length, hop_length=args.hop_length,\n",
    "    #                            class_dict=CLASS_TO_IDX)\n",
    "    # datamodule.setup()\n",
    "\n",
    "\n",
    "\n",
    "    # wandb is a great way to debug and visualize this model\n",
    "    wandb_logger = WandbLogger(project=\"pl-kws\")\n",
    "\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        dirpath=os.path.join(args.path, \"checkpoints\"),\n",
    "        filename=\"resnet18-kws-best-acc\",\n",
    "        save_top_k=1,\n",
    "        verbose=True,\n",
    "        monitor='test_acc',\n",
    "        mode='max',\n",
    "    )\n",
    "    idx_to_class = {v: k for k, v in CLASS_TO_IDX.items()}\n",
    "    trainer = Trainer(accelerator=args.accelerator,\n",
    "                      devices=args.devices,\n",
    "                      precision=args.precision,\n",
    "                      max_epochs=args.max_epochs,\n",
    "                      logger=wandb_logger if not args.no_wandb else None,\n",
    "                      callbacks=[model_checkpoint])\n",
    "    model.hparams.sample_rate = datamodule.sample_rate\n",
    "    model.hparams.idx_to_class = idx_to_class\n",
    "    trainer.fit(model, datamodule=datamodule)\n",
    "    trainer.test(model, datamodule=datamodule)\n",
    "\n",
    "    wandb.finish()\n",
    "    #trainer.save_checkpoint('../mnist/checkpoint.ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'criterion' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['criterion'])`.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth: off, Prediction: off\n"
     ]
    }
   ],
   "source": [
    "# # https://pytorch-lightning.readthedocs.io/en/stable/common/production_inference.html\n",
    "# model = model.load_from_checkpoint(os.path.join(\n",
    "#     args.path, \"checkpoints\", \"resnet18-kws-best-acc.ckpt\"))\n",
    "# model.eval()\n",
    "# script = model.to_torchscript()\n",
    "\n",
    "# # save for use in production environment\n",
    "# model_path = os.path.join(args.path, \"checkpoints\",\n",
    "#                           \"resnet18-kws-best-acc.pt\")\n",
    "# torch.jit.save(script, model_path)\n",
    "\n",
    "# # list wav files given a folder\n",
    "# label = CLASSES[2:]\n",
    "# label = np.random.choice(label)\n",
    "# path = os.path.join(args.path, \"SpeechCommands/speech_commands_v0.02/\")\n",
    "# path = os.path.join(path, label)\n",
    "# wav_files = [os.path.join(path, f)\n",
    "#              for f in os.listdir(path) if f.endswith('.wav')]\n",
    "# # select random wav file\n",
    "# wav_file = np.random.choice(wav_files)\n",
    "# waveform, sample_rate = torchaudio.load(wav_file)\n",
    "# transform = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate,\n",
    "#                                                  n_fft=args.n_fft,\n",
    "#                                                  win_length=args.win_length,\n",
    "#                                                  hop_length=args.hop_length,\n",
    "#                                                  n_mels=args.n_mels,\n",
    "#                                                  power=2.0)\n",
    "\n",
    "# mel = ToTensor()(librosa.power_to_db(\n",
    "#     transform(waveform).squeeze().numpy(), ref=np.max))\n",
    "# mel = mel.unsqueeze(0)\n",
    "# scripted_module = torch.jit.load(model_path)\n",
    "# pred = torch.argmax(scripted_module(mel), dim=1)\n",
    "# print(f\"Ground Truth: {label}, Prediction: {idx_to_class[pred.item()]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
