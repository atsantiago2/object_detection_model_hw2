{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer for CIFAR10\n",
    "\n",
    "A configurable transformer model will be used for CIFAR10 image classification. \n",
    "\n",
    "The vision transformer model is a modified version of ViT. The changes are:\n",
    "1) No position embedding.\n",
    "2) No dropout is used.\n",
    "3) All encoder features are used for class prediction.\n",
    "\n",
    "The code below is a simplified version of Timm modules.\n",
    "\n",
    "Let us import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install ipykernel pytorch-lightning  pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch -c conda-forge --update-deps --force-reinstall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "\n",
    "def get_args():\n",
    "    parser = ArgumentParser(description='PyTorch Transformer')\n",
    "    parser.add_argument('--depth', type=int, default=12, help='depth')\n",
    "    parser.add_argument('--embed_dim', type=int, default=64, help='embedding dimension')\n",
    "    parser.add_argument('--num_heads', type=int, default=4, help='num_heads')\n",
    "\n",
    "    parser.add_argument('--patch_num', type=int, default=8, help='patch_num')\n",
    "    parser.add_argument('--kernel_size', type=int, default=3, help='kernel size')\n",
    "    parser.add_argument('--batch_size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: )')\n",
    "    parser.add_argument('--max-epochs', type=int, default=2, metavar='N',\n",
    "                        help='number of epochs to train (default: 0)')\n",
    "    parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
    "                        help='learning rate (default: 0.0)')\n",
    "\n",
    "    parser.add_argument('--accelerator', default='gpu', type=str, metavar='N')\n",
    "    parser.add_argument('--devices', default=1, type=int, metavar='N')\n",
    "    parser.add_argument('--dataset', default='cifar10', type=str, metavar='N')\n",
    "    parser.add_argument('--num_workers', default=4, type=int, metavar='N')\n",
    "    args = parser.parse_args(\"\")\n",
    "    return args\n",
    "\n",
    "args = get_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer, LightningDataModule\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchmetrics.functional import accuracy\n",
    "from einops import rearrange\n",
    "from torch import nn\n",
    "from torchvision.datasets.cifar import CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Module\n",
    "\n",
    "The `Attention` module is the core of the vision transformer model. It implements the attention mechanism:\n",
    "\n",
    "1) Multiply QKV by their weights\n",
    "2) Perform dot product on Q and K. \n",
    "3) Normalize the result in 2) by sqrt of `head_dim`  \n",
    "4) Softmax is applied to the result.\n",
    "5) Perform dot product on the result of 4) and V and the result is the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Module\n",
    "\n",
    "The MLP module is a made of two linear layers. A non-linear activation is applied to the output of the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "      \n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Block Module\n",
    "\n",
    "The `Block` module represents one encoder transformer block. It consists of two sub-modules:\n",
    "1) The Attention module\n",
    "2) The MLP module\n",
    "\n",
    "Layer norm is applied before and after the Attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, dim, num_heads, mlp_ratio=4., qkv_bias=False, \n",
    "            act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias) \n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer) \n",
    "   \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Transformer Module\n",
    "\n",
    "The feature encoder is made of several transformer blocks. The most important attributes are:\n",
    "1) `depth` : representing the number of encoder blocks\n",
    "2) `num_heads` : representing the number of attention heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_blocks, mlp_ratio=4., qkv_bias=False,  \n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([Block(dim, num_heads, mlp_ratio, qkv_bias, \n",
    "                                     act_layer, norm_layer) for _ in range(num_blocks)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The optional parameter initialization as adopted from `timm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights_vit_timm(module: nn.Module):\n",
    "    \"\"\" ViT weight initialization, original timm impl (for reproducibility) \"\"\"\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.trunc_normal_(module.weight, mean=0.0, std=0.02)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif hasattr(module, 'init_weights'):\n",
    "        module.init_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Lightning for CIFAR10 Image Classification\n",
    "\n",
    "We use the `Transformer` module to build the feature encoder. Before the `Transformer` can be used, we convert the input image into patches. The patches are then embedded into a linear space. The output is then passed to the Transformer.\n",
    "\n",
    "Another difference between this model is we use all output features for the final classification. In the ViT, only the first feature is used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LitTransformer(LightningModule):\n",
    "    def __init__(self, num_classes=10, lr=0.001, max_epochs=30, depth=12, embed_dim=64,\n",
    "                 head=4, patch_dim=192, seqlen=16, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.encoder = Transformer(dim=embed_dim, num_heads=head, num_blocks=depth, mlp_ratio=4.,\n",
    "                                   qkv_bias=False, act_layer=nn.GELU, norm_layer=nn.LayerNorm)\n",
    "        self.embed = torch.nn.Linear(patch_dim, embed_dim)\n",
    "\n",
    "        self.fc = nn.Linear(seqlen * embed_dim, num_classes)\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init_weights_vit_timm(self)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear projection\n",
    "        x = self.embed(x)\n",
    "            \n",
    "        # Encoder\n",
    "        x = self.encoder(x)\n",
    "        x = x.flatten(start_dim=1)\n",
    "\n",
    "        # Classification head\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        # this decays the learning rate to 0 after max_epochs using cosine annealing\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=self.hparams.max_epochs)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        acc = accuracy(y_hat, y)\n",
    "        return {\"y_hat\": y_hat, \"test_loss\": loss, \"test_acc\": acc}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"test_loss\"] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x[\"test_acc\"] for x in outputs]).mean()\n",
    "        self.log(\"test_loss\", avg_loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_acc\", avg_acc*100., on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.test_step(batch, batch_idx)\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        return self.test_epoch_end(outputs)\n",
    "\n",
    "\n",
    "# a lightning data module for cifar 10 dataset\n",
    "class LitCifar10(LightningDataModule):\n",
    "    def __init__(self, batch_size=32, num_workers=32, patch_num=4, **kwargs):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.patch_num = patch_num\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.train_set = CIFAR10(root='./data', train=True,\n",
    "                                 download=True, transform=torchvision.transforms.ToTensor())\n",
    "        self.test_set = CIFAR10(root='./data', train=False,\n",
    "                                download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        x, y = zip(*batch)\n",
    "        x = torch.stack(x, dim=0)\n",
    "        y = torch.LongTensor(y)\n",
    "        x = rearrange(x, 'b c (p1 h) (p2 w) -> b (p1 p2) (c h w)', p1=self.patch_num, p2=self.patch_num)\n",
    "        return x, y\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.train_set, batch_size=self.batch_size, \n",
    "                                        shuffle=True, collate_fn=self.collate_fn,\n",
    "                                        num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.test_set, batch_size=self.batch_size, \n",
    "                                        shuffle=False, collate_fn=self.collate_fn,\n",
    "                                        num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.test_dataloader()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on different settings\n",
    "\n",
    "The following table shows different performances on different settings. Generally, Transformer is better than MLP in terms of accuracy and parameter count. However, the performance is worse compared to CNN models. \n",
    "\n",
    "However, the most important thing to note is that Transformers are more general purpose models than CNNs. They can process different types of data. They can process multiple types of data at the same time. This is why they are considered to be the backbone of many high-performing models like BERT, GPT3, PalM and Gato.\n",
    "\n",
    "| **Depth** | **Head** | **Embed dim** | **Patch size** | **Seq len** | **Params** | **Accuracy** | \n",
    "| -: | -: | -: | -: | -: | -: | -: |\n",
    "| 12 | 4 | 32 | 4x4 | 64 | 173k | 68.2% |\n",
    "| 12 | 4 | 64 | 4x4 | 64 | 641k | 71.1% |\n",
    "| 12 | 4 | 128 | 4x4 | 64 | 2.5M | 71.5% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed dim: 64\n",
      "Patch size: 4\n",
      "Sequence length: 64\n",
      "LitTransformer(\n",
      "  (encoder): Transformer(\n",
      "    (blocks): ModuleList(\n",
      "      (0): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (2): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (3): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (4): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (5): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (6): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (7): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (8): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (9): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (10): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (11): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (embed): Linear(in_features=48, out_features=64, bias=True)\n",
      "  (fc): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  (loss): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "datamodule = LitCifar10(batch_size=args.batch_size,\n",
    "                        patch_num=args.patch_num, \n",
    "                        num_workers=args.num_workers * args.devices)\n",
    "datamodule.prepare_data()\n",
    "\n",
    "data = iter(datamodule.train_dataloader()).next()\n",
    "patch_dim = data[0].shape[-1]\n",
    "seqlen = data[0].shape[-2]\n",
    "print(\"Embed dim:\", args.embed_dim)\n",
    "print(\"Patch size:\", 32 // args.patch_num)\n",
    "print(\"Sequence length:\", seqlen)\n",
    "\n",
    "\n",
    "model = LitTransformer(num_classes=10, lr=args.lr, epochs=args.max_epochs, \n",
    "                        depth=args.depth, embed_dim=args.embed_dim, head=args.num_heads,\n",
    "                        patch_dim=patch_dim, seqlen=seqlen,)\n",
    "print(model)\n",
    "trainer = Trainer(accelerator=args.accelerator, devices=args.devices,\n",
    "                    max_epochs=args.max_epochs, precision=16 if args.accelerator == 'gpu' else 32,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo.ipynb Cell 20'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo.ipynb#ch0000019?line=0'>1</a>\u001b[0m t \u001b[39m=\u001b[39m datamodule\u001b[39m.\u001b[39mtrain_dataloader()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo.ipynb#ch0000019?line=1'>2</a>\u001b[0m t\u001b[39m.\u001b[39;49mdataset[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "t = datamodule.train_dataloader()\n",
    "t.dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.dataset[0]["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = iter(datamodule.train_dataloader()).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.2353, 0.1882, 0.2314,  ..., 0.2078, 0.2235, 0.2510],\n",
       "          [0.3843, 0.3098, 0.2118,  ..., 0.3333, 0.2157, 0.2824],\n",
       "          [0.2784, 0.2549, 0.3647,  ..., 0.1647, 0.1569, 0.2275],\n",
       "          ...,\n",
       "          [0.1059, 0.1294, 0.2000,  ..., 0.3608, 0.6392, 0.4980],\n",
       "          [0.3020, 0.4078, 0.1765,  ..., 0.3647, 0.2510, 0.5098],\n",
       "          [0.4392, 0.4941, 0.5412,  ..., 0.3961, 0.4706, 0.6000]],\n",
       " \n",
       "         [[0.7059, 0.6980, 0.7216,  ..., 0.3608, 0.3686, 0.3569],\n",
       "          [0.6980, 0.7020, 0.7176,  ..., 0.3686, 0.3647, 0.3725],\n",
       "          [0.6980, 0.7216, 0.7373,  ..., 0.3882, 0.4039, 0.4118],\n",
       "          ...,\n",
       "          [0.4235, 0.4784, 0.4706,  ..., 0.3373, 0.3255, 0.3490],\n",
       "          [0.4941, 0.5882, 0.5137,  ..., 0.3255, 0.3490, 0.3412],\n",
       "          [0.5725, 0.4980, 0.4588,  ..., 0.3373, 0.2863, 0.2745]],\n",
       " \n",
       "         [[0.2275, 0.2471, 0.2275,  ..., 0.2667, 0.2275, 0.2510],\n",
       "          [0.2314, 0.2353, 0.2235,  ..., 0.2235, 0.2824, 0.4000],\n",
       "          [0.2510, 0.2784, 0.3686,  ..., 0.1961, 0.2353, 0.2980],\n",
       "          ...,\n",
       "          [0.9569, 0.9647, 0.9490,  ..., 0.9725, 0.9765, 0.9804],\n",
       "          [0.9608, 0.9765, 0.9765,  ..., 0.9765, 0.9765, 0.9765],\n",
       "          [0.9686, 0.9608, 0.9608,  ..., 0.9843, 0.9725, 0.9608]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.5686, 0.5569, 0.6196,  ..., 0.8863, 0.7725, 0.7412],\n",
       "          [0.5098, 0.4471, 0.4471,  ..., 0.7098, 0.7059, 0.5216],\n",
       "          [0.4588, 0.4078, 0.3686,  ..., 0.5333, 0.5294, 0.4941],\n",
       "          ...,\n",
       "          [0.6157, 0.6275, 0.6314,  ..., 0.3529, 0.3725, 0.5059],\n",
       "          [0.7412, 0.7686, 0.7412,  ..., 0.6078, 0.5294, 0.6431],\n",
       "          [0.6667, 0.6980, 0.6627,  ..., 0.7765, 0.6353, 0.5608]],\n",
       " \n",
       "         [[0.4588, 0.4627, 0.4784,  ..., 0.8431, 0.8392, 0.8588],\n",
       "          [0.4980, 0.5059, 0.5176,  ..., 0.9216, 0.9098, 0.8980],\n",
       "          [0.5373, 0.5490, 0.5647,  ..., 0.9059, 0.9137, 0.9333],\n",
       "          ...,\n",
       "          [0.6824, 0.6784, 0.6667,  ..., 0.6000, 0.6078, 0.5922],\n",
       "          [0.6667, 0.6627, 0.6431,  ..., 0.6000, 0.6000, 0.5686],\n",
       "          [0.6235, 0.6157, 0.5843,  ..., 0.3373, 0.2902, 0.2667]],\n",
       " \n",
       "         [[0.5647, 0.3137, 0.2235,  ..., 0.3725, 0.2627, 0.2588],\n",
       "          [0.2902, 0.2824, 0.2980,  ..., 0.3529, 0.3333, 0.3529],\n",
       "          [0.6471, 0.6627, 0.5725,  ..., 0.3686, 0.3490, 0.4314],\n",
       "          ...,\n",
       "          [0.4314, 0.4627, 0.4863,  ..., 0.3725, 0.4824, 0.5882],\n",
       "          [0.4824, 0.3882, 0.4000,  ..., 0.5686, 0.5216, 0.5725],\n",
       "          [0.3255, 0.4314, 0.4353,  ..., 0.5137, 0.4902, 0.4980]]]),\n",
       " tensor([6, 6, 5, 0, 6, 3, 7, 6, 4, 2, 5, 1, 4, 2, 4, 0, 5, 2, 4, 8, 1, 6, 1, 9,\n",
       "         6, 0, 7, 6, 0, 0, 9, 7, 1, 6, 7, 1, 5, 9, 4, 1, 3, 5, 7, 0, 1, 4, 4, 0,\n",
       "         6, 3, 0, 5, 4, 4, 8, 3, 4, 3, 9, 5, 9, 8, 8, 3]))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64, 48])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo.ipynb Cell 26'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo.ipynb#ch0000030?line=0'>1</a>\u001b[0m data[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mnumpy()\u001b[39m.\u001b[39;49mtranspose(\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mValueError\u001b[0m: axes don't match array"
     ]
    }
   ],
   "source": [
    "data[0][0].numpy().transpose(1, 2, 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# plt.imshow(data[0][0].numpy().transpose(1, 2, 0))\n",
    "plt.imshow(data[0][0].numpy().transpose(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
