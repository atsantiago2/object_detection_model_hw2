{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer for CIFAR10\n",
    "\n",
    "A configurable transformer model will be used for CIFAR10 image classification. \n",
    "\n",
    "The vision transformer model is a modified version of ViT. The changes are:\n",
    "1) No position embedding.\n",
    "2) No dropout is used.\n",
    "3) All encoder features are used for class prediction.\n",
    "\n",
    "The code below is a simplified version of Timm modules.\n",
    "\n",
    "Let us import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install ipykernel pytorch-lightning  pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch -c conda-forge --update-deps --force-reinstall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "\n",
    "def get_args():\n",
    "    parser = ArgumentParser(description='PyTorch Transformer')\n",
    "    parser.add_argument('--depth', type=int, default=12, help='depth')\n",
    "    parser.add_argument('--embed_dim', type=int, default=64, help='embedding dimension')\n",
    "    parser.add_argument('--num_heads', type=int, default=4, help='num_heads')\n",
    "\n",
    "    parser.add_argument('--patch_num', type=int, default=8, help='patch_num')\n",
    "    parser.add_argument('--kernel_size', type=int, default=3, help='kernel size')\n",
    "    parser.add_argument('--batch_size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: )')\n",
    "    parser.add_argument('--max-epochs', type=int, default=2, metavar='N',\n",
    "                        help='number of epochs to train (default: 0)')\n",
    "    parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
    "                        help='learning rate (default: 0.0)')\n",
    "\n",
    "    parser.add_argument('--accelerator', default='gpu', type=str, metavar='N')\n",
    "    parser.add_argument('--devices', default=1, type=int, metavar='N')\n",
    "    parser.add_argument('--dataset', default='cifar10', type=str, metavar='N')\n",
    "    parser.add_argument('--num_workers', default=4, type=int, metavar='N')\n",
    "    args = parser.parse_args(\"\")\n",
    "    return args\n",
    "\n",
    "args = get_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer, LightningDataModule\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchmetrics.functional import accuracy\n",
    "from einops import rearrange\n",
    "from torch import nn\n",
    "from torchvision.datasets.cifar import CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Module\n",
    "\n",
    "The `Attention` module is the core of the vision transformer model. It implements the attention mechanism:\n",
    "\n",
    "1) Multiply QKV by their weights\n",
    "2) Perform dot product on Q and K. \n",
    "3) Normalize the result in 2) by sqrt of `head_dim`  \n",
    "4) Softmax is applied to the result.\n",
    "5) Perform dot product on the result of 4) and V and the result is the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Module\n",
    "\n",
    "The MLP module is a made of two linear layers. A non-linear activation is applied to the output of the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "      \n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Block Module\n",
    "\n",
    "The `Block` module represents one encoder transformer block. It consists of two sub-modules:\n",
    "1) The Attention module\n",
    "2) The MLP module\n",
    "\n",
    "Layer norm is applied before and after the Attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, dim, num_heads, mlp_ratio=4., qkv_bias=False, \n",
    "            act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias) \n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer) \n",
    "   \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Transformer Module\n",
    "\n",
    "The feature encoder is made of several transformer blocks. The most important attributes are:\n",
    "1) `depth` : representing the number of encoder blocks\n",
    "2) `num_heads` : representing the number of attention heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_blocks, mlp_ratio=4., qkv_bias=False,  \n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([Block(dim, num_heads, mlp_ratio, qkv_bias, \n",
    "                                     act_layer, norm_layer) for _ in range(num_blocks)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The optional parameter initialization as adopted from `timm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights_vit_timm(module: nn.Module):\n",
    "    \"\"\" ViT weight initialization, original timm impl (for reproducibility) \"\"\"\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.trunc_normal_(module.weight, mean=0.0, std=0.02)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif hasattr(module, 'init_weights'):\n",
    "        module.init_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Lightning for CIFAR10 Image Classification\n",
    "\n",
    "We use the `Transformer` module to build the feature encoder. Before the `Transformer` can be used, we convert the input image into patches. The patches are then embedded into a linear space. The output is then passed to the Transformer.\n",
    "\n",
    "Another difference between this model is we use all output features for the final classification. In the ViT, only the first feature is used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LitTransformer(LightningModule):\n",
    "    def __init__(self, num_classes=10, lr=0.001, max_epochs=30, depth=12, embed_dim=64,\n",
    "                 head=4, patch_dim=192, seqlen=16, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.encoder = Transformer(dim=embed_dim, num_heads=head, num_blocks=depth, mlp_ratio=4.,\n",
    "                                   qkv_bias=False, act_layer=nn.GELU, norm_layer=nn.LayerNorm)\n",
    "        self.embed = torch.nn.Linear(patch_dim, embed_dim)\n",
    "\n",
    "        self.fc = nn.Linear(seqlen * embed_dim, num_classes)\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init_weights_vit_timm(self)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear projection\n",
    "        x = self.embed(x)\n",
    "            \n",
    "        # Encoder\n",
    "        x = self.encoder(x)\n",
    "        x = x.flatten(start_dim=1)\n",
    "\n",
    "        # Classification head\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        # this decays the learning rate to 0 after max_epochs using cosine annealing\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=self.hparams.max_epochs)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        acc = accuracy(y_hat, y)\n",
    "        return {\"y_hat\": y_hat, \"test_loss\": loss, \"test_acc\": acc}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"test_loss\"] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x[\"test_acc\"] for x in outputs]).mean()\n",
    "        self.log(\"test_loss\", avg_loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_acc\", avg_acc*100., on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.test_step(batch, batch_idx)\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        return self.test_epoch_end(outputs)\n",
    "\n",
    "\n",
    "# a lightning data module for cifar 10 dataset\n",
    "class LitCifar10(LightningDataModule):\n",
    "    def __init__(self, batch_size=32, num_workers=32, patch_num=4, **kwargs):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.patch_num = patch_num\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.train_set = CIFAR10(root='./data', train=True,\n",
    "                                 download=True, transform=torchvision.transforms.ToTensor())\n",
    "        self.test_set = CIFAR10(root='./data', train=False,\n",
    "                                download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        x, y = zip(*batch)\n",
    "        x = torch.stack(x, dim=0)\n",
    "        y = torch.LongTensor(y)\n",
    "        # x = rearrange(x, 'b c (p1 h) (p2 w) -> b (p1 p2) (c h w)', p1=self.patch_num, p2=self.patch_num)\n",
    "        return x, y\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.train_set, batch_size=self.batch_size, \n",
    "                                        shuffle=True, collate_fn=self.collate_fn,\n",
    "                                        num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.test_set, batch_size=self.batch_size, \n",
    "                                        shuffle=False, collate_fn=self.collate_fn,\n",
    "                                        num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.test_dataloader()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on different settings\n",
    "\n",
    "The following table shows different performances on different settings. Generally, Transformer is better than MLP in terms of accuracy and parameter count. However, the performance is worse compared to CNN models. \n",
    "\n",
    "However, the most important thing to note is that Transformers are more general purpose models than CNNs. They can process different types of data. They can process multiple types of data at the same time. This is why they are considered to be the backbone of many high-performing models like BERT, GPT3, PalM and Gato.\n",
    "\n",
    "| **Depth** | **Head** | **Embed dim** | **Patch size** | **Seq len** | **Params** | **Accuracy** | \n",
    "| -: | -: | -: | -: | -: | -: | -: |\n",
    "| 12 | 4 | 32 | 4x4 | 64 | 173k | 68.2% |\n",
    "| 12 | 4 | 64 | 4x4 | 64 | 641k | 71.1% |\n",
    "| 12 | 4 | 128 | 4x4 | 64 | 2.5M | 71.5% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed dim: 64\n",
      "Patch size: 4\n",
      "Sequence length: 32\n",
      "LitTransformer(\n",
      "  (encoder): Transformer(\n",
      "    (blocks): ModuleList(\n",
      "      (0): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (2): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (3): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (4): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (5): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (6): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (7): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (8): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (9): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (10): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (11): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (embed): Linear(in_features=32, out_features=64, bias=True)\n",
      "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
      "  (loss): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "datamodule = LitCifar10(batch_size=args.batch_size,\n",
    "                        patch_num=args.patch_num, \n",
    "                        num_workers=args.num_workers * args.devices)\n",
    "datamodule.prepare_data()\n",
    "\n",
    "data = iter(datamodule.train_dataloader()).next()\n",
    "patch_dim = data[0].shape[-1]\n",
    "seqlen = data[0].shape[-2]\n",
    "print(\"Embed dim:\", args.embed_dim)\n",
    "print(\"Patch size:\", 32 // args.patch_num)\n",
    "print(\"Sequence length:\", seqlen)\n",
    "\n",
    "\n",
    "model = LitTransformer(num_classes=10, lr=args.lr, epochs=args.max_epochs, \n",
    "                        depth=args.depth, embed_dim=args.embed_dim, head=args.num_heads,\n",
    "                        patch_dim=patch_dim, seqlen=seqlen,)\n",
    "print(model)\n",
    "trainer = Trainer(accelerator=args.accelerator, devices=args.devices,\n",
    "                    max_epochs=args.max_epochs, precision=16 if args.accelerator == 'gpu' else 32,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo.ipynb Cell 20'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo.ipynb#ch0000019?line=0'>1</a>\u001b[0m t \u001b[39m=\u001b[39m datamodule\u001b[39m.\u001b[39mtrain_dataloader()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo.ipynb#ch0000019?line=1'>2</a>\u001b[0m t\u001b[39m.\u001b[39;49mdataset[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "t = datamodule.train_dataloader()\n",
    "t.dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.dataset[0]["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = iter(datamodule.train_dataloader()).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.2353, 0.1882, 0.2314,  ..., 0.2078, 0.2235, 0.2510],\n",
       "          [0.3843, 0.3098, 0.2118,  ..., 0.3333, 0.2157, 0.2824],\n",
       "          [0.2784, 0.2549, 0.3647,  ..., 0.1647, 0.1569, 0.2275],\n",
       "          ...,\n",
       "          [0.1059, 0.1294, 0.2000,  ..., 0.3608, 0.6392, 0.4980],\n",
       "          [0.3020, 0.4078, 0.1765,  ..., 0.3647, 0.2510, 0.5098],\n",
       "          [0.4392, 0.4941, 0.5412,  ..., 0.3961, 0.4706, 0.6000]],\n",
       " \n",
       "         [[0.7059, 0.6980, 0.7216,  ..., 0.3608, 0.3686, 0.3569],\n",
       "          [0.6980, 0.7020, 0.7176,  ..., 0.3686, 0.3647, 0.3725],\n",
       "          [0.6980, 0.7216, 0.7373,  ..., 0.3882, 0.4039, 0.4118],\n",
       "          ...,\n",
       "          [0.4235, 0.4784, 0.4706,  ..., 0.3373, 0.3255, 0.3490],\n",
       "          [0.4941, 0.5882, 0.5137,  ..., 0.3255, 0.3490, 0.3412],\n",
       "          [0.5725, 0.4980, 0.4588,  ..., 0.3373, 0.2863, 0.2745]],\n",
       " \n",
       "         [[0.2275, 0.2471, 0.2275,  ..., 0.2667, 0.2275, 0.2510],\n",
       "          [0.2314, 0.2353, 0.2235,  ..., 0.2235, 0.2824, 0.4000],\n",
       "          [0.2510, 0.2784, 0.3686,  ..., 0.1961, 0.2353, 0.2980],\n",
       "          ...,\n",
       "          [0.9569, 0.9647, 0.9490,  ..., 0.9725, 0.9765, 0.9804],\n",
       "          [0.9608, 0.9765, 0.9765,  ..., 0.9765, 0.9765, 0.9765],\n",
       "          [0.9686, 0.9608, 0.9608,  ..., 0.9843, 0.9725, 0.9608]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.5686, 0.5569, 0.6196,  ..., 0.8863, 0.7725, 0.7412],\n",
       "          [0.5098, 0.4471, 0.4471,  ..., 0.7098, 0.7059, 0.5216],\n",
       "          [0.4588, 0.4078, 0.3686,  ..., 0.5333, 0.5294, 0.4941],\n",
       "          ...,\n",
       "          [0.6157, 0.6275, 0.6314,  ..., 0.3529, 0.3725, 0.5059],\n",
       "          [0.7412, 0.7686, 0.7412,  ..., 0.6078, 0.5294, 0.6431],\n",
       "          [0.6667, 0.6980, 0.6627,  ..., 0.7765, 0.6353, 0.5608]],\n",
       " \n",
       "         [[0.4588, 0.4627, 0.4784,  ..., 0.8431, 0.8392, 0.8588],\n",
       "          [0.4980, 0.5059, 0.5176,  ..., 0.9216, 0.9098, 0.8980],\n",
       "          [0.5373, 0.5490, 0.5647,  ..., 0.9059, 0.9137, 0.9333],\n",
       "          ...,\n",
       "          [0.6824, 0.6784, 0.6667,  ..., 0.6000, 0.6078, 0.5922],\n",
       "          [0.6667, 0.6627, 0.6431,  ..., 0.6000, 0.6000, 0.5686],\n",
       "          [0.6235, 0.6157, 0.5843,  ..., 0.3373, 0.2902, 0.2667]],\n",
       " \n",
       "         [[0.5647, 0.3137, 0.2235,  ..., 0.3725, 0.2627, 0.2588],\n",
       "          [0.2902, 0.2824, 0.2980,  ..., 0.3529, 0.3333, 0.3529],\n",
       "          [0.6471, 0.6627, 0.5725,  ..., 0.3686, 0.3490, 0.4314],\n",
       "          ...,\n",
       "          [0.4314, 0.4627, 0.4863,  ..., 0.3725, 0.4824, 0.5882],\n",
       "          [0.4824, 0.3882, 0.4000,  ..., 0.5686, 0.5216, 0.5725],\n",
       "          [0.3255, 0.4314, 0.4353,  ..., 0.5137, 0.4902, 0.4980]]]),\n",
       " tensor([6, 6, 5, 0, 6, 3, 7, 6, 4, 2, 5, 1, 4, 2, 4, 0, 5, 2, 4, 8, 1, 6, 1, 9,\n",
       "         6, 0, 7, 6, 0, 0, 9, 7, 1, 6, 7, 1, 5, 9, 4, 1, 3, 5, 7, 0, 1, 4, 4, 0,\n",
       "         6, 3, 0, 5, 4, 4, 8, 3, 4, 3, 9, 5, 9, 8, 8, 3]))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64, 48])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo.ipynb Cell 26'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo.ipynb#ch0000030?line=0'>1</a>\u001b[0m data[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mnumpy()\u001b[39m.\u001b[39;49mtranspose(\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mValueError\u001b[0m: axes don't match array"
     ]
    }
   ],
   "source": [
    "data[0][0].numpy().transpose(1, 2, 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# plt.imshow(data[0][0].numpy().transpose(1, 2, 0))\n",
    "plt.imshow(data[0][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
