{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer for CIFAR10\n",
    "\n",
    "A configurable transformer model will be used for CIFAR10 image classification. \n",
    "\n",
    "The vision transformer model is a modified version of ViT. The changes are:\n",
    "1) No position embedding.\n",
    "2) No dropout is used.\n",
    "3) All encoder features are used for class prediction.\n",
    "\n",
    "The code below is a simplified version of Timm modules.\n",
    "\n",
    "Let us import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install ipykernel pytorch-lightning  pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch -c conda-forge --update-deps --force-reinstall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dl/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torchvision\n",
    "\n",
    "# from argparse import ArgumentParser\n",
    "# from pytorch_lightning import LightningModule, Trainer, LightningDataModule\n",
    "# from torch.optim import Adam\n",
    "# from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "# from torchmetrics.functional import accuracy\n",
    "# from einops import rearrange\n",
    "# from torch import nn\n",
    "# from torchvision.datasets.cifar import CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Module\n",
    "\n",
    "The `Attention` module is the core of the vision transformer model. It implements the attention mechanism:\n",
    "\n",
    "1) Multiply QKV by their weights\n",
    "2) Perform dot product on Q and K. \n",
    "3) Normalize the result in 2) by sqrt of `head_dim`  \n",
    "4) Softmax is applied to the result.\n",
    "5) Perform dot product on the result of 4) and V and the result is the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Module\n",
    "\n",
    "The MLP module is a made of two linear layers. A non-linear activation is applied to the output of the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "      \n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Block Module\n",
    "\n",
    "The `Block` module represents one encoder transformer block. It consists of two sub-modules:\n",
    "1) The Attention module\n",
    "2) The MLP module\n",
    "\n",
    "Layer norm is applied before and after the Attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, dim, num_heads, mlp_ratio=4., qkv_bias=False, \n",
    "            act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias) \n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer) \n",
    "   \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Transformer Module\n",
    "\n",
    "The feature encoder is made of several transformer blocks. The most important attributes are:\n",
    "1) `depth` : representing the number of encoder blocks\n",
    "2) `num_heads` : representing the number of attention heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_blocks, mlp_ratio=4., qkv_bias=False,  \n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([Block(dim, num_heads, mlp_ratio, qkv_bias, \n",
    "                                     act_layer, norm_layer) for _ in range(num_blocks)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The optional parameter initialization as adopted from `timm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights_vit_timm(module: nn.Module):\n",
    "    \"\"\" ViT weight initialization, original timm impl (for reproducibility) \"\"\"\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.trunc_normal_(module.weight, mean=0.0, std=0.02)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif hasattr(module, 'init_weights'):\n",
    "        module.init_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Lightning for CIFAR10 Image Classification\n",
    "\n",
    "We use the `Transformer` module to build the feature encoder. Before the `Transformer` can be used, we convert the input image into patches. The patches are then embedded into a linear space. The output is then passed to the Transformer.\n",
    "\n",
    "Another difference between this model is we use all output features for the final classification. In the ViT, only the first feature is used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LightningModule' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo.ipynb#ch0000014?line=0'>1</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mLitTransformer\u001b[39;00m(LightningModule):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo.ipynb#ch0000014?line=1'>2</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, num_classes\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m, max_epochs\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m, depth\u001b[39m=\u001b[39m\u001b[39m12\u001b[39m, embed_dim\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo.ipynb#ch0000014?line=2'>3</a>\u001b[0m                  head\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, patch_dim\u001b[39m=\u001b[39m\u001b[39m192\u001b[39m, seqlen\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo.ipynb#ch0000014?line=3'>4</a>\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LightningModule' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class LitTransformer(LightningModule):\n",
    "    def __init__(self, num_classes=10, lr=0.001, max_epochs=30, depth=12, embed_dim=64,\n",
    "                 head=4, patch_dim=192, seqlen=16, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.encoder = Transformer(dim=embed_dim, num_heads=head, num_blocks=depth, mlp_ratio=4.,\n",
    "                                   qkv_bias=False, act_layer=nn.GELU, norm_layer=nn.LayerNorm)\n",
    "        self.embed = torch.nn.Linear(patch_dim, embed_dim)\n",
    "\n",
    "        self.fc = nn.Linear(seqlen * embed_dim, num_classes)\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init_weights_vit_timm(self)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear projection\n",
    "        x = self.embed(x)\n",
    "            \n",
    "        # Encoder\n",
    "        x = self.encoder(x)\n",
    "        x = x.flatten(start_dim=1)\n",
    "\n",
    "        # Classification head\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        # this decays the learning rate to 0 after max_epochs using cosine annealing\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=self.hparams.max_epochs)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        acc = accuracy(y_hat, y)\n",
    "        return {\"y_hat\": y_hat, \"test_loss\": loss, \"test_acc\": acc}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"test_loss\"] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x[\"test_acc\"] for x in outputs]).mean()\n",
    "        self.log(\"test_loss\", avg_loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_acc\", avg_acc*100., on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.test_step(batch, batch_idx)\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        return self.test_epoch_end(outputs)\n",
    "\n",
    "\n",
    "# a lightning data module for cifar 10 dataset\n",
    "class LitCifar10(LightningDataModule):\n",
    "    def __init__(self, batch_size=32, num_workers=32, patch_num=4, **kwargs):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.patch_num = patch_num\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.train_set = CIFAR10(root='./data', train=True,\n",
    "                                 download=True, transform=torchvision.transforms.ToTensor())\n",
    "        self.test_set = CIFAR10(root='./data', train=False,\n",
    "                                download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        x, y = zip(*batch)\n",
    "        x = torch.stack(x, dim=0)\n",
    "        y = torch.LongTensor(y)\n",
    "        x = rearrange(x, 'b c (p1 h) (p2 w) -> b (p1 p2) (c h w)', p1=self.patch_num, p2=self.patch_num)\n",
    "        return x, y\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.train_set, batch_size=self.batch_size, \n",
    "                                        shuffle=True, collate_fn=self.collate_fn,\n",
    "                                        num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.test_set, batch_size=self.batch_size, \n",
    "                                        shuffle=False, collate_fn=self.collate_fn,\n",
    "                                        num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.test_dataloader()\n",
    "\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = ArgumentParser(description='PyTorch Transformer')\n",
    "    parser.add_argument('--depth', type=int, default=12, help='depth')\n",
    "    parser.add_argument('--embed_dim', type=int, default=64, help='embedding dimension')\n",
    "    parser.add_argument('--num_heads', type=int, default=4, help='num_heads')\n",
    "\n",
    "    parser.add_argument('--patch_num', type=int, default=8, help='patch_num')\n",
    "    parser.add_argument('--kernel_size', type=int, default=3, help='kernel size')\n",
    "    parser.add_argument('--batch_size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: )')\n",
    "    parser.add_argument('--max-epochs', type=int, default=30, metavar='N',\n",
    "                        help='number of epochs to train (default: 0)')\n",
    "    parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
    "                        help='learning rate (default: 0.0)')\n",
    "\n",
    "    parser.add_argument('--accelerator', default='gpu', type=str, metavar='N')\n",
    "    parser.add_argument('--devices', default=1, type=int, metavar='N')\n",
    "    parser.add_argument('--dataset', default='cifar10', type=str, metavar='N')\n",
    "    parser.add_argument('--num_workers', default=4, type=int, metavar='N')\n",
    "    args = parser.parse_args(\"\")\n",
    "    return args\n",
    "\n",
    "args = get_args()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on different settings\n",
    "\n",
    "The following table shows different performances on different settings. Generally, Transformer is better than MLP in terms of accuracy and parameter count. However, the performance is worse compared to CNN models. \n",
    "\n",
    "However, the most important thing to note is that Transformers are more general purpose models than CNNs. They can process different types of data. They can process multiple types of data at the same time. This is why they are considered to be the backbone of many high-performing models like BERT, GPT3, PalM and Gato.\n",
    "\n",
    "| **Depth** | **Head** | **Embed dim** | **Patch size** | **Seq len** | **Params** | **Accuracy** | \n",
    "| -: | -: | -: | -: | -: | -: | -: |\n",
    "| 12 | 4 | 32 | 4x4 | 64 | 173k | 68.2% |\n",
    "| 12 | 4 | 64 | 4x4 | 64 | 641k | 71.1% |\n",
    "| 12 | 4 | 128 | 4x4 | 64 | 2.5M | 71.5% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     args = get_args()\n",
    "\n",
    "#     datamodule = LitCifar10(batch_size=args.batch_size,\n",
    "#                             patch_num=args.patch_num, \n",
    "#                             num_workers=args.num_workers * args.devices)\n",
    "#     datamodule.prepare_data()\n",
    "\n",
    "#     data = iter(datamodule.train_dataloader()).next()\n",
    "#     patch_dim = data[0].shape[-1]\n",
    "#     seqlen = data[0].shape[-2]\n",
    "#     print(\"Embed dim:\", args.embed_dim)\n",
    "#     print(\"Patch size:\", 32 // args.patch_num)\n",
    "#     print(\"Sequence length:\", seqlen)\n",
    "\n",
    "\n",
    "#     model = LitTransformer(num_classes=10, lr=args.lr, epochs=args.max_epochs, \n",
    "#                            depth=args.depth, embed_dim=args.embed_dim, head=args.num_heads,\n",
    "#                            patch_dim=patch_dim, seqlen=seqlen,)\n",
    "\n",
    "#     trainer = Trainer(accelerator=args.accelerator, devices=args.devices,\n",
    "#                       max_epochs=args.max_epochs, precision=16 if args.accelerator == 'gpu' else 32,)\n",
    "#     trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed dim: 64\n",
      "Patch size: 4\n",
      "Sequence length: 64\n",
      "LitTransformer(\n",
      "  (encoder): Transformer(\n",
      "    (blocks): ModuleList(\n",
      "      (0): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (2): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (3): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (4): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (5): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (6): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (7): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (8): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (9): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (10): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (11): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (embed): Linear(in_features=48, out_features=64, bias=True)\n",
      "  (fc): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  (loss): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "datamodule = LitCifar10(batch_size=args.batch_size,\n",
    "                        patch_num=args.patch_num, \n",
    "                        num_workers=args.num_workers * args.devices)\n",
    "datamodule.prepare_data()\n",
    "\n",
    "data = iter(datamodule.train_dataloader()).next()\n",
    "patch_dim = data[0].shape[-1]\n",
    "seqlen = data[0].shape[-2]\n",
    "print(\"Embed dim:\", args.embed_dim)\n",
    "print(\"Patch size:\", 32 // args.patch_num)\n",
    "print(\"Sequence length:\", seqlen)\n",
    "\n",
    "\n",
    "model = LitTransformer(num_classes=10, lr=args.lr, epochs=args.max_epochs, \n",
    "                        depth=args.depth, embed_dim=args.embed_dim, head=args.num_heads,\n",
    "                        patch_dim=patch_dim, seqlen=seqlen,)\n",
    "print(model)\n",
    "trainer = Trainer(accelerator=args.accelerator, devices=args.devices,\n",
    "                    max_epochs=args.max_epochs, precision=16 if args.accelerator == 'gpu' else 32,)\n",
    "# trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params\n",
      "---------------------------------------------\n",
      "0 | encoder | Transformer      | 597 K \n",
      "1 | embed   | Linear           | 3.1 K \n",
      "2 | fc      | Linear           | 41.0 K\n",
      "3 | loss    | CrossEntropyLoss | 0     \n",
      "---------------------------------------------\n",
      "641 K     Trainable params\n",
      "0         Non-trainable params\n",
      "641 K     Total params\n",
      "1.283     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:  40%|████      | 380/939 [00:09<00:13, 40.77it/s, loss=0.557, v_num=4, test_loss=1.080, test_acc=67.20]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:724: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = datamodule.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.dataset[1][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
