{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: librosa==0.9.1 in /home/dl/.local/lib/python3.10/site-packages (from -r rqts_gpu.txt (line 1)) (0.9.1)\n",
      "Requirement already satisfied: numpy==1.22.4 in /home/dl/.local/lib/python3.10/site-packages (from -r rqts_gpu.txt (line 2)) (1.22.4)\n",
      "Collecting PySimpleGUI==4.60.1\n",
      "  Using cached PySimpleGUI-4.60.1-py3-none-any.whl (509 kB)\n",
      "Collecting sounddevice==0.4.4\n",
      "  Using cached sounddevice-0.4.4-py3-none-any.whl (31 kB)\n",
      "Collecting validators==0.19.0\n",
      "  Using cached validators-0.19.0.tar.gz (30 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.10.2+cu113 (from versions: 1.11.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.10.2+cu113\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r rqts_gpu.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torchaudio, torchvision\n",
    "import matplotlib.pyplot as plt \n",
    "import librosa\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "from einops import rearrange\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer, LightningDataModule, Callback\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from torchmetrics.functional import accuracy\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchaudio.datasets import SPEECHCOMMANDS\n",
    "from torchaudio.datasets.speechcommands import load_speechcommands_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/dl/Desktop/dl/object_detection_model_hw2/hw3/train copy.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/train%20copy.ipynb#ch0000003?line=7'>8</a>\u001b[0m \u001b[39m# make a dictionary from CLASSES to integers\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/train%20copy.ipynb#ch0000003?line=8'>9</a>\u001b[0m CLASS_TO_IDX \u001b[39m=\u001b[39m {c: i \u001b[39mfor\u001b[39;00m i, c \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(CLASSES)}\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/train%20copy.ipynb#ch0000003?line=10'>11</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(args\u001b[39m.\u001b[39mpath):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/train%20copy.ipynb#ch0000003?line=11'>12</a>\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(args\u001b[39m.\u001b[39mpath, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "CLASSES = ['silence', 'unknown', 'backward', 'bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'follow',\n",
    "            'forward', 'four', 'go', 'happy', 'house', 'learn', 'left', 'marvin', 'nine', 'no',\n",
    "            'off', 'on', 'one', 'right', 'seven', 'sheila', 'six', 'stop', 'three',\n",
    "            'tree', 'two', 'up', 'visual', 'wow', 'yes', 'zero']\n",
    "\n",
    "\n",
    "# make a dictionary from CLASSES to integers\n",
    "CLASS_TO_IDX = {c: i for i, c in enumerate(CLASSES)}\n",
    "\n",
    "if not os.path.exists(args.path):\n",
    "    os.makedirs(args.path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "def get_args():\n",
    "    parser = ArgumentParser(description='PyTorch Transformer')\n",
    "    parser.add_argument('--depth', type=int, default=12, help='depth')\n",
    "    parser.add_argument('--embed_dim', type=int, default=64, help='embedding dimension')\n",
    "    parser.add_argument('--num_heads', type=int, default=4, help='num_heads')\n",
    "\n",
    "\n",
    "\n",
    "    parser.add_argument('--batch-size', type=int, default=3, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "\n",
    "\n",
    "    parser.add_argument('--max-epochs', type=int, default=30, metavar='N',\n",
    "                        help='number of epochs to train (default: 30)')\n",
    "    parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
    "                        help='learning rate (default: 0.001)')\n",
    "    parser.add_argument('--patch_num', type=int, default=4, help='patch_num')\n",
    "    parser.add_argument('--kernel_size', type=int, default=3, help='kernel size')\n",
    "\n",
    "    parser.add_argument('--accelerator', default='gpu', type=str, metavar='N')\n",
    "    parser.add_argument('--devices', default=1, type=int, metavar='N')\n",
    "    # parser.add_argument('--dataset', default='cifar10', type=str, metavar='N')\n",
    "    parser.add_argument('--num_workers', default=4, type=int, metavar='N')\n",
    "\n",
    "\n",
    "    # where dataset will be stored\n",
    "    parser.add_argument(\"--path\", type=str, default=\"data/speech_commands/\")\n",
    "    # 35 keywords + silence + unknown\n",
    "    parser.add_argument(\"--num-classes\", type=int, default=37)\n",
    "\n",
    "    # mel spectrogram parameters\n",
    "    parser.add_argument(\"--n-fft\", type=int, default=1024)\n",
    "    parser.add_argument(\"--n-mels\", type=int, default=32)\n",
    "    parser.add_argument(\"--win-length\", type=int, default=None)\n",
    "    parser.add_argument(\"--hop-length\", type=int, default=512)\n",
    "    # parser.add_argument(\"--hop-length\", type=int, default=334)\n",
    "\n",
    "\n",
    "    # 16-bit fp model to reduce the size\n",
    "    parser.add_argument(\"--precision\", default=16)\n",
    "    # parser.add_argument(\"--accelerator\", default='gpu')\n",
    "    # parser.add_argument(\"--devices\", default=1)\n",
    "    # parser.add_argument(\"--num-workers\", type=int, default=96)\n",
    "\n",
    "    parser.add_argument(\"--no-wandb\", default=True, action='store_true')\n",
    "\n",
    "    \n",
    "    args = parser.parse_args(\"\")\n",
    "    return args\n",
    "args = get_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lightning Data module basic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SilenceDataset(SPEECHCOMMANDS):\n",
    "    def __init__(self, root):\n",
    "        super(SilenceDataset, self).__init__(root, subset='training')\n",
    "        self.len = len(self._walker) // 35\n",
    "        path = os.path.join(self._path, torchaudio.datasets.speechcommands.EXCEPT_FOLDER)\n",
    "        self.paths = [os.path.join(path, p) for p in os.listdir(path) if p.endswith('.wav')]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = np.random.randint(0, len(self.paths))\n",
    "        filepath = self.paths[index]\n",
    "        waveform, sample_rate = torchaudio.load(filepath)\n",
    "        return waveform, sample_rate, \"silence\", 0, 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "class UnknownDataset(SPEECHCOMMANDS):\n",
    "    def __init__(self, root):\n",
    "        super(UnknownDataset, self).__init__(root, subset='training')\n",
    "        self.len = len(self._walker) // 35\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = np.random.randint(0, len(self._walker))\n",
    "        fileid = self._walker[index]\n",
    "        waveform, sample_rate, _, speaker_id, utterance_number = load_speechcommands_item(fileid, self._path)\n",
    "        return waveform, sample_rate, \"unknown\", speaker_id, utterance_number\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class KWSDataModule(LightningDataModule):\n",
    "    def __init__(self, path, batch_size=128, num_workers=0, patch_num=4\n",
    "                , n_fft=512, n_mels=128, win_length=None, hop_length=256\n",
    "                , class_dict={}\n",
    "                , **kwargs):\n",
    "\n",
    "        print('_'*20, 'kws init')\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self.path = path\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.patch_num = patch_num\n",
    "\n",
    "        # Window\n",
    "        self.n_fft = n_fft\n",
    "        self.n_mels = n_mels\n",
    "        self.win_length = win_length\n",
    "        self.hop_length = hop_length\n",
    "        self.class_dict = class_dict\n",
    "\n",
    "    def prepare_data(self):\n",
    "        print('_'*20, 'kws prep data')\n",
    "        self.train_dataset = torchaudio.datasets.SPEECHCOMMANDS(self.path,\n",
    "                                                                download=True,\n",
    "                                                                subset='training')\n",
    "\n",
    "        silence_dataset = SilenceDataset(self.path)\n",
    "        unknown_dataset = UnknownDataset(self.path)\n",
    "        # self.train_dataset = torch.utils.data.ConcatDataset([self.train_dataset, silence_dataset, unknown_dataset])\n",
    "        self.train_dataset = torch.utils.data.ConcatDataset([self.train_dataset, silence_dataset, unknown_dataset])\n",
    "\n",
    "        self.train_set = torch.utils.data.ConcatDataset([self.train_dataset, silence_dataset, unknown_dataset])\n",
    "\n",
    "\n",
    "\n",
    "        self.val_set = torchaudio.datasets.SPEECHCOMMANDS(self.path, download=True, subset='validation')\n",
    "        self.val_dataset = torchaudio.datasets.SPEECHCOMMANDS(self.path,\n",
    "                                                              download=True,\n",
    "                                                              subset='validation')\n",
    "\n",
    "        self.test_set = torchaudio.datasets.SPEECHCOMMANDS(self.path, download=True, subset='testing')      \n",
    "        self.test_dataset = torchaudio.datasets.SPEECHCOMMANDS(self.path,\n",
    "                                                               download=True,\n",
    "                                                               subset='testing')     \n",
    "\n",
    "        _, sample_rate, _, _, _ = self.train_dataset[0]\n",
    "        self.sample_rate = sample_rate\n",
    "        self.transform = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate,\n",
    "                                                              n_fft=self.n_fft,\n",
    "                                                              win_length=self.win_length,\n",
    "                                                              hop_length=self.hop_length,\n",
    "                                                              n_mels=self.n_mels,\n",
    "                                                              power=2.0)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        print('_'*20, 'setup ')\n",
    "        self.prepare_data()\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        print('_'*20, 'kws train dataloader')\n",
    "        return torch.utils.data.DataLoader( self.train_dataset,\n",
    "                                            batch_size=self.batch_size,\n",
    "                                            num_workers=self.num_workers,\n",
    "                                            shuffle=True,\n",
    "                                            pin_memory=True,\n",
    "                                            collate_fn=self.collate_fn\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        print('_'*20, 'kws val dataloader')\n",
    "        return torch.utils.data.DataLoader( self.val_dataset,\n",
    "                                            batch_size=self.batch_size,\n",
    "                                            num_workers=self.num_workers,\n",
    "                                            shuffle=False,\n",
    "                                            pin_memory=True,\n",
    "                                            collate_fn=self.collate_fn\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        print('_'*20, 'kws test dataloader')\n",
    "        return torch.utils.data.DataLoader( self.test_dataset,\n",
    "                                            batch_size=self.batch_size,\n",
    "                                            num_workers=self.num_workers,\n",
    "                                            shuffle=True,\n",
    "                                            pin_memory=True,\n",
    "                                            collate_fn=self.collate_fn\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        mels = []\n",
    "        xmels = []\n",
    "        labels = []\n",
    "        wavs = []\n",
    "        for sample in batch:\n",
    "            waveform, sample_rate, label, speaker_id, utterance_number = sample\n",
    "            # ensure that all waveforms are 1sec in length; if not pad with zeros\n",
    "            if waveform.shape[-1] < sample_rate:\n",
    "                waveform = torch.cat([waveform, torch.zeros((1, sample_rate - waveform.shape[-1]))], dim=-1)\n",
    "            elif waveform.shape[-1] > sample_rate:\n",
    "                waveform = waveform[:,:sample_rate]\n",
    "\n",
    "            # mel from power to db\n",
    "            mel1 = ToTensor()(librosa.power_to_db(self.transform(waveform).squeeze().numpy(), ref=np.max))\n",
    "\n",
    "            # print('mel1 shapre', mel1.shape)\n",
    "            # mel1 = rearrange(mel1, 'c h w -> c h w', p1=self.patch_num, p2=self.patch_num)\n",
    "            # print('mel1 shapre', mel1.shape)\n",
    "\n",
    "            xmels.append(xmels)\n",
    "            mels.append(mel1)\n",
    "            labels.append(torch.tensor(self.class_dict[label]))\n",
    "            wavs.append(waveform)\n",
    "\n",
    "        mels = torch.stack(mels)\n",
    "        labels = torch.stack(labels)\n",
    "        wavs = torch.stack(wavs)\n",
    "\n",
    "        # print('mels sh:', mels.shape)\n",
    "        # x = rearrange(mels, 'b 1 h w -> b h w')\n",
    "        x = mels\n",
    "        x = rearrange(x, 'b c (p1 h) (p2 w) -> b (p1 p2) (c h w)', p1=self.patch_num, p2=self.patch_num)\n",
    "        # print('x sh:', x.shape)\n",
    "        # return x, labels, wavs, sample_rate\n",
    "\n",
    "        return x, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ kws init\n",
      "____________________ kws prep data\n",
      "____________________ kws train dataloader\n"
     ]
    }
   ],
   "source": [
    "\n",
    "datamodule = KWSDataModule(batch_size=args.batch_size, num_workers=args.num_workers,\n",
    "                        path=args.path, n_fft=args.n_fft, n_mels=args.n_mels,\n",
    "                        win_length=args.win_length, hop_length=args.hop_length,\n",
    "                        class_dict=CLASS_TO_IDX)\n",
    "datamodule.prepare_data()\n",
    "data = iter(datamodule.train_dataloader()).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 16, 64])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# x = data[0][i]\n",
    "# y = data[1][i]\n",
    "# x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/dl/Desktop/dl/object_detection_model_hw2/hw3/train.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/train.ipynb#ch0000006?line=1'>2</a>\u001b[0m x \u001b[39m=\u001b[39m data[\u001b[39m0\u001b[39m][i]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/train.ipynb#ch0000006?line=2'>3</a>\u001b[0m y \u001b[39m=\u001b[39m data[\u001b[39m1\u001b[39m][i]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/train.ipynb#ch0000006?line=3'>4</a>\u001b[0m w \u001b[39m=\u001b[39m data[\u001b[39m2\u001b[39;49m][i]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/train.ipynb#ch0000006?line=4'>5</a>\u001b[0m sample_rate \u001b[39m=\u001b[39m data[\u001b[39m3\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# i = 0\n",
    "# x = data[0][i]\n",
    "# y = data[1][i]\n",
    "# w = data[2][i]\n",
    "# sample_rate = data[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "# print(args.hop_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 48])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate,\n",
    "#                                                               n_fft=args.n_fft,\n",
    "#                                                               win_length=args.win_length,\n",
    "#                                                               hop_length=334,\n",
    "#                                                               n_mels=args.n_mels,\n",
    "#                                                               power=2.0)\n",
    "# mel1 = ToTensor()(librosa.power_to_db(transform(w).squeeze().numpy(), ref=np.max))\n",
    "# mel1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fs = sample_rate\n",
    "# Time = np.linspace(0, len(w) / fs, num=len(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# fig = plt.figure\n",
    "# # plt.figure(1)\n",
    "# # plt.imshow(data[0][0][0]) # mels\n",
    "# plt.imshow(mel1[0]) # mels\n",
    "\n",
    "# # plt.plot(Time, w)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PL LigtningMOdule MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed dim: 64\n",
      "Patch size: 8\n",
      "Sequence length: 16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # data = iter(datamodule.test_dataloader()).next()\n",
    "# patch_dim = data[0].shape[-1]\n",
    "# seqlen = data[0].shape[-2]\n",
    "# print(\"Embed dim:\", args.embed_dim)\n",
    "# print(\"Patch size:\", 32 // args.patch_num)\n",
    "# print(\"Sequence length:\", seqlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f07a23ff0a0>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# fig = plt.figure\n",
    "# # plt.imshow(data[0][0][0]) # mels\n",
    "# plt.imshow(x) # mels\n",
    "# # [1] = labels\n",
    "# # [2] wav\n",
    "# # plt.imshow(data[2][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to ewdit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "      \n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, dim, num_heads, mlp_ratio=4., qkv_bias=False, \n",
    "            act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias) \n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer) \n",
    "   \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_blocks, mlp_ratio=4., qkv_bias=False,  \n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([Block(dim, num_heads, mlp_ratio, qkv_bias, \n",
    "                                     act_layer, norm_layer) for _ in range(num_blocks)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights_vit_timm(module: nn.Module):\n",
    "    \"\"\" ViT weight initialization, original timm impl (for reproducibility) \"\"\"\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.trunc_normal_(module.weight, mean=0.0, std=0.02)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif hasattr(module, 'init_weights'):\n",
    "        module.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "class LitTransformer(LightningModule):\n",
    "    def __init__(self, num_classes=10, lr=0.001, max_epochs=30, depth=12, embed_dim=64,\n",
    "                 head=4, patch_dim=192, seqlen=16, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.encoder = Transformer(dim=embed_dim, num_heads=head, num_blocks=depth, mlp_ratio=4.,\n",
    "                                   qkv_bias=False, act_layer=nn.GELU, norm_layer=nn.LayerNorm)\n",
    "        self.embed = torch.nn.Linear(patch_dim, embed_dim)\n",
    "\n",
    "        self.fc = nn.Linear(seqlen * embed_dim, num_classes)\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init_weights_vit_timm(self)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear projection\n",
    "        x = self.embed(x)\n",
    "            \n",
    "        # Encoder\n",
    "        x = self.encoder(x)\n",
    "        x = x.flatten(start_dim=1)\n",
    "\n",
    "        # Classification head\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        # this decays the learning rate to 0 after max_epochs using cosine annealing\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=self.hparams.max_epochs)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        acc = accuracy(y_hat, y)\n",
    "        return {\"y_hat\": y_hat, \"test_loss\": loss, \"test_acc\": acc}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"test_loss\"] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x[\"test_acc\"] for x in outputs]).mean()\n",
    "        self.log(\"test_loss\", avg_loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_acc\", avg_acc*100., on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.test_step(batch, batch_idx)\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        return self.test_epoch_end(outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WandbCallback(Callback):\n",
    "\n",
    "    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n",
    "        # log 10 sample audio predictions from the first batch\n",
    "        if batch_idx == 0:\n",
    "            n = 10\n",
    "            mels, labels, wavs = batch\n",
    "            preds = outputs[\"preds\"]\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "\n",
    "            labels = labels.cpu().numpy()\n",
    "            preds = preds.cpu().numpy()\n",
    "            \n",
    "            wavs = torch.squeeze(wavs, dim=1)\n",
    "            wavs = [ (wav.cpu().numpy()*32768.0).astype(\"int16\") for wav in wavs]\n",
    "            \n",
    "            sample_rate = pl_module.hparams.sample_rate\n",
    "            idx_to_class = pl_module.hparams.idx_to_class\n",
    "            \n",
    "            # log audio samples and predictions as a W&B Table\n",
    "            columns = ['audio', 'mel', 'ground truth', 'prediction']\n",
    "            data = [[wandb.Audio(wav, sample_rate=sample_rate), wandb.Image(mel), idx_to_class[label], idx_to_class[pred]] for wav, mel, label, pred in list(\n",
    "                zip(wavs[:n], mels[:n], labels[:n], preds[:n]))]\n",
    "            wandb_logger.log_table(\n",
    "                key='ResNet18 on KWS using PyTorch Lightning',\n",
    "                columns=columns,\n",
    "                data=data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments and Other Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ kws init\n",
      "____________________ kws prep data\n",
      "____________________ kws train dataloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed dim: 64\n",
      "Patch size: 8\n",
      "Sequence length: 16\n",
      "LitTransformer(\n",
      "  (encoder): Transformer(\n",
      "    (blocks): ModuleList(\n",
      "      (0): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (2): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (3): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (4): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (5): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (6): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (7): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (8): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (9): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (10): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (11): Block(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (embed): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  (loss): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "datamodule = KWSDataModule(batch_size=args.batch_size, num_workers=args.num_workers,\n",
    "                        path=args.path, n_fft=args.n_fft, n_mels=args.n_mels,\n",
    "                        win_length=args.win_length, hop_length=args.hop_length,\n",
    "                        class_dict=CLASS_TO_IDX)\n",
    "datamodule.prepare_data()\n",
    "\n",
    "data = iter(datamodule.train_dataloader()).next()\n",
    "patch_dim = data[0].shape[-1]\n",
    "seqlen = data[0].shape[-2]\n",
    "print(\"Embed dim:\", args.embed_dim)\n",
    "print(\"Patch size:\", 32 // args.patch_num)\n",
    "print(\"Sequence length:\", seqlen)\n",
    "\n",
    "\n",
    "model = LitTransformer(num_classes=10, lr=args.lr, epochs=args.max_epochs, \n",
    "                        depth=args.depth, embed_dim=args.embed_dim, head=args.num_heads,\n",
    "                        patch_dim=patch_dim, seqlen=seqlen,)\n",
    "print(model)\n",
    "trainer = Trainer(accelerator=args.accelerator, devices=args.devices,\n",
    "                    max_epochs=args.max_epochs, precision=16 if args.accelerator == 'gpu' else 32,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ kws prep data\n",
      "____________________ setup \n",
      "____________________ kws prep data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params\n",
      "---------------------------------------------\n",
      "0 | encoder | Transformer      | 597 K \n",
      "1 | embed   | Linear           | 4.2 K \n",
      "2 | fc      | Linear           | 10.2 K\n",
      "3 | loss    | CrossEntropyLoss | 0     \n",
      "---------------------------------------------\n",
      "611 K     Trainable params\n",
      "0         Non-trainable params\n",
      "611 K     Total params\n",
      "1.224     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ kws val dataloader                            \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:721\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=719'>720</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=720'>721</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=721'>722</a>\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:809\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=805'>806</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_ckpt_path(\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=806'>807</a>\u001b[0m     ckpt_path, model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=807'>808</a>\u001b[0m )\n\u001b[0;32m--> <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=808'>809</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=810'>811</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1234\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1231'>1232</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1233'>1234</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1235'>1236</a>\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1321\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1319'>1320</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1320'>1321</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1343\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1341'>1342</a>\u001b[0m \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1342'>1343</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1344'>1345</a>\u001b[0m \u001b[39m# enable train mode\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1404\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1402'>1403</a>\u001b[0m \u001b[39m# reload dataloaders\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1403'>1404</a>\u001b[0m val_loop\u001b[39m.\u001b[39;49m_reload_evaluation_dataloaders()\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1404'>1405</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_sanity_val_batches \u001b[39m=\u001b[39m [\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1405'>1406</a>\u001b[0m     \u001b[39mmin\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_sanity_val_steps, val_batches) \u001b[39mfor\u001b[39;00m val_batches \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_val_batches\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1406'>1407</a>\u001b[0m ]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:240\u001b[0m, in \u001b[0;36mEvaluationLoop._reload_evaluation_dataloaders\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=238'>239</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mval_dataloaders \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39m_should_reload_val_dl:\n\u001b[0;32m--> <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=239'>240</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mreset_val_dataloader()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1948\u001b[0m, in \u001b[0;36mTrainer.reset_val_dataloader\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1946'>1947</a>\u001b[0m \u001b[39mif\u001b[39;00m source\u001b[39m.\u001b[39mis_defined() \u001b[39mand\u001b[39;00m has_step \u001b[39mand\u001b[39;00m enable_validation:\n\u001b[0;32m-> <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1947'>1948</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_val_batches, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_dataloaders \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_connector\u001b[39m.\u001b[39;49m_reset_eval_dataloader(\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1948'>1949</a>\u001b[0m         RunningStage\u001b[39m.\u001b[39;49mVALIDATING, model\u001b[39m=\u001b[39;49mpl_module\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1949'>1950</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1951'>1952</a>\u001b[0m     \u001b[39m# store epoch of dataloader reset for reload_dataloaders_every_n_epochs\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:404\u001b[0m, in \u001b[0;36mDataConnector._reset_eval_dataloader\u001b[0;34m(self, mode, model)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py?line=401'>402</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, dataloader \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloaders):\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py?line=402'>403</a>\u001b[0m     orig_num_batches \u001b[39m=\u001b[39m num_batches \u001b[39m=\u001b[39m (\n\u001b[0;32m--> <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py?line=403'>404</a>\u001b[0m         \u001b[39mlen\u001b[39m(dataloader) \u001b[39mif\u001b[39;00m has_len_all_ranks(dataloader, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mstrategy, module) \u001b[39melse\u001b[39;00m \u001b[39mfloat\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39minf\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py?line=404'>405</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py?line=405'>406</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_worker_check(dataloader, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmode\u001b[39m.\u001b[39mdataloader_prefix\u001b[39m}\u001b[39;00m\u001b[39m_dataloader \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:124\u001b[0m, in \u001b[0;36mhas_len_all_ranks\u001b[0;34m(dataloader, training_type, model)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py?line=122'>123</a>\u001b[0m local_length \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(dataloader)\n\u001b[0;32m--> <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py?line=123'>124</a>\u001b[0m total_length \u001b[39m=\u001b[39m training_type\u001b[39m.\u001b[39mreduce(torch\u001b[39m.\u001b[39;49mtensor(local_length)\u001b[39m.\u001b[39;49mto(model\u001b[39m.\u001b[39;49mdevice), reduce_op\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msum\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py?line=125'>126</a>\u001b[0m \u001b[39mif\u001b[39;00m total_length \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/dl/Desktop/dl/object_detection_model_hw2/hw3/train copy.ipynb Cell 27'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/train%20copy.ipynb#ch0000026?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, datamodule\u001b[39m=\u001b[39;49mdatamodule)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:768\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=748'>749</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=749'>750</a>\u001b[0m \u001b[39mRuns the full optimization routine.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=750'>751</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=764'>765</a>\u001b[0m \u001b[39m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=765'>766</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=766'>767</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[0;32m--> <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=767'>768</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=768'>769</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=769'>770</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:736\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=733'>734</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mreconciliate_processes(traceback\u001b[39m.\u001b[39mformat_exc())\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=734'>735</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39m\"\u001b[39m\u001b[39mon_exception\u001b[39m\u001b[39m\"\u001b[39m, exception)\n\u001b[0;32m--> <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=735'>736</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_teardown()\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=736'>737</a>\u001b[0m \u001b[39m# teardown might access the stage so we reset it after\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=737'>738</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstage \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1298\u001b[0m, in \u001b[0;36mTrainer._teardown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1294'>1295</a>\u001b[0m \u001b[39m\"\"\"This is the Trainer's internal teardown, unrelated to the `teardown` hooks in LightningModule and\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1295'>1296</a>\u001b[0m \u001b[39mCallback; those are handled by :meth:`_call_teardown_hook`.\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1296'>1297</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mpost_dispatch(\u001b[39mself\u001b[39m)\n\u001b[0;32m-> <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1297'>1298</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrategy\u001b[39m.\u001b[39;49mteardown()\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1298'>1299</a>\u001b[0m loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_active_loop\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1299'>1300</a>\u001b[0m \u001b[39m# loop should never be `None` here but it can because we don't know the trainer stage with `ddp_spawn`\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/strategies/single_device.py:96\u001b[0m, in \u001b[0;36mSingleDeviceStrategy.teardown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/strategies/single_device.py?line=92'>93</a>\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mteardown()\n\u001b[1;32m     <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/strategies/single_device.py?line=93'>94</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_device\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/strategies/single_device.py?line=94'>95</a>\u001b[0m     \u001b[39m# GPU teardown\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/strategies/single_device.py?line=95'>96</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module\u001b[39m.\u001b[39;49mcpu()\n\u001b[1;32m     <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/strategies/single_device.py?line=96'>97</a>\u001b[0m     \u001b[39m# clean up memory\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/strategies/single_device.py?line=97'>98</a>\u001b[0m     torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/core/mixins/device_dtype_mixin.py:147\u001b[0m, in \u001b[0;36mDeviceDtypeModuleMixin.cpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/core/mixins/device_dtype_mixin.py?line=140'>141</a>\u001b[0m \u001b[39m\"\"\"Moves all model parameters and buffers to the CPU.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/core/mixins/device_dtype_mixin.py?line=141'>142</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/core/mixins/device_dtype_mixin.py?line=142'>143</a>\u001b[0m \u001b[39mReturns:\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/core/mixins/device_dtype_mixin.py?line=143'>144</a>\u001b[0m \u001b[39m    Module: self\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/core/mixins/device_dtype_mixin.py?line=144'>145</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/core/mixins/device_dtype_mixin.py?line=145'>146</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__update_properties(device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m--> <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/core/mixins/device_dtype_mixin.py?line=146'>147</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcpu()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:718\u001b[0m, in \u001b[0;36mModule.cpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=708'>709</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcpu\u001b[39m(\u001b[39mself\u001b[39m: T) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=709'>710</a>\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the CPU.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=710'>711</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=711'>712</a>\u001b[0m \u001b[39m    .. note::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=715'>716</a>\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=716'>717</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=717'>718</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(\u001b[39mlambda\u001b[39;49;00m t: t\u001b[39m.\u001b[39;49mcpu())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=575'>576</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=576'>577</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=577'>578</a>\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=579'>580</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=580'>581</a>\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=581'>582</a>\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=582'>583</a>\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=587'>588</a>\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=588'>589</a>\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=575'>576</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=576'>577</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=577'>578</a>\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=579'>580</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=580'>581</a>\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=581'>582</a>\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=582'>583</a>\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=587'>588</a>\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=588'>589</a>\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 578 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=575'>576</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=576'>577</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=577'>578</a>\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=579'>580</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=580'>581</a>\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=581'>582</a>\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=582'>583</a>\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=587'>588</a>\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=588'>589</a>\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:601\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=596'>597</a>\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=597'>598</a>\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=598'>599</a>\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=599'>600</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=600'>601</a>\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=601'>602</a>\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=602'>603</a>\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:718\u001b[0m, in \u001b[0;36mModule.cpu.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=708'>709</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcpu\u001b[39m(\u001b[39mself\u001b[39m: T) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=709'>710</a>\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the CPU.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=710'>711</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=711'>712</a>\u001b[0m \u001b[39m    .. note::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=715'>716</a>\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=716'>717</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=717'>718</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(\u001b[39mlambda\u001b[39;00m t: t\u001b[39m.\u001b[39;49mcpu())\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n",
    "#     waveform = waveform.numpy()\n",
    "\n",
    "#     num_channels, num_frames = waveform.shape\n",
    "#     time_axis = torch.arange(0, num_frames) / sample_rate\n",
    "\n",
    "#     figure, axes = plt.subplots(num_channels, 1)\n",
    "#     if num_channels == 1:\n",
    "#         axes = [axes]\n",
    "#     for c in range(num_channels):\n",
    "#         axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
    "#         axes[c].grid(True)\n",
    "#         if num_channels > 1:\n",
    "#             axes[c].set_ylabel(f'Channel {c+1}')\n",
    "#         if xlim:\n",
    "#             axes[c].set_xlim(xlim)\n",
    "#         if ylim:\n",
    "#             axes[c].set_ylim(ylim)\n",
    "#     figure.suptitle(title)\n",
    "#     plt.show(block=False)\n",
    "\n",
    "# plot_waveform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
