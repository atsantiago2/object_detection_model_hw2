{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer for CIFAR10\n",
    "\n",
    "A configurable transformer model will be used for CIFAR10 image classification. \n",
    "\n",
    "The vision transformer model is a modified version of ViT. The changes are:\n",
    "1) No position embedding.\n",
    "2) No dropout is used.\n",
    "3) All encoder features are used for class prediction.\n",
    "\n",
    "The code below is a simplified version of Timm modules.\n",
    "\n",
    "Let us import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from pytorch_lightning import LightningModule, Trainer, LightningDataModule\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchmetrics.functional import accuracy\n",
    "from einops import rearrange\n",
    "from torch import nn\n",
    "from torchvision.datasets.cifar import CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Module\n",
    "\n",
    "The `Attention` module is the core of the vision transformer model. It implements the attention mechanism:\n",
    "\n",
    "1) Multiply QKV by their weights\n",
    "2) Perform dot product on Q and K. \n",
    "3) Normalize the result in 2) by sqrt of `head_dim`  \n",
    "4) Softmax is applied to the result.\n",
    "5) Perform dot product on the result of 4) and V and the result is the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Module\n",
    "\n",
    "The MLP module is a made of two linear layers. A non-linear activation is applied to the output of the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "      \n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Block Module\n",
    "\n",
    "The `Block` module represents one encoder transformer block. It consists of two sub-modules:\n",
    "1) The Attention module\n",
    "2) The MLP module\n",
    "\n",
    "Layer norm is applied before and after the Attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, dim, num_heads, mlp_ratio=4., qkv_bias=False, \n",
    "            act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias) \n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer) \n",
    "   \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Transformer Module\n",
    "\n",
    "The feature encoder is made of several transformer blocks. The most important attributes are:\n",
    "1) `depth` : representing the number of encoder blocks\n",
    "2) `num_heads` : representing the number of attention heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_blocks, mlp_ratio=4., qkv_bias=False,  \n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([Block(dim, num_heads, mlp_ratio, qkv_bias, \n",
    "                                     act_layer, norm_layer) for _ in range(num_blocks)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The optional parameter initialization as adopted from `timm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights_vit_timm(module: nn.Module):\n",
    "    \"\"\" ViT weight initialization, original timm impl (for reproducibility) \"\"\"\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.trunc_normal_(module.weight, mean=0.0, std=0.02)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif hasattr(module, 'init_weights'):\n",
    "        module.init_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Lightning for CIFAR10 Image Classification\n",
    "\n",
    "We use the `Transformer` module to build the feature encoder. Before the `Transformer` can be used, we convert the input image into patches. The patches are then embedded into a linear space. The output is then passed to the Transformer.\n",
    "\n",
    "Another difference between this model is we use all output features for the final classification. In the ViT, only the first feature is used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LitTransformer(LightningModule):\n",
    "    def __init__(self, num_classes=10, lr=0.001, max_epochs=30, depth=12, embed_dim=64,\n",
    "                 head=4, patch_dim=192, seqlen=16, **kwargs):\n",
    "        super().__init__()\n",
    "        print('_'*20, 'init')\n",
    "        self.save_hyperparameters()\n",
    "        self.encoder = Transformer(dim=embed_dim, num_heads=head, num_blocks=depth, mlp_ratio=4.,\n",
    "                                   qkv_bias=False, act_layer=nn.GELU, norm_layer=nn.LayerNorm)\n",
    "        self.embed = torch.nn.Linear(patch_dim, embed_dim)\n",
    "\n",
    "        self.fc = nn.Linear(seqlen * embed_dim, num_classes)\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        print('_'*20, 'reset param')\n",
    "        init_weights_vit_timm(self)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        print('_'*20, 'fwd')\n",
    "        # Linear projection\n",
    "        x = self.embed(x)\n",
    "            \n",
    "        # Encoder\n",
    "        x = self.encoder(x)\n",
    "        x = x.flatten(start_dim=1)\n",
    "\n",
    "        # Classification head\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        print('_'*20, 'config optimizers')\n",
    "        optimizer = Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        # this decays the learning rate to 0 after max_epochs using cosine annealing\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=self.hparams.max_epochs)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        print('_'*20, 'steps')\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # print('_'*20, 'test step')\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        acc = accuracy(y_hat, y)\n",
    "        return {\"y_hat\": y_hat, \"test_loss\": loss, \"test_acc\": acc}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        # print('_'*20, 'test ep end')\n",
    "        avg_loss = torch.stack([x[\"test_loss\"] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x[\"test_acc\"] for x in outputs]).mean()\n",
    "        self.log(\"test_loss\", avg_loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_acc\", avg_acc*100., on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # print('_'*20, 'valid step')\n",
    "        return self.test_step(batch, batch_idx)\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # print('_'*20, 'vald epoc end')\n",
    "        return self.test_epoch_end(outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# a lightning data module for cifar 10 dataset\n",
    "class LitCifar10(LightningDataModule):\n",
    "    def __init__(self, batch_size=32, num_workers=32, patch_num=4, **kwargs):\n",
    "        print('_'*20, 'datamodule init')\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.patch_num = patch_num\n",
    "\n",
    "    def prepare_data(self):\n",
    "        print('_'*20, 'datamodule prepare data')\n",
    "        self.train_set = CIFAR10(root='./data', train=True,\n",
    "                                 download=True, transform=torchvision.transforms.ToTensor())\n",
    "        self.test_set = CIFAR10(root='./data', train=False,\n",
    "                                download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        print('_'*20, 'datamodule collate fn')\n",
    "        x, y = zip(*batch)\n",
    "        x = torch.stack(x, dim=0)\n",
    "        y = torch.LongTensor(y)\n",
    "\n",
    "        print('\\nx before rearrange:{}'.format(x.shape))\n",
    "\n",
    "        x = rearrange(x, 'b c (p1 h) (p2 w) -> b (p1 p2) (c h w)', p1=self.patch_num, p2=self.patch_num)\n",
    "\n",
    "\n",
    "        print('xhsape', x.shape)\n",
    "        print('yhsape', y.shape)\n",
    "        return x, y\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        print('_'*20, 'datamodule train dataloader')\n",
    "        return torch.utils.data.DataLoader(self.train_set, batch_size=self.batch_size, \n",
    "                                        shuffle=True, collate_fn=self.collate_fn,\n",
    "                                        num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        print('_'*20, 'datamodule test dataload')\n",
    "        return torch.utils.data.DataLoader(self.test_set, batch_size=self.batch_size, \n",
    "                                        shuffle=False, collate_fn=self.collate_fn,\n",
    "                                        num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        print('_'*20, 'datamodule valdataloader')\n",
    "        return self.test_dataloader()\n",
    "\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = ArgumentParser(description='PyTorch Transformer')\n",
    "    parser.add_argument('--depth', type=int, default=12, help='depth')\n",
    "    parser.add_argument('--embed_dim', type=int, default=128, help='embedding dimension')\n",
    "    parser.add_argument('--num_heads', type=int, default=4, help='num_heads')\n",
    "\n",
    "    parser.add_argument('--patch_num', type=int, default=8, help='patch_num')\n",
    "    parser.add_argument('--kernel_size', type=int, default=3, help='kernel size')\n",
    "    parser.add_argument('--batch_size', type=int, default=44, metavar='N',\n",
    "                        help='input batch size for training (default: )')\n",
    "    parser.add_argument('--max-epochs', type=int, default=1, metavar='N',\n",
    "                        help='number of epochs to train (default: 0)')\n",
    "    parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
    "                        help='learning rate (default: 0.0)')\n",
    "\n",
    "    parser.add_argument('--accelerator', default='gpu', type=str, metavar='N')\n",
    "    parser.add_argument('--devices', default=1, type=int, metavar='N')\n",
    "    parser.add_argument('--dataset', default='cifar10', type=str, metavar='N')\n",
    "    parser.add_argument('--num_workers', default=16, type=int, metavar='N')\n",
    "    args = parser.parse_args(\"\")\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on different settings\n",
    "\n",
    "The following table shows different performances on different settings. Generally, Transformer is better than MLP in terms of accuracy and parameter count. However, the performance is worse compared to CNN models. \n",
    "\n",
    "However, the most important thing to note is that Transformers are more general purpose models than CNNs. They can process different types of data. They can process multiple types of data at the same time. This is why they are considered to be the backbone of many high-performing models like BERT, GPT3, PalM and Gato.\n",
    "\n",
    "| **Depth** | **Head** | **Embed dim** | **Patch size** | **Seq len** | **Params** | **Accuracy** | \n",
    "| -: | -: | -: | -: | -: | -: | -: |\n",
    "| 12 | 4 | 32 | 4x4 | 64 | 173k | 68.2% |\n",
    "| 12 | 4 | 64 | 4x4 | 64 | 641k | 71.1% |\n",
    "| 12 | 4 | 128 | 4x4 | 64 | 2.5M | 71.5% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ datamodule init\n",
      "____________________ datamodule prepare data\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "____________________ datamodule train dataloader\n",
      "____________________ datamodule collate fn\n",
      "\n",
      "x before rearrange:torch.Size([64, 3, 32, 32])\n",
      "____________________ torch.Size([64, 64, 48])datamodule collate fn\n",
      "________________________________________ ____________________ datamodule collate fn________________________________________\n",
      "________________________________________________________________________________\n",
      "x before rearrange:torch.Size([64, 3, 32, 32])____________________\n",
      "    datamodule collate fn datamodule collate fn  xhsapedatamodule collate fndatamodule collate fn\n",
      "x before rearrange:torch.Size([64, 3, 32, 32])\n",
      "____________________datamodule collate fndatamodule collate fn \n",
      "\n",
      "datamodule collate fn\n",
      "datamodule collate fntorch.Size([64, 64, 48]) \n",
      "torch.Size([64, 64, 48])\n",
      "\n",
      "\n",
      "datamodule collate fn\n",
      "\n",
      "x before rearrange:torch.Size([64, 3, 32, 32])\n",
      "\n",
      "x before rearrange:torch.Size([64, 3, 32, 32])yhsape\n",
      "x before rearrange:torch.Size([64, 3, 32, 32])\n",
      "x before rearrange:torch.Size([64, 3, 32, 32])\n",
      "\n",
      "x before rearrange:torch.Size([64, 3, 32, 32])\n",
      "x before rearrange:torch.Size([64, 3, 32, 32])torch.Size([64, 64, 48])\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "____________________xhsape ____________________xhsape\n",
      " torch.Size([64, 64, 48])datamodule collate fntorch.Size([64, 64, 48])torch.Size([64, 64, 48])torch.Size([64])\n",
      "torch.Size([64, 64, 48]) torch.Size([64, 64, 48]) \n",
      "\n",
      "\n",
      "x before rearrange:torch.Size([64, 3, 32, 32])____________________\n",
      "x before rearrange:torch.Size([64, 3, 32, 32])\n",
      "datamodule collate fntorch.Size([64, 64, 48])\n",
      "torch.Size([64, 64, 48]) \n",
      "xhsape\n",
      "\n",
      "xhsape\n",
      "\n",
      "datamodule collate fn  \n",
      "torch.Size([64, 64, 48])yhsape\n",
      " \n",
      "x before rearrange:torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 64, 48])\n",
      "xhsape\n",
      "torch.Size([64, 64, 48]) \n",
      "x before rearrange:torch.Size([64, 3, 32, 32])\n",
      "\n",
      " \n",
      "torch.Size([64])\n",
      "datamodule collate fnxhsapetorch.Size([64, 64, 48])torch.Size([64, 64, 48])\n",
      "\n",
      "\n",
      "x before rearrange:torch.Size([64, 3, 32, 32])torch.Size([64, 64, 48])xhsape xhsape\n",
      "\n",
      "torch.Size([64, 64, 48]) \n",
      "xhsapeyhsape\n",
      "x before rearrange:torch.Size([64, 3, 32, 32])torch.Size([64, 64, 48])\n",
      "torch.Size([64, 64, 48]) \n",
      " yhsapeyhsapeyhsape\n",
      "torch.Size([64, 64, 48])torch.Size([64, 64, 48]) torch.Size([64, 64, 48])  xhsape \n",
      "\n",
      "torch.Size([64])torch.Size([64])torch.Size([64, 64, 48])torch.Size([64]) torch.Size([64])xhsapeyhsape\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([64, 64, 48])\n",
      "  xhsape\n",
      "\n",
      "\n",
      "torch.Size([64, 64, 48]) yhsape\n",
      "yhsape\n",
      "____________________torch.Size([64, 64, 48])  torch.Size([64])\n",
      "yhsapetorch.Size([64]) xhsapetorch.Size([64]) datamodule collate fn yhsape\n",
      "torch.Size([64, 64, 48])\n",
      "\n",
      " torch.Size([64])xhsape\n",
      "\n",
      "x before rearrange:torch.Size([64, 3, 32, 32])torch.Size([64]) torch.Size([64, 64, 48])\n",
      "\n",
      "\n",
      "torch.Size([64, 64, 48])____________________\n",
      "____________________yhsape____________________________________________________________\n",
      "\n",
      " \n",
      "   yhsapedatamodule collate fn____________________\n",
      "x before rearrange:torch.Size([64, 3, 32, 32])torch.Size([64])yhsapedatamodule collate fn\n",
      " ____________________ datamodule collate fntorch.Size([64])\n",
      "\n",
      " \n",
      "datamodule collate fn \n",
      "x before rearrange:torch.Size([64, 3, 32, 32])\n",
      "\n",
      "torch.Size([64])\n",
      "\n",
      "\n",
      "x before rearrange:torch.Size([64, 3, 32, 32])yhsapedatamodule collate fn\n",
      "x before rearrange:torch.Size([64, 3, 32, 32])\n",
      "x before rearrange:torch.Size([64, 3, 32, 32])____________________torch.Size([64, 64, 48])\n",
      "\n",
      "torch.Size([64, 64, 48])  torch.Size([64, 64, 48]) \n",
      "\n",
      "x before rearrange:torch.Size([64, 3, 32, 32])datamodule collate fndatamodule collate fn\n",
      "\n",
      "datamodule collate fnxhsape\n",
      "torch.Size([64, 64, 48])\n",
      "xhsape\n",
      " \n",
      " \n",
      "x before rearrange:torch.Size([64, 3, 32, 32])\n",
      "x before rearrange:torch.Size([64, 3, 32, 32])xhsapetorch.Size([64, 64, 48])torch.Size([64, 64, 48])\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "torch.Size([64, 64, 48])xhsape\n",
      "____________________\n",
      "\n",
      "  torch.Size([64, 64, 48])\n",
      "\n",
      "x before rearrange:torch.Size([64, 3, 32, 32])torch.Size([64, 64, 48])xhsape torch.Size([64, 64, 48])\n",
      "\n",
      " yhsape\n",
      "xhsapetorch.Size([64, 64, 48])yhsapedatamodule collate fn\n",
      " xhsapetorch.Size([64])____________________torch.Size([64])____________________torch.Size([64, 64, 48])\n",
      "torch.Size([64, 64, 48])\n",
      "  yhsape____________________torch.Size([64, 64, 48])____________________ \n",
      "yhsapetorch.Size([64, 64, 48])torch.Size([64])  \n",
      "\n",
      "\n",
      "x before rearrange:torch.Size([64, 3, 32, 32])torch.Size([64, 64, 48]) datamodule collate fn\n",
      "torch.Size([64])xhsape\n",
      "yhsape\n",
      "datamodule collate fndatamodule collate fn\n",
      "  \n",
      "xhsape\n",
      "xhsapetorch.Size([64, 64, 48]) torch.Size([64, 64, 48])torch.Size([64])\n",
      "  \n",
      "x before rearrange:torch.Size([64, 3, 32, 32])\n",
      "\n",
      "\n",
      "torch.Size([64, 64, 48])\n",
      "torch.Size([64, 64, 48]) \n",
      "x before rearrange:torch.Size([64, 3, 32, 32]) \n",
      "torch.Size([64])\n",
      "yhsapexhsape\n",
      "datamodule collate fn\n",
      "\n",
      "\n",
      "torch.Size([64, 64, 48]) yhsape\n",
      "x before rearrange:torch.Size([64, 3, 32, 32])yhsape\n",
      "yhsapetorch.Size([64, 64, 48])  \n",
      "x before rearrange:torch.Size([64, 3, 32, 32])torch.Size([64, 64, 48])\n",
      " \n",
      "\n",
      "torch.Size([64])torch.Size([64, 64, 48])\n",
      "yhsape torch.Size([64])\n",
      " \n",
      "\n",
      "torch.Size([64])xhsapexhsapetorch.Size([64])\n",
      " torch.Size([64, 64, 48])\n",
      "\n",
      "torch.Size([64, 64, 48])\n",
      "torch.Size([64])torch.Size([64, 64, 48])\n",
      "yhsape  xhsape\n",
      "\n",
      "torch.Size([64])\n",
      "\n",
      "torch.Size([64, 64, 48])yhsape torch.Size([64, 64, 48])xhsape torch.Size([64, 64, 48])\n",
      "yhsape torch.Size([64])\n",
      " \n",
      "torch.Size([64])yhsape\n",
      " torch.Size([64])\n",
      "\n",
      "yhsape torch.Size([64])\n",
      "Embed dim: 128\n",
      "Patch size: 4\n",
      "Sequence length: 64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args = get_args()\n",
    "\n",
    "datamodule = LitCifar10(batch_size=args.batch_size,\n",
    "                        patch_num=args.patch_num, \n",
    "                        num_workers=args.num_workers * args.devices)\n",
    "datamodule.prepare_data()\n",
    "\n",
    "data = iter(datamodule.train_dataloader()).next()\n",
    "patch_dim = data[0].shape[-1]\n",
    "seqlen = data[0].shape[-2]\n",
    "print(\"Embed dim:\", args.embed_dim)\n",
    "print(\"Patch size:\", 32 // args.patch_num)\n",
    "print(\"Sequence length:\", seqlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64, 48])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 8, 3, 8, 0, 0, 1, 6, 4, 2, 9, 0, 2, 2, 3, 3, 7, 1, 7, 6, 9, 4, 7, 6,\n",
       "        6, 5, 5, 0, 3, 8, 9, 1, 4, 7, 1, 0, 6, 5, 9, 9, 2, 9, 8, 5, 7, 1, 3, 6,\n",
       "        1, 7, 0, 6, 1, 1, 8, 1, 2, 3, 3, 9, 5, 9, 1, 0])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f890c795de0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fig = plt.figure\n",
    "plt.imshow(data[0][4][2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 8, 3, 8, 0, 0, 1, 6, 4, 2, 9, 0, 2, 2, 3, 3, 7, 1, 7, 6, 9, 4, 7, 6,\n",
       "        6, 5, 5, 0, 3, 8, 9, 1, 4, 7, 1, 0, 6, 5, 9, 9, 2, 9, 8, 5, 7, 1, 3, 6,\n",
       "        1, 7, 0, 6, 1, 1, 8, 1, 2, 3, 3, 9, 5, 9, 1, 0])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ datamodule init\n",
      "____________________ datamodule prepare data\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "____________________ datamodule train dataloader\n",
      "________________________________________ ________________________________________________________________________________________________________________________datamodule collate fn ____________________ ____________________ ____________________ ____________________datamodule collate fn________________________________________    ____________________datamodule collate fn____________________datamodule collate fn   \n",
      "\n",
      "datamodule collate fn  datamodule collate fndatamodule collate fndatamodule collate fn \n",
      "datamodule collate fn \n",
      "datamodule collate fndatamodule collate fndatamodule collate fndatamodule collate fndatamodule collate fn\n",
      "torch.Size([64, 3, 32, 32])\n",
      "\n",
      "torch.Size([64, 3, 32, 32])\n",
      "datamodule collate fndatamodule collate fn\n",
      "\n",
      "torch.Size([64, 3, 32, 32])\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 3, 32, 32])torch.Size([64, 3, 32, 32])torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 3, 32, 32])xhsape\n",
      "xhsapetorch.Size([64, 3, 32, 32])\n",
      "\n",
      "torch.Size([64, 3, 32, 32])xhsapetorch.Size([64, 3, 32, 32])\n",
      "\n",
      "torch.Size([64, 3, 32, 32])torch.Size([64, 3, 32, 32])\n",
      " xhsape\n",
      " xhsape\n",
      "\n",
      "xhsapexhsape \n",
      "xhsapetorch.Size([64, 3, 32, 32])xhsape torch.Size([64, 3, 32, 32])xhsape\n",
      "xhsape  xhsapetorch.Size([64, 3, 32, 32]) xhsape \n",
      "\n",
      " torch.Size([64, 3, 32, 32])  \n",
      "xhsapetorch.Size([64, 3, 32, 32])torch.Size([64, 3, 32, 32]) \n",
      " xhsapetorch.Size([64, 3, 32, 32])torch.Size([64, 3, 32, 32])yhsape\n",
      "torch.Size([64, 3, 32, 32])torch.Size([64, 3, 32, 32])yhsape torch.Size([64, 3, 32, 32])\n",
      "\n",
      "torch.Size([64, 3, 32, 32])yhsapetorch.Size([64, 3, 32, 32])\n",
      " \n",
      " yhsape\n",
      "\n",
      "\n",
      "torch.Size([64, 3, 32, 32]) yhsapeyhsape \n",
      "\n",
      "yhsapetorch.Size([64, 3, 32, 32])yhsapeyhsapetorch.Size([64]) yhsapeyhsapetorch.Size([64])\n",
      "xhsape  torch.Size([64])yhsapeyhsape \n",
      " \n",
      "torch.Size([64])   \n",
      " yhsapetorch.Size([64]) \n",
      "torch.Size([64]) yhsape\n",
      "torch.Size([64])torch.Size([64])torch.Size([64])torch.Size([64])torch.Size([64])torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64])torch.Size([64])\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([64])yhsape \n",
      "\n",
      " torch.Size([64])torch.Size([64])\n",
      "________________________________________ datamodule collate fn\n",
      "________________________________________________________________________________\n",
      "____________________  ____________________ ____________________torch.Size([64, 3, 32, 32])____________________ datamodule collate fn    \n",
      "datamodule collate fndatamodule collate fndatamodule collate fn____________________\n",
      "datamodule collate fn\n",
      "datamodule collate fn\n",
      "\n",
      "____________________ datamodule collate fn \n",
      "\n",
      "____________________torch.Size([64, 3, 32, 32])torch.Size([64, 3, 32, 32]) xhsape\n",
      "torch.Size([64, 3, 32, 32])torch.Size([64, 3, 32, 32])\n",
      "\n",
      " \n",
      "datamodule collate fndatamodule collate fn \n",
      "torch.Size([64, 3, 32, 32])\n",
      "datamodule collate fntorch.Size([64, 3, 32, 32])xhsapexhsape\n",
      "torch.Size([64, 3, 32, 32])\n",
      "\n",
      "xhsape____________________xhsapedatamodule collate fn\n",
      "\n",
      "  \n",
      "torch.Size([64, 3, 32, 32])datamodule collate fn  xhsapetorch.Size([64, 3, 32, 32])\n",
      "datamodule collate fnxhsapetorch.Size([64, 3, 32, 32])\n",
      " torch.Size([64, 3, 32, 32])torch.Size([64, 3, 32, 32])\n",
      "\n",
      "torch.Size([64, 3, 32, 32])\n",
      " \n",
      "torch.Size([64, 3, 32, 32])xhsape torch.Size([64, 3, 32, 32])torch.Size([64, 3, 32, 32])\n",
      "yhsapetorch.Size([64, 3, 32, 32])\n",
      "yhsapexhsapetorch.Size([64, 3, 32, 32])torch.Size([64, 3, 32, 32])\n",
      " torch.Size([64, 3, 32, 32])xhsape\n",
      "yhsape xhsape\n",
      "____________________\n",
      "  \n",
      "yhsape____________________ torch.Size([64, 3, 32, 32])\n",
      "xhsape torch.Size([64])xhsape\n",
      " yhsape torch.Size([64])torch.Size([64, 3, 32, 32])xhsape  torch.Size([64, 3, 32, 32])yhsape\n",
      " torch.Size([64])\n",
      " yhsapetorch.Size([64, 3, 32, 32]) datamodule collate fn\n",
      "\n",
      " torch.Size([64])datamodule collate fn\n",
      "yhsape torch.Size([64, 3, 32, 32])torch.Size([64, 3, 32, 32])\n",
      " \n",
      "torch.Size([64])\n",
      "yhsapetorch.Size([64, 3, 32, 32])\n",
      " \n",
      "yhsape\n",
      "torch.Size([64])\n",
      "yhsapetorch.Size([64]) \n",
      "\n",
      "torch.Size([64, 3, 32, 32])yhsapetorch.Size([64, 3, 32, 32])yhsape\n",
      "torch.Size([64])torch.Size([64])yhsape  \n",
      " \n",
      "\n",
      " \n",
      "torch.Size([64])torch.Size([64])torch.Size([64])\n",
      "\n",
      "\n",
      "xhsape \n",
      "torch.Size([64])torch.Size([64, 3, 32, 32]) \n",
      "\n",
      "torch.Size([64])yhsape \n",
      "torch.Size([64])\n",
      "xhsape torch.Size([64, 3, 32, 32])\n",
      "yhsape torch.Size([64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed dim: 128\n",
      "Patch size: 4\n",
      "Sequence length: 32\n",
      "____________________ init\n",
      "____________________ reset param\n",
      "____________________ datamodule prepare data\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params\n",
      "---------------------------------------------\n",
      "0 | encoder | Transformer      | 2.4 M \n",
      "1 | embed   | Linear           | 4.2 K \n",
      "2 | fc      | Linear           | 41.0 K\n",
      "3 | loss    | CrossEntropyLoss | 0     \n",
      "---------------------------------------------\n",
      "2.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 M     Total params\n",
      "4.840     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ config optimizers\n",
      "Sanity Checking: 0it [00:00, ?it/s]____________________ datamodule valdataloader\n",
      "____________________ datamodule test dataload\n",
      "____________________ datamodule collate fn____________________\n",
      "________________________________________   torch.Size([64, 3, 32, 32])datamodule collate fndatamodule collate fndatamodule collate fn\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([64, 3, 32, 32])torch.Size([64, 3, 32, 32])torch.Size([64, 3, 32, 32])\n",
      "\n",
      "xhsape\n",
      "xhsapexhsape torch.Size([64, 3, 32, 32])torch.Size([64, 3, 32, 32])____________________xhsape________________________________________________________________________________\n",
      "____________________  ____________________  datamodule collate fn yhsape \n",
      "torch.Size([64, 3, 32, 32])datamodule collate fn\n",
      "  \n",
      "torch.Size([64, 3, 32, 32]) torch.Size([64])yhsape ____________________torch.Size([64, 3, 32, 32])datamodule collate fn____________________ \n",
      "datamodule collate fn\n",
      "\n",
      " ____________________ \n",
      "torch.Size([64])____________________torch.Size([64, 3, 32, 32])yhsapedatamodule collate fn ____________________ \n",
      "\n",
      "torch.Size([64, 3, 32, 32]) datamodule collate fn\n",
      "datamodule collate fntorch.Size([64])\n",
      "\n",
      "xhsape\n",
      "xhsape\n",
      " xhsape torch.Size([64, 3, 32, 32])datamodule collate fntorch.Size([64, 3, 32, 32])datamodule collate fn   datamodule collate fn\n",
      "torch.Size([64, 3, 32, 32])\n",
      "\n",
      "datamodule collate fn\n",
      "torch.Size([64, 3, 32, 32])\n",
      "\n",
      "xhsapetorch.Size([64, 3, 32, 32])datamodule collate fntorch.Size([64, 3, 32, 32])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([64, 3, 32, 32])\n",
      "xhsapetorch.Size([64, 3, 32, 32])xhsapetorch.Size([64, 3, 32, 32])\n",
      "yhsapexhsapetorch.Size([64, 3, 32, 32])yhsape torch.Size([64, 3, 32, 32])\n",
      " \n",
      "xhsape   ____________________\n",
      " torch.Size([64, 3, 32, 32])xhsape\n",
      "torch.Size([64, 3, 32, 32])xhsapetorch.Size([64, 3, 32, 32]) torch.Size([64])torch.Size([64, 3, 32, 32]) yhsape torch.Size([64])\n",
      "xhsape\n",
      " \n",
      "Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]datamodule collate fn\n",
      " \n",
      "\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "yhsapetorch.Size([64, 3, 32, 32])yhsape\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64]) yhsape____________________\n",
      " torch.Size([64, 3, 32, 32])xhsapeyhsape\n",
      "\n",
      "  torch.Size([64])yhsape  torch.Size([64])\n",
      "yhsapetorch.Size([64, 3, 32, 32]) \n",
      "datamodule collate fn \n",
      "yhsapetorch.Size([64, 3, 32, 32])\n",
      " torch.Size([64])torch.Size([64]) torch.Size([64])\n",
      "xhsapetorch.Size([64])\n",
      "\n",
      "torch.Size([64])\n",
      " yhsape\n",
      "torch.Size([64, 3, 32, 32])____________________ torch.Size([64])\n",
      "\n",
      " \n",
      "datamodule collate fntorch.Size([64, 3, 32, 32])yhsape\n",
      " xhsapetorch.Size([64])____________________\n",
      "\n",
      "torch.Size([64, 3, 32, 32])yhsape\n",
      " xhsapetorch.Size([64])____________________ \n",
      "torch.Size([64])torch.Size([64, 3, 32, 32]) \n",
      " \n",
      "____________________\n",
      "\n",
      "________________________________________torch.Size([64, 3, 32, 32])datamodule collate fn\n",
      " \n",
      "yhsapeyhsapedatamodule collate fn ________________________________________ torch.Size([64, 3, 32, 32])torch.Size([64])\n",
      " \n",
      "torch.Size([64, 3, 32, 32]) \n",
      "datamodule collate fn \n",
      "datamodule collate fn datamodule collate fn xhsapetorch.Size([64])____________________datamodule collate fn________________________________________\n",
      "\n",
      "xhsapedatamodule collate fn\n",
      "  torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 3, 32, 32])datamodule collate fn\n",
      "torch.Size([64, 3, 32, 32]) datamodule collate fn\n",
      "torch.Size([64, 3, 32, 32]) ____________________\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 3, 32, 32]) \n",
      " xhsape\n",
      "\n",
      "xhsapedatamodule collate fn____________________datamodule collate fntorch.Size([64, 3, 32, 32])xhsape yhsape____________________ \n",
      "\n",
      "\n",
      " torch.Size([64, 3, 32, 32])xhsape\n",
      "torch.Size([64, 3, 32, 32])torch.Size([64, 3, 32, 32])torch.Size([64])  \n",
      "\n",
      "\n",
      "\n",
      "datamodule collate fn \n",
      "____________________yhsapexhsape\n",
      "yhsapetorch.Size([64, 3, 32, 32]) \n",
      "    \n",
      "xhsapexhsapetorch.Size([64])torch.Size([64])torch.Size([64, 3, 32, 32])xhsape  \n",
      "\n",
      " torch.Size([64, 3, 32, 32])torch.Size([64, 3, 32, 32])datamodule collate fnyhsapetorch.Size([64, 3, 32, 32])torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 3, 32, 32])torch.Size([64, 3, 32, 32])\n",
      " \n",
      "yhsape\n",
      "\n",
      " xhsape\n",
      "xhsapetorch.Size([64]) yhsape torch.Size([64, 3, 32, 32])\n",
      " torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64])\n",
      "\n",
      "torch.Size([64, 3, 32, 32])yhsapeyhsape\n",
      "torch.Size([64])datamodule collate fn xhsape\n",
      "torch.Size([64, 3, 32, 32])torch.Size([64, 3, 32, 32])\n",
      "\n",
      "yhsape\n",
      "yhsape  torch.Size([64])\n",
      "\n",
      "torch.Size([64, 3, 32, 32]) yhsape\n",
      "xhsape\n",
      "  torch.Size([64, 3, 32, 32])torch.Size([64])torch.Size([64])\n",
      "\n",
      " torch.Size([64])\n",
      "yhsapetorch.Size([64])yhsape  torch.Size([64, 3, 32, 32])\n",
      "yhsape torch.Size([64])\n",
      "\n",
      "\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "____________________ fwd\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo (2).ipynb Cell 23'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo%20%282%29.ipynb#ch0000016?line=16'>17</a>\u001b[0m model \u001b[39m=\u001b[39m LitTransformer(num_classes\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, lr\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mlr, epochs\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mmax_epochs, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo%20%282%29.ipynb#ch0000016?line=17'>18</a>\u001b[0m                        depth\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mdepth, embed_dim\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39membed_dim, head\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo%20%282%29.ipynb#ch0000016?line=18'>19</a>\u001b[0m                        patch_dim\u001b[39m=\u001b[39mpatch_dim, seqlen\u001b[39m=\u001b[39mseqlen,)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo%20%282%29.ipynb#ch0000016?line=20'>21</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(accelerator\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39maccelerator, devices\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mdevices,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo%20%282%29.ipynb#ch0000016?line=21'>22</a>\u001b[0m                   max_epochs\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mmax_epochs, precision\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39maccelerator \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m32\u001b[39m,)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo%20%282%29.ipynb#ch0000016?line=22'>23</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, datamodule\u001b[39m=\u001b[39;49mdatamodule)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:768\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=748'>749</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=749'>750</a>\u001b[0m \u001b[39mRuns the full optimization routine.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=750'>751</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=764'>765</a>\u001b[0m \u001b[39m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=765'>766</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=766'>767</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[0;32m--> <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=767'>768</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=768'>769</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=769'>770</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:721\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=718'>719</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=719'>720</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=720'>721</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=721'>722</a>\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=722'>723</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:809\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=804'>805</a>\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=805'>806</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_ckpt_path(\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=806'>807</a>\u001b[0m     ckpt_path, model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=807'>808</a>\u001b[0m )\n\u001b[0;32m--> <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=808'>809</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=810'>811</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=811'>812</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1234\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1229'>1230</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1231'>1232</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1233'>1234</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1235'>1236</a>\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1236'>1237</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1321\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1318'>1319</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1319'>1320</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1320'>1321</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1343\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1339'>1340</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pre_training_routine()\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1341'>1342</a>\u001b[0m \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1342'>1343</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1344'>1345</a>\u001b[0m \u001b[39m# enable train mode\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1345'>1346</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1411\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1408'>1409</a>\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1409'>1410</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m-> <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1410'>1411</a>\u001b[0m     val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1412'>1413</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1414'>1415</a>\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/loops/base.py?line=201'>202</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/loops/base.py?line=202'>203</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/loops/base.py?line=203'>204</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/loops/base.py?line=204'>205</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/loops/base.py?line=205'>206</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:154\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=151'>152</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_dataloaders \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=152'>153</a>\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdataloader_idx\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataloader_idx\n\u001b[0;32m--> <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=153'>154</a>\u001b[0m dl_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher, dl_max_batches, kwargs)\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=155'>156</a>\u001b[0m \u001b[39m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=156'>157</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs\u001b[39m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/loops/base.py?line=201'>202</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/loops/base.py?line=202'>203</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/loops/base.py?line=203'>204</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/loops/base.py?line=204'>205</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/loops/base.py?line=205'>206</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:127\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=123'>124</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=125'>126</a>\u001b[0m \u001b[39m# lightning module methods\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=126'>127</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=127'>128</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_step_end(output)\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=129'>130</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:222\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=219'>220</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_call_strategy_hook(\u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39mkwargs\u001b[39m.\u001b[39mvalues())\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=220'>221</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=221'>222</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_strategy_hook(\u001b[39m\"\u001b[39;49m\u001b[39mvalidation_step\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=223'>224</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1763\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1759'>1760</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1761'>1762</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1762'>1763</a>\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1764'>1765</a>\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1765'>1766</a>\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:344\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py?line=338'>339</a>\u001b[0m \u001b[39m\"\"\"The actual validation step.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py?line=339'>340</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py?line=340'>341</a>\u001b[0m \u001b[39mSee :meth:`~pytorch_lightning.core.lightning.LightningModule.validation_step` for more details\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py?line=341'>342</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py?line=342'>343</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mval_step_context():\n\u001b[0;32m--> <a href='file:///home/dl/.local/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py?line=343'>344</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mvalidation_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo (2).ipynb Cell 14'\u001b[0m in \u001b[0;36mLitTransformer.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo%20%282%29.ipynb#ch0000013?line=64'>65</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidation_step\u001b[39m(\u001b[39mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo%20%282%29.ipynb#ch0000013?line=65'>66</a>\u001b[0m     \u001b[39m# print('_'*20, 'valid step')\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo%20%282%29.ipynb#ch0000013?line=66'>67</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtest_step(batch, batch_idx)\n",
      "\u001b[1;32m/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo (2).ipynb Cell 14'\u001b[0m in \u001b[0;36mLitTransformer.test_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo%20%282%29.ipynb#ch0000013?line=49'>50</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtest_step\u001b[39m(\u001b[39mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo%20%282%29.ipynb#ch0000013?line=50'>51</a>\u001b[0m     \u001b[39m# print('_'*20, 'test step')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo%20%282%29.ipynb#ch0000013?line=51'>52</a>\u001b[0m     x, y \u001b[39m=\u001b[39m batch\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo%20%282%29.ipynb#ch0000013?line=52'>53</a>\u001b[0m     y_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo%20%282%29.ipynb#ch0000013?line=53'>54</a>\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(y_hat, y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo%20%282%29.ipynb#ch0000013?line=54'>55</a>\u001b[0m     acc \u001b[39m=\u001b[39m accuracy(y_hat, y)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo (2).ipynb Cell 14'\u001b[0m in \u001b[0;36mLitTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo%20%282%29.ipynb#ch0000013?line=24'>25</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo%20%282%29.ipynb#ch0000013?line=26'>27</a>\u001b[0m \u001b[39m# Encoder\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo%20%282%29.ipynb#ch0000013?line=27'>28</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo%20%282%29.ipynb#ch0000013?line=28'>29</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mflatten(start_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo%20%282%29.ipynb#ch0000013?line=30'>31</a>\u001b[0m \u001b[39m# Classification head\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo (2).ipynb Cell 10'\u001b[0m in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo%20%282%29.ipynb#ch0000009?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo%20%282%29.ipynb#ch0000009?line=8'>9</a>\u001b[0m     \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo%20%282%29.ipynb#ch0000009?line=9'>10</a>\u001b[0m         x \u001b[39m=\u001b[39m block(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo%20%282%29.ipynb#ch0000009?line=10'>11</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo (2).ipynb Cell 8'\u001b[0m in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo%20%282%29.ipynb#ch0000007?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo%20%282%29.ipynb#ch0000007?line=13'>14</a>\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm1(x))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo%20%282%29.ipynb#ch0000007?line=14'>15</a>\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo%20%282%29.ipynb#ch0000007?line=15'>16</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/dl/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo (2).ipynb Cell 4'\u001b[0m in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo%20%282%29.ipynb#ch0000003?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo%20%282%29.ipynb#ch0000003?line=12'>13</a>\u001b[0m     B, N, C \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo%20%282%29.ipynb#ch0000003?line=13'>14</a>\u001b[0m     qkv \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqkv(x)\u001b[39m.\u001b[39mreshape(B, N, \u001b[39m3\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, C \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads)\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m4\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dl/Desktop/dl/object_detection_model_hw2/hw3/transformer_demo%20%282%29.ipynb#ch0000003?line=14'>15</a>\u001b[0m     q, k, v \u001b[39m=\u001b[39m qkv\u001b[39m.\u001b[39munbind(\u001b[39m0\u001b[39m)   \u001b[39m# make torchscript happy (cannot use tensor as tuple)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args = get_args()\n",
    "\n",
    "    datamodule = LitCifar10(batch_size=args.batch_size,\n",
    "                            patch_num=args.patch_num, \n",
    "                            num_workers=args.num_workers * args.devices)\n",
    "    datamodule.prepare_data()\n",
    "\n",
    "    data = iter(datamodule.train_dataloader()).next()\n",
    "    patch_dim = data[0].shape[-1]\n",
    "    seqlen = data[0].shape[-2]\n",
    "    print(\"Embed dim:\", args.embed_dim)\n",
    "    print(\"Patch size:\", 32 // args.patch_num)\n",
    "    print(\"Sequence length:\", seqlen)\n",
    "\n",
    "\n",
    "    model = LitTransformer(num_classes=10, lr=args.lr, epochs=args.max_epochs, \n",
    "                           depth=args.depth, embed_dim=args.embed_dim, head=args.num_heads,\n",
    "                           patch_dim=patch_dim, seqlen=seqlen,)\n",
    "\n",
    "    trainer = Trainer(accelerator=args.accelerator, devices=args.devices,\n",
    "                      max_epochs=args.max_epochs, precision=16 if args.accelerator == 'gpu' else 32,)\n",
    "    trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
